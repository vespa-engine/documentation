---
# Copyright Vespa.ai. All rights reserved.
title: "Configuration Servers"
category: oss
redirect_from:
- /en/cloudconfig/configuration-server.html
- /en/operations/configuration-server.html
---

<p>
  Vespa Configuration Servers host the endpoint where application packages are deployed -
  and serves generated configuration to all services -
  see the <a href="/en/overview.html">overview</a>
  and <a href="/en/applications.html">application packages</a> for details.
  I.e., one cannot configure Vespa without config servers, and services cannot run without it.
</p>
<p>
  It is useful to understand the <a href="/en/operations-selfhosted/config-sentinel.html">Vespa start sequence</a>.
  Refer to the sample applications
  <a href="https://github.com/vespa-engine/sample-apps/tree/master/examples/operations/multinode">multinode</a> and
  <a href="https://github.com/vespa-engine/sample-apps/tree/master/examples/operations/multinode-HA">multinode-HA</a>
  for practical examples of multi-configserver configuration.
</p>
<p>
  Vespa configuration is set up using one or more configuration servers (config servers).
  A config server uses <a href="https://zookeeper.apache.org/">Apache ZooKeeper</a>
  as a distributed data storage for the configuration system.
  In addition, each node runs a config proxy to cache configuration data -
  find an overview at <a href="/en/operations-selfhosted/config-sentinel.html">services start</a>.
</p>



<h2 id="status-and-config-generation">Status and config generation</h2>
<p>
  Check the health of a running config server using (replace localhost with hostname):
</p>
<pre>
$ curl http://localhost:19071/state/v1/health
</pre>
<p>
  Note that the config server is a service is itself, and runs with file-based configuration.
  The application packages deployed will not change the config server -
  the config server serves this configuration to all other Vespa nodes.
  This will hence always be config generation 0:
</p>
<pre>
$ curl http://localhost:19071/state/v1/config
</pre>
<p>
  Details in <a href="https://github.com/vespa-engine/vespa/blob/master/configserver/src/main/sh/start-configserver">
  start-configserver</a>.
</p>



<h2 id="redundancy">Redundancy</h2>
<p>
  The config servers are defined in
  <a href="/en/operations-selfhosted/files-processes-and-ports.html#environment-variables">VESPA_CONFIGSERVERS</a>,
  <a href="/en/reference/services.html">services.xml</a> and <a href="/en/reference/hosts.html">hosts.xml</a>:
</p>
<pre>
$ VESPA_CONFIGSERVERS=myserver0.mydomain.com,myserver1.mydomain.com,myserver2.mydomain.com
</pre>
<pre>{% highlight xml %}
<services>
    <admin version="2.0">
        <configservers>
            <configserver hostalias="admin0" />
            <configserver hostalias="admin1" />
            <configserver hostalias="admin2" />
        </configservers>
    </admin>
</services>
{% endhighlight %}</pre>
<pre>{% highlight xml %}
<hosts>
    <host name="myserver0.mydomain.com">
        <alias>admin0</alias>
    </host>
    <host name="myserver1.mydomain.com">
        <alias>admin1</alias>
    </host>
    <host name="myserver2.mydomain.com">
        <alias>admin2</alias>
    </host>
</hosts>
{% endhighlight %}</pre>
<p>
  <a href="/en/operations-selfhosted/files-processes-and-ports.html#environment-variables">VESPA_CONFIGSERVERS</a>
  must be set on all nodes.
  This is a comma- or whitespace-separated list with the hostname of all config servers,
  like <em>myhost1.mydomain.com,myhost2.mydomain.com,myhost3.mydomain.com</em>.
</p>
<p>
  When there are multiple config servers, the <a href="/en/operations-selfhosted/config-proxy.html">config proxy</a>
  will pick a config server randomly (to achieve load balancing between config servers).
  The config proxy is fault-tolerant and will switch to another config server (if there is more than one)
  if the one it is using becomes unavailable or there is an error in the configuration it receives.
</p>
<p>
  For the system to tolerate <em>n</em> failures,
  <a href="#zookeeper">ZooKeeper</a> by design requires using <em>(2*n)+1</em> nodes.
  Consequently, only an odd numbers of nodes is useful,
  so you need minimum 3 nodes to have a fault-tolerant config system.
</p>
<p>
  Even when using just one config server, the application will work if the server goes down
  (but deploying application changes will not work).
  Since the <em>config proxy</em> runs on every node and caches configs,
  it will continue to serve config to the services on that node.
  However, restarting a node when config servers are unavailable
  means that services on the node will be unable to start since the cache will be destroyed when restarting the config proxy.
</p>
<p>
  Refer to the <a href="/en/reference/services-admin.html#configservers">admin model reference</a>
  for more details on <em>services.xml</em>.
</p>



<h2 id="start-sequence">Start sequence</h2>
<p>
  To bootstrap a Vespa application instance, the high-level steps are:
</p>
<ul>
  <li>Start config servers</li>
  <li>Deploy config</li>
  <li>Start Vespa nodes</li>
</ul>
<p>
  <a href="https://github.com/vespa-engine/sample-apps/tree/master/examples/operations/multinode-HA">multinode-HA</a>
  is a great guide on how to start a multinode Vespa application instance - try this first.
  Detailed steps for config server startup:
</p>
<ol>
  <li>
    Set <a href="/en/operations-selfhosted/files-processes-and-ports.html#environment-variables">VESPA_CONFIGSERVERS</a>
    on all nodes, using fully qualified hostnames and the same value on all nodes, including the config servers.
  </li>
  <li>
    Start the config server on the nodes configured in <em>services/hosts.xml</em>.
    Make sure the startup is successful by inspecting
    <a href="/en/reference/state-v1.html#state-v1-health">/state/v1/health</a>, default on port 19071:
<pre>
$ curl http://localhost:19071/state/v1/health
</pre>
<pre>{% highlight json %}
{
    "time" : 1651147368066,
    "status" : {
        "code" : "up"
    },
    "metrics" : {
        "snapshot" : {
            "from" : 1.651147308063E9,
            "to" : 1.651147367996E9
        }
    }
}
{% endhighlight %}</pre>
      If there is no response on the health API, two things can have happened:
    <ul>
      <li>The config server process did not start - inspect logs using <code>vespa-logfmt</code>,
        or check <em>$VESPA_HOME/logs/vespa/vespa.log</em>, normally <em>/opt/vespa/logs/vespa/vespa.log</em>.</li>
      <li>The config server process started, and is waiting for <a href="#zookeeper">Zookeeper quorum</a>:</li>
    </ul>
<pre>
$ vespa-logfmt -S configserver
</pre>
<pre>
<span class="pre-hilite">configserver     Container.com.yahoo.vespa.zookeeper.ZooKeeperRunner	Starting ZooKeeper server with /opt/vespa/var/zookeeper/conf/zookeeper.cfg. Trying to establish ZooKeeper quorum (members: [node0.vespanet, node1.vespanet, node2.vespanet], attempt 1)</span>
configserver     Container.com.yahoo.container.handler.threadpool.ContainerThreadpoolImpl	Threadpool 'default-pool': min=12, max=600, queue=0
configserver     Container.com.yahoo.vespa.config.server.tenant.TenantRepository	Adding tenant 'default', created 2022-04-28T13:02:24.182Z. Bootstrapping in PT0.175576S
configserver     Container.com.yahoo.vespa.config.server.rpc.RpcServer	Rpc server will listen on port 19070
configserver     Container.com.yahoo.container.jdisc.state.StateMonitor	Changing health status code from 'initializing' to 'up'
configserver     Container.com.yahoo.jdisc.http.server.jetty.Janitor	Creating janitor executor with 2 threads
configserver     Container.com.yahoo.jdisc.http.server.jetty.JettyHttpServer	Threadpool size: min=22, max=22
configserver     Container.org.eclipse.jetty.server.Server	jetty-9.4.46.v20220331; built: 2022-03-31T16:38:08.030Z; git: bc17a0369a11ecf40bb92c839b9ef0a8ac50ea18; jvm 11.0.14.1+1-
configserver     Container.org.eclipse.jetty.server.handler.ContextHandler	Started o.e.j.s.ServletContextHandler@341c0dfc{19071,/,null,AVAILABLE}
configserver     Container.org.eclipse.jetty.server.AbstractConnector	Started configserver@3cd6d147{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:19071}
configserver     Container.org.eclipse.jetty.server.Server	Started @21955ms
configserver     Container.com.yahoo.container.jdisc.ConfiguredApplication	Switching to the latest deployed set of configurations and components. <span class="pre-hilite">Application config generation: 0</span>
</pre>
      It will hang until quorum is reached, and the second highlighted log line is emitted.
      Root causes for missing quorum can be:
    <ul>
      <li>
        No connectivity between the config servers.
        Zookeeper logs the members like <code>(members: [node0.vespanet, node1.vespanet, node2.vespanet], attempt 1)</code>.
        Verify that the nodes running config server can reach each other on port 2181.
      </li>
      <li>
        No connectivity can be wrong network config.
        <a href="https://github.com/vespa-engine/sample-apps/tree/master/examples/operations/multinode-HA">multinode-HA</a>
        uses a docker network, make sure there are no underscores in the hostnames.
      </li>
    </ul>
  </li>
  <li>
    Once all config servers return <code>up</code> on <em>state/v1/health</em>,
    an application package can be deployed.
    This means, if deploy fails, it is always a good idea to verify the config server health first -
    if config servers are up, and deploy fails, it is most likely an issue with the application package -
    if so, refer to <a href="/en/application-packages.html">application packages</a>.
  </li>
  <li>
    A successful deployment logs the following, for the <em>prepare</em> and <em>activate</em> steps:
<pre>
Container.com.yahoo.vespa.config.server.ApplicationRepository	Session 2 prepared successfully.
Container.com.yahoo.vespa.config.server.deploy.Deployment	Session 2 activated successfully using no host provisioner. Config generation 2. File references: [file '9cfc8dc57f415c72']
Container.com.yahoo.vespa.config.server.session.SessionRepository	Session activated: 2
</pre>
  </li>
  <li>
    Start the Vespa nodes.
    Technically, they can be started at any time.
    When troubleshooting, it is easier to make sure the config servers are started successfully,
    and deployment was successful - before starting any other nodes.
    Refer to the <a href="/en/operations-selfhosted/config-sentinel.html">Vespa start sequence</a>
    and <a href="/en/operations-selfhosted/admin-procedures.html#vespa-start-stop-restart">Vespa start / stop / restart</a>.
  </li>
</ol>
<p>
  Make sure to look for logs on all config servers when debugging.
</p>



<h2 id="scaling-up">Scaling up</h2>
<p>
  Add a config server node for increased fault tolerance or when replacing a node.
  Read up on <a href="#zookeeper-configuration">ZooKeeper configuration</a> before continuing.
  Although it is <em>possible</em> to add more than one config server at a time,
  doing it one by one is recommended, to keep the ZooKeeper quorum intact.
</p>
<p>
  Due to the ZooKeeper majority vote, use one or three config servers.
</p>
<ol>
  <li>
    Install <em>vespa</em> on new config server node.
  </li><li>
    Append the config server node's hostname to VESPA_CONFIGSERVERS on all nodes, then (re)start
    all config servers in sequence to update the ZooKeeper config. By appending, the current
    config server nodes keep their current ZooKeeper index. Restart the existing config server(s)
    first. Config server will log which servers are configured when starting up to vespa log.
  </li><li>
    Update <em>services.xml</em> and <em>hosts.xml</em> with the new set of config servers,
    then <em>vespa prepare</em> and <em>vespa activate</em>.
  </li>
  <li>
    Restart other nodes one by one to start using the new config servers.
    This will let the vespa nodes use the updated set of config servers.
  </li>
</ol>
<p>
  The config servers will automatically redistribute the application data to new nodes.
</p>



<h2 id="scaling-down">Scaling down</h2>
<p>
  This is the inverse of scaling up, and the procedure is the same.
  Remove config servers from the end of <em>VESPA_CONFIGSERVERS</em>,
  and here one can remove two nodes in one go, if going from three to one.
</p>

<h2 id="replacing-nodes">Replacing nodes</h2>
<p>
  <li>Make sure to replace only one node at a time.</li>
  <li>If you have only one config server you need to first scale up with a new node, then scale down by removing the old node.</li>
  <li>If you have 3 or more you can replace one of the old nodes in VESPA_CONFIGSERVERS with the new one instead of adding one, otherwise same procedure as
    in <a href="#scaling-up">Scaling up</a>. Repeat for each node you want to replace.</li>



<h2 id="tools">Tools</h2>
<p>
  Tools to access config:
</p>
<ul>
  <li><a href="/en/operations-selfhosted/vespa-cmdline-tools.html#vespa-get-config">vespa-get-config</a></li>
  <li><a href="/en/operations-selfhosted/vespa-cmdline-tools.html#vespa-configproxy-cmd">vespa-configproxy-cmd</a></li>
  <li><a href="/en/reference/config-rest-api-v2.html">Config API</a></li>
</ul>



<h2 id="zookeeper">ZooKeeper</h2>
<p>
  <a href="https://zookeeper.apache.org/">ZooKeeper</a>
  handles data consistency across multiple config servers.
  The config server Java application runs a ZooKeeper server,
  embedded with an RPC frontend that the other nodes use.
  ZooKeeper stores data internally in <em>nodes</em> that can have <em>sub-nodes</em>,
  similar to a file system.
</p>
<p>
  At <a href="/en/reference/vespa-cli/vespa_prepare.html">vespa prepare</a>, the application's files,
  along with global configurations, are stored in ZooKeeper.
  The application data is stored under <em>/config/v2/tenants/default/sessions/[sessionid]/userapp</em>.
  At <a href="/en/reference/vespa-cli/vespa_activate.html">vespa activate</a>,
  the newest application is activated <em>live</em>
  by writing the session id into <em>/config/v2/tenants/default/applications/default:default:default</em>.
  It is at that point the other nodes get configured.
</p>
<p>
  Use <em>vespa-zkcli</em> to inspect state, replace with actual session id:
</p>
<pre>
$ vespa-zkcli ls  /config/v2/tenants/default/sessions/<span class="pre-hilite">sessionid</span>/userapp
$ vespa-zkcli get /config/v2/tenants/default/sessions/<span class="pre-hilite">sessionid</span>/userapp/services.xml
</pre>
<p>
  The ZooKeeper server logs to <em>$VESPA_HOME/logs/vespa/zookeeper.configserver.0.log (files are rotated with sequence number)</em>
</p>


<h3 id="zookeeper-configuration">ZooKeeper configuration</h3>
<p>
  The members of the ZooKeeper cluster is generated based on the contents of
  <a href="/en/operations-selfhosted/files-processes-and-ports.html#environment-variables">VESPA_CONFIGSERVERS</a>.
  <em>$VESPA_HOME/var/zookeeper/conf/zookeeper.cfg</em> is written when (re)starting the config server.
  Hence, config server(s) must all be restarted when <code>VESPA_CONFIGSERVERS</code> changes.
</p>
<p>
  The order of the nodes is used to create indexes in <em>zookeeper.cfg</em>, do not change node order.
</p>


<h3 id="zookeeper-recovery">ZooKeeper recovery</h3>
<p>
  If the config server(s) should experience data corruption,
  for instance a hardware failure, use the following recovery procedure.
  One example of such a scenario is if <em>$VESPA_HOME/logs/vespa/zookeeper.configserver.0.log</em>
  says <em>java.io.IOException: Negative seek offset at java.io.RandomAccessFile.seek(Native Method)</em>,
  which indicates ZooKeeper has not been able to recover after a full disk.
  There is no need to restart Vespa on other nodes during the procedure:
</p>
<ol>
  <li><a href="/en/operations-selfhosted/vespa-cmdline-tools.html#vespa-stop-configserver">vespa-stop-configserver</a></li>
  <li><a href="/en/operations-selfhosted/vespa-cmdline-tools.html#vespa-configserver-remove-state">vespa-configserver-remove-state</a></li>
  <li><a href="/en/operations-selfhosted/vespa-cmdline-tools.html#vespa-start-configserver">vespa-start-configserver</a></li>
  <li><a href="/en/vespa-cli.html#deployment">vespa</a> prepare &lt;application path&gt;</li>
  <li><a href="/en/vespa-cli.html#deployment">vespa</a> activate</li>
</ol>
<p>
  This procedure completely cleans out ZooKeeper's internal data snapshots and deploys from scratch.
</p>
<p>
  Note that by default the <a href="../content/content-nodes.html#cluster-controller">cluster controller</a>
  that maintains the state of the content cluster will use the shared same ZooKeeper instance,
  so the content cluster state is also reset when removing state.
  Manually set state will be lost (e.g. a node with user state <em>down</em>).
  It is possible to run cluster-controllers in standalone zookeeper mode -
  see <a href="/en/reference/services-admin.html#cluster-controllers"> standalone-zookeeper</a>.
</p>


<h3 id="zookeeper-barrier-timeout">ZooKeeper barrier timeout</h3>
<p>
  If the config servers are heavily loaded, or the applications being deployed are big,
  the internals of the server may time out when synchronizing with the other servers during deploy.
  To work around, increase the timeout by setting:
  <a href="/en/operations-selfhosted/files-processes-and-ports.html#environment-variables">
  VESPA_CONFIGSERVER_ZOOKEEPER_BARRIER_TIMEOUT</a>
  to 600 (seconds) or higher, and restart the config servers.
</p>


<h2 id="configuration">Configuration</h2>
<p>
  To access config from a node not running the config system
  (e.g. doing feeding via the Document API),
  use the environment variable
  <a href="/en/operations-selfhosted/files-processes-and-ports.html#environment-variables">VESPA_CONFIG_SOURCES</a>:
</p>
<pre>
$ export VESPA_CONFIG_SOURCES="myadmin0.mydomain.com:19071,myadmin1.mydomain.com:19071"
</pre>
<p>
  Alternatively, for Java programs, use the system property <em>configsources</em>
  and set it programmatically or on the command line with the <em>-D</em> option to Java.
  The syntax for the value is the same as for <em>VESPA_CONFIG_SOURCES</em>.
</p>


<h3 id="system-requirements">System requirements</h3>
<p>
  The minimum heap size for the JVM it runs under is 128 Mb and max heap size is 2 GB
  (which can be changed with a <a href="/en/performance/container-tuning.html#config-server-and-config-proxy">setting</a>).
  It writes a transaction log that is regularly purged of old items, so little disk space is required.
  Note that running on a server that has a lot of disk I/O
  will adversely affect performance and is not recommended.
</p>


<h3 id="ports">Ports</h3>
<p>
  The config server RPC port can be changed by setting
  <a href="/en/operations-selfhosted/files-processes-and-ports.html#environment-variables">VESPA_CONFIGSERVER_RPC_PORT</a>
  on all nodes in the system.
</p>
<p>
  Changing HTTP port requires changing the port in
  <em>$VESPA_HOME/conf/configserver-app/services.xml</em>:
</p>
<pre>{% highlight xml %}
<http>
    <server port="19079" id="configserver" />
</http>
{% endhighlight %}</pre>
<p>
  When deploying, use the <em>-p</em> option, if port is changed from the default.
</p>


<h2 id="troubleshooting">Troubleshooting</h2>
<table class="table">
  <thead>
    <tr><th>Problem</th><th>Description</th></tr>
  </thead>
  <tbody>
    <tr>
      <th>Health checks</th>
      <td>
        <p id="health-checks">
          Verify that a config server is up and running using
          <a href="/en/reference/state-v1.html#state-v1-health">/state/v1/health</a>,
          see <a href="#start-sequence">start sequence</a>.
          Status code is <code>up</code> if the server is up and has finished bootstrapping.
        </p>
        <p>
          Alternatively, use
          <a href="http://localhost:19071/status.html" data-proofer-ignore>http://localhost:19071/status.html</a>
          which will return response code 200 if server is up and has finished bootstrapping.
        </p>
        <p>
          Metrics are found at
          <a href="/en/reference/state-v1.html#state-v1-metrics">/state/v1/metrics</a>.
          Use <a href="/en/operations-selfhosted/vespa-cmdline-tools.html#vespa-model-inspect">vespa-model-inspect</a>
          to find host and port number, port is 19071 by default.
        </p>
      </td>
    <tr>
      <th>Consistency</th>
      <td>
        <p id="consistency">
          When having more than one config server, consistency between the servers is crucial.
          <a href="http://localhost:19071/status" data-proofer-ignore>http://localhost:19071/status</a>
          can be used to check that settings for config servers are the same for all servers.
        </p>
        <p>
          <a href="/en/operations-selfhosted/vespa-cmdline-tools.html#vespa-config-status">vespa-config-status</a>
          can be used to check config on nodes.
        </p>
        <p>
          <a href="http://localhost:19071/application/v2/tenant/default/application/default" data-proofer-ignore>
            http://localhost:19071/application/v2/tenant/default/application/default</a>
          displays active config generation and should be the same on all servers,
          and the same as in response from running
          <a href="/en/vespa-cli.html#deployment">vespa deploy</a>
        </p>
      </td>
    </tr><tr>
      <th>Bad Node</th>
      <td>
        <p id="bad-node">
          If running with more than one config server and one of these goes down or has hardware failure,
          the cluster will still work and serve config as usual
          (clients will switch to use one of the good servers).
          It is not necessary to remove a bad server from the configuration.
        </p>
        <p>
          Deploying applications will take longer,
          as <a href="/en/vespa-cli.html#deployment">vespa deploy</a>
          will not be able to complete a deployment on all servers when one of them is down.
          If this is troublesome, lower the <a href="#zookeeper-barrier-timeout">barrier timeout</a> -
          (default value is 120 seconds).
        </p>
        <p>
          Note also that if you have not configured
          <a href="/en/reference/services-admin.html#cluster-controller">cluster controllers</a> explicitly,
          these will run on the config server nodes and the operation of these might be affected.
          This is another reason for not trying to manually remove a bad node from the config server setup.
        </p>
      </td>
    </tr><tr>
      <th>Stuck filedistribution</th>
      <td>
        <p id="stuck-filedistribution">
          The config system distributes binary files (such as jar bundle files)
          using <a href="/en/deployment.html#file-distribution">file-distribution</a> -
          use <a href="/en/operations-selfhosted/vespa-cmdline-tools.html#vespa-status-filedistribution">
          vespa-status-filedistribution</a> to see detailed status if it gets stuck.
        </p>
      </td>
    </tr>
    <tr>
      <th>Memory</th>
      <td>
        <p id="memory">
          Insufficient memory on the host / in the container running the config server will cause
          startup or deploy / configuration problems -
          see <a href="/en/operations-selfhosted/docker-containers.html">Docker containers</a>.
        </p>
      </td>
    </tr>
  <tr>
    <th>ZooKeeper</th>
    <td>
      <p id="zookeeper-troubleshooting">
        The following can be caused by a full disk on the config server, or clocks out of sync:
      </p>
<pre>
at com.yahoo.vespa.zookeeper.ZooKeeperRunner.startServer(ZooKeeperRunner.java:92)
Caused by: java.io.IOException: The accepted epoch, 10 is less than the current epoch, 48
</pre>
      <p>Users have reported that "Copying the currentEpoch to acceptedEpoch fixed the problem".</p>
    </td>
  </tr>
  </tbody>
</table>

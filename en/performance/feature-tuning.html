---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Vespa Serving Tuning"
redirect_from:
- /documentation/performance/feature-tuning.html
---

<p>
  This document describes tuning certain features of an application for high performance,
  the main focus is on content cluster search features,
  see <a href="container-tuning.html">Container tuning</a> for tuning of container clusters.
  The <a href="sizing-search.html">search sizing guide</a> is about scaling an application deployment.
</p>



<h2 id="attribute-vs-index">Attribute v.s index</h2>
<p>
  The <a href="../attributes.html">attribute</a> documentation summaries when to use
  <a href="../reference/schema-reference.html#attribute">attribute</a>
  in the <a href="../reference/schema-reference.html#indexing">indexing</a> statement.
  Also see the <a href="../operations/admin-procedures.html#schema-changes">procedure</a>
  for changing from attribute to index and vice-versa.
</p>
<pre>
field timestamp type long {
    indexing: summary | attribute
    rank:     filter
}
</pre>
<p>
  If both index and attribute is configured for string type fields,
  Vespa will do search and matching against the index with default match <code>text</code>.
  All numeric type fields and tensor fields are attribute (in-memory) fields in Vespa.
</p>




<h2 id="when-to-use-fast-search-for-attribute-fields">When to use fast-search for attribute fields</h2>
<p>
  By default, Vespa does not build any posting list index structures over <em>attribute</em> fields.
  Adding <em>fast-search</em> to the attribute definition as shown below
  will add an in-memory B-tree posting list structure
  which enables faster search for some cases (but not all, see next paragraph):
</p>
<pre>
field timestamp type long {
    indexing:  summary | attribute
    attribute: fast-search
    rank:      filter
}
</pre>
<p>
  When Vespa runs a query with multiple query items, it builds a query execution plan.
  It tries to optimize the plan so the temporary result set is as small as possible.
  To do this, query tree items that are restrictive (matching few documents) are evaluated early .
  The query execution plan looks at hit count estimates for each part of the query tree
  using the index and B-tree dictionaries which track the number of documents a given term occurs in.
</p>
<p>
  However, for attribute fields without <a href="../attributes.html#fast-search">fast-search</a>
  there is no hit count estimate,
  so the estimate becomes equal to the total number of documents (matches all)
  and is thus moved to the end of the query evaluation.
  A query with only one query term searching an attribute field without <code>fast-search</code>
  would be a linear scan over all documents and thus expensive:
</p>
<pre>
select * from sources * where range(timestamp, 0, 100);
</pre>
<p>
  But if this query term is <em>and</em>-ed with another term which matches fewer documents,
  that term will determine the cost instead, and fast-search won't be necessary, e.g.:
</p>
<pre>
select * from sources * where range(timestamp, 0, 100) and uuid contains "123e4567-e89b-12d3-a456-426655440000";
</pre>
<p>The general rules of thumb for when to use fast-search for an attribute field is:</p>
<ul>
  <li>Use <em>fast-search</em> if the attribute field is searched without any other query terms</li>
  <li>Use <em>fast-search</em> if the attribute field could limit the total number of hits efficiently</li>
</ul>
<p>
  Changing fast-search aspect of the attribute is a
  <a href="../reference/schema-reference.html#modifying-schemas">live change</a>
  which does not require any re-feeding, so testing the performance with and without is low effort.
  Adding or removing <em>fast-search</em> requires restart.
</p>

<p>Note that <em>attribute</em> fields with <em>fast-search</em> that are not used in term based
  <a href="../ranking.html">ranking</a> should use <em>rank: filter</em>
for optimal performance. See reference <a href="../reference/schema-reference.html#rank">rank: filter</a>.
</p>
<p>
  See optimization for sorting on a <em>single-value numeric attribute with fast-search</em> using
  <a href="../reference/query-api-reference.html#sorting.degrading">sorting.degrading</a>.
</p>


<h2 id="hybrid-taat-daat">Hybrid TAAT and DAAT query evaluation</h2>
<p>
  Vespa supports <strong>hybrid</strong> query evaluation over inverted indexes,
  combining <em>TAAT</em> and <em>DAAT</em> evaluation to combine the best of both query evaluation techniques.
  Hybrid is not enabled per default and is triggered by a run time query parameter.
</p>
<ul>
  <li><strong>TAAT:</strong> <em>Term At A Time</em> scores documents one query term at a time.
    The entire posting iterator can be read per query term and the score of a document is accumulated.
    It is CPU cache friendly as posting data is read sequentially without random seeking the posting list iterator.
    The downside is that <em>TAAT</em> limits the term based ranking function to be a linear sum of term scores.
    This downside is one reason why most search engines uses <em>DAAT</em>.</li>
  <li><strong>DAAT:</strong> <em>Document At A Time</em> scores documents completely one at a time.
    This requires multiple seeks in the term posting lists,
    which is CPU cache unfriendly, but allows non-linear ranking functions. </li>
</ul>
<p>
  Generally Vespa does <em>DAAT</em> (document-at-a-time) query evaluation
  and not <em>TAAT</em> (term-at-a time) for the reason listed above.
</p>
<p>
  Ranking (score calculation) and matching (does the document match the query logic)
  is not fully two separate disjunct phases,
  where one first find matches and in a later phase calculates the ranking score.
  Matching and <em>first-phase</em> score calculation is interleaved when using <em>DAAT</em>.
</p>
<p>
  The <em>first-phase</em> ranking score is assigned to the hit when it satisfies the query constraints.
  At that point, the term iterators are positioned at the document id
  and one can unpack additional data from the term posting lists -
  e.g. for term proximity scoring used by the <a href="../nativerank.html">nativeRank</a> ranking feature,
  which also requires unpacking of positions of the term within the document.
</p>
<p>
  The way hybrid query evaluation is done is that <em>TAAT</em> is used for sub-branches
  of the overall query tree which is not used for term based ranking.
</p>
<p>
  Using <em>TAAT</em> can speed up query matching significantly (up to 30-50%)
  in cases where the query tree is large and complex,
  and where only parts of the query tree is used for term based ranking.
  Examples of query tree branches that would require <em>DAAT</em>
  is using text ranking features like <a href="../reference/rank-features.html">bm25 or nativeRank</a>.

  The list of ranking features which can handle <em>TAAT</em> is long,
  but using <a href="../tensor-user-guide.html">attribute or tensor</a> features only
  can have the entire tree evaluated using <em>TAAT</em>.
</p>
<p>
  For example, for a query where there is a user text query from an end user,
  one can use <em>userQuery()</em> YQL syntax and combine it with application level constraints.
  The application level filter constraints in the query could benefit from using <em>TAAT</em>.
  Given the following document schema:
</p>
<pre>
search news {
  document news {
    field title type string {}
    field body type string{}
    field popularity type float {}
    field market type string {
      rank:filter
      indexing: attribute
      attribute: fast-search
    }
    field language type string {
      rank:filter
      indexing: attribute
      attribute: fast-search
    }
  }
  fieldset default {
    fields: title,body
  }
  rank-profile text-and-popularity {
    first-phase {
      expression: attribute(popularity) + log10(bm25(title)) + log10(bm25(body))
    }
  }
}
</pre>
<p>
  In this case the rank profile only uses two ranking features,
  the popularity attribute and the <a href="../reference/bm25.html">bm25</a> score of the userQuery().
  These are used in the default fieldset containing the title and body.
  Notice how neither <em>market</em> or <em>language</em> is used in the ranking expression.
</p>
<p>
  In this query example, there is a language constraint and a market constraint,
  where both language and market is queried with a long list of valid values using OR,
  meaning that the document should match any of the market constraints and any of the language constraints.
</p>
<pre>
{
  'hits': 10,
  'ranking.profile': text-and-popularity",
  'yql': 'select * from sources * where userQuery() and \
   (language contains "en" or language contains "br") and \
   (market contains "us" or market contains "eu" or market contains "apac" or market contains ".." ... ..... ..)',
  'query': 'cat video',
  'ranking.matching.termwiselimit': 0.1
}
</pre>
<p>
  The language and the market constraints in the query tree are not used in the
  ranking score and that part of the query tree could be evaluated using <em>TAAT</em>.
  See also <a href="#multi-lookup-set-filtering">multi lookup set filter</a>
  for how to most efficiently search with large set filters.

  The subtree result is then passed as a bit vector into the <em>DAAT</em> query evaluation,
  which could speed up the overall evaluation significantly.
</p>
<p>
  Enabling hybrid <em>TAAT</em> is done by passing
  <code>ranking.matching.termwiselimit=0.1</code> as a request parameter. It's possible to
  evaluate the performance impact by changing this limit. Setting the limit to 0 will force
  termwise evaluation, which might hurt performance.
</p>
<p>
  One can evaluate if using the hybrid evaluation improves search performance by adding the above parameter.
  The limit is compared to the hit fraction estimate of the entire query tree,
  if the hit fraction estimate is higher than the limit,
  the termwise evaluation is used to evaluate sub-branch of the query.
</p>


<h2 id="indexing-uuids">Indexing uuids</h2>
<p>
  When configuring <a href="../reference/schema-reference.html#type:string">string</a>
  type fields with <code>index</code>,
  the default <a href="../reference/schema-reference.html#match">match</a> mode is <code>text</code>.
  This means Vespa will <a href="../linguistics.html#tokenization">tokenize</a> the content and index the tokens.
</p>
<p>
  The string representation of an <a href="https://en.wikipedia.org/wiki/Universally_unique_identifier">
  Universally unique identifier</a> (UUID) is 32 hexadecimal (base 16) digits,
  in five groups, separated by hyphens, in the form 8-4-4-4-12,
  for a total of 36 characters (32 alphanumeric characters and four hyphens).
</p>
<p>
  Example: Indexing <code>123e4567-e89b-12d3-a456-426655440000</code> with the above document definition,
  Vespa will tokenize this into 5 tokens: <code>[123e4567,e89b,12d3,a456,426655440000]</code>,
  each of which could be matched independently, leading to possible incorrect matches.
</p>
<p>
  To avoid this, change the mode to <a href="../reference/schema-reference.html#word">match: word</a> to
  treat the entire uuid as <i>one</i> token/word:
</p>
<pre>
field uuid type string {
    indexing: summary | index
    match:    word
    rank:     filter
}
</pre>
<p>
  In addition, configure the <code>uuid</code> as a
  <a href="../reference/schema-reference.html#rank">rank: filter</a> field -
  the field will then be represented as efficient as possible during search and ranking.
  The <code>rank:filter</code> behavior can also be triggered at query time on a per-query item basis
  by the <code>com.yahoo.prelude.query.Item.setRanked()</code>
  in a <a href="../searcher-development.html">custom searcher</a>.
</p>



<h2 id="parent-child-and-search-performance">Parent child and search performance</h2>
<p>
  When searching imported attribute fields (with <code>fast-search</code>) from parent document types
  there is an additional indirection that can be reduced significantly
  if the imported field is defined with <code>rank:filter</code>
  and <a href="../reference/services-content.html#visibility-delay">visibility-delay</a> is configured to &gt; 0.
  The <a href="../reference/schema-reference.html#rank">rank:filter</a> setting impacts posting list granularity
  and <code>visibility-delay</code> enables a cache for the indirection between the child and parent document.
</p>



<h2 id="ranking-and-ml-model-inferences">Ranking and ML Model inferences</h2>
<p>
  Vespa <a href="sizing-search.html">scales</a> with the number of hits the query retrieves per node/search thread,
  and which needs to be evaluated by the first-phase ranking function.
  Read more on <a href="../phased-ranking.html">phased ranking</a>.
  Using phased ranking enables spending more resources during a second phase ranking step than in the first-phase.
  The first-phase should be focused on getting decent recall (retrieve relevant documents in the top k),
  while second phase is used to tune precision.
</p><p>
  For <a href="../text-matching-ranking.html">text ranking</a> applications,
  consider using the <a href="../using-wand-with-vespa.html">WAND</a> query operator -
  WAND can efficiently (sublinear) find the top k documents using an inner scoring function.
</p>


<h2 id="multi-lookup-set-filtering">Multi Lookup - Set filtering</h2>
<p>
  Several real-world search use cases are built around limiting or filtering based on a set filter.
  If the contents of a field in the document matches any of the values in the query set, it should be retrieved.
  E.g. searching data for a set of users:
</p>
<pre>
select * from sources * where user_id = 1 or user_id = 2 or user_id = 3 or user_id = 3 or user_id = 4 or user_id 5 ...
</pre>
<p>
  For OR filters over the same field it is strongly recommended using the
  <a href="../reference/query-language-reference.html#weightedset">weighted set query operator</a>.
  It has considerably less overhead than plain OR for set filtering:
</p>
<pre>
select * from sources * where weightedSet(user_id, {"1":1, "2":1, "3":1})
</pre>
<p>
  Attribute fields used like the above without other stronger query terms,
  should have <code>fast-search</code> and <code>rank:filter</code>.
  If there is a large number of unique values in the field,
  it is faster to use <code>hash</code> dictionary instead of <code>btree</code>,
  which is the default data structure for dictionaries for attribute fields with <code>fast-search</code>:
</p>
<pre>
field user_id type long {
  indexing: summary | attribute
  attribute:fast-search
  dictionary:hash
  rank:filter
}
</pre>
<p>
  E.g. if having 10M unique user_ids in the dictionary, a search for 1000 users per query,
  <em>btree</em> dictionary would be 1000 lookup times log(10M),
  while <em>hash</em> based would be 1000 lookups times 1.
</p>
<p>
  The <code>weightedSet</code> query set filtering approach works great in combination with <em>TAAT</em>,
  see <a href="#hybrid-taat-daat">hybrid TAAT/DAAT</a> section.
</p>
<p>
  Also see the <a href="../reference/schema-reference.html#dictionary">dictionary schema reference</a>.
</p>

<h2 id="document-summaries-hits">Document summaries - hits</h2>
<p>
  If queries request many (thousands) of hits from a content cluster with few content nodes,
  increasing the <a href="caches-in-vespa.html">summary cache</a> might reduce latency and cost.
</p><p>
  Using <a href="../document-summaries.html">explicit document summaries</a>,
  Vespa can support memory-only summary fetching, if all fields referenced in the document summary are
  <strong>all</strong> defined with <code>attribute</code>. Dedicated in-memory summaries avoid (potential)
  disk read and summary chunk decompression.
  Vespa document summaries are stored using compressed
  <a href="../reference/services-content.html#summary-store-logstore-chunk">chunks</a>.
  See also the <a href="practical-search-performance-guide.html#hits-and-summaries">practical
    search performance guide on hits fetching</a>.
</p>


<h2 id="boolean-numeric-text-attribute">Boolean, numeric, text attribute</h2>
<p>
When selecting attribute field type, considering performance, this is a rule of thumb:
</p>
<ol>
  <li>Use boolean if a field is a boolean (max two values)</li>
  <li>Use a string attribute if there is a set of values - only unique strings are stored</li>
  <li>Use a numeric attribute for range searches</li>
  <li>Use a numeric attribute if the data is really numeric, don't replace numeric with string numeric</li>
</ol>
<p>
Refer to <a href="../attributes.html">attributes</a> for details.
</p>



<h2 id="tensor-ranking">Tensor ranking</h2>
<p>
  The ranking workload can be significant for large tensors -
  it is important to have an understanding of both the potential memory and computational cost for each query.
</p>


<h3 id="memory">Memory</h3>
<p>
  Assume the dot product of two tensors with 1000 values of 8 bytes each as in
  <code>tensor&lt;double&gt;(x[1000])</code>.
  With one query tensor and one document tensor,
  the dot product is <code>sum(query(tensor1) * attribute(tensor2))</code>.
  Given a Haswell CPU architecture, where the theoretical upper memory bandwidth is 68 GB/sec,
  this gives 68 GB/sec / 8 KB = 9M ranking evaluations/sec.
  In other words, for a 1 M index, 9 queries per second before being memory bound.
</p>
<p>
  See below for using smaller <a href="#cell-value-types">cell value types</a>, and read more about
  <a href="https://blog.vespa.ai/from-research-to-production-scaling-a-state-of-the-art-machine-learning-system/#model-quantization">
  quantization</a>.
</p>
<h3 id="compute">Compute</h3>
<p>
  When using tensor types with at least one mapped dimension (sparse or mixed tensor),
  <a href="../reference/schema-reference.html#attribute">attribute: fast-rank</a>
  can be used to optimize the tensor attribute for ranking expression evaluation at the cost of using more memory.
  This is a good tradeoff if benchmarking indicates significant latency improvements with <code>fast-rank</code>.
</p>
<p>
  When optimizing ranking functions with tensors, try to avoid temporary objects.
  Use the <a href="https://docs.vespa.ai/playground/">Tensor Playground</a> to evaluate what the expressions map to,
  using the execution details
  <svg height="21" viewBox="0 0 21 21" width="21" xmlns="http://www.w3.org/2000/svg">
    <g fill="none" fill-rule="evenodd" stroke="currentColor"
       stroke-linecap="round" stroke-linejoin="round" transform="translate(2 2)">
      <path d="m5.5.5h6v5h-6z"/>
      <path d="m10.5 11.5h6v5h-6z"/>
      <path d="m.5 11.5h6v5h-6z"/>
      <path d="m3.498 11.5v-3h10v3"/>
      <path d="m8.5 8.5v-3"/>
    </g>
  </svg>
  to list the detailed steps - find examples below.
</p>


<h3 id="multiphase-ranking">Multiphase ranking</h3>
<p>
  To save both memory and compute resources, use <a href="../phased-ranking.html">multiphase ranking</a>.
  In short, use less expensive ranking evaluations to find the most promising candidates,
  then a high-precision evaluation for the top-k candidates.
</p>
<p>
  The blog post series on <a href="https://blog.vespa.ai/building-billion-scale-vector-search/">
  Building Billion-Scale Vector Search</a> is a good read on this.
</p>


<h3 id="cell-value-types">Cell value types</h3>
<table class="table">
  <thead>
  <tr>
    <th>Type</th>
    <th>Description</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <th>double</th>
    <td>
      <p id="double">
        The 64-bit floating-point <code>double</code> format is the default tensor cell type.
        It gives the best precision at the cost of high memory usage and somewhat slower calculations.
        Using a smaller value type increases performance, trading off precision,
        so consider changing to one of the cell types below before scaling the application.
      </p>
    </td>
  </tr>
  <tr>
    <th>float</th>
    <td>
      <p id="float">
        The 32-bit floating-point format <code>float</code> should usually be used for all tensors
        when scaling for production.
        Note that some frameworks like tensorflow prefers 32-bit floats.
        A vector with 1000 dimensions,
        <code>tensor&lt;float&gt;(x[1000])</code> uses approximately 4K memory per tensor value.
      </p>
    </td>
  </tr>
  <tr>
    <th>bfloat16</th>
    <td>
      <p id="bfloat16">
        This type has the range as a normal 32-bit float but only 8 bits of precision,
        and can be thought of as a "float with lossy compression" -
        see <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">Wikipedia</a>.
        If memory (or memory bandwidth) is a concern,
        change the most space-consuming tensors to use the <code>bfloat16</code> cell type.
        Some careful analysis of the data is required before using this type.
      </p>
      <p>
        When doing calculations, <code>bfloat16</code> will act as if it was a 32-bit float,
        but the smaller size comes with a potential computational overhead.
        In most cases, the <code>bfloat16</code> needs conversion to a 32-bit float
        before the actual calculation can take place, adding an extra conversion step.
      </p>
      <p>
        In some cases, having tensors with <code>bfloat16</code> cells might bypass some build-in optimizations
        (like matrix multiplication) that will be hardware accelerated only if the cells are the same type.
        To avoid this,
        use the <a href="../reference/ranking-expressions.html#cell_cast">cell_cast</a> tensor operation
        to make sure the cells are of the right type before doing the more expensive operations.
      </p>
    </td>
  </tr>
  <tr>
    <th>int8</th>
    <td>
      <p id="int8">
        If using machine-learning to generate a model with data quantization,
        one can target the <code>int8</code> cell value type,
        which is a signed integer with range from -128 to +127 only.
        This is also treated like a "float with limited range and lossy compression" by the Vespa tensor framework,
        and gives results as if it was a 32-bit float when any calculation is done.
        This type is also suitable when representing boolean values (0 or 1).
      </p>
      {% include note.html content='If the input for an <code>int8</code> cell is not directly representable,
      the resulting cell value is undefined,
      so take care to only input numbers in the <code>[-128,127]</code> range.' %}
      <p>
        It's also possible to use <code>int8</code> representing binary data for
        <a href="../reference/schema-reference.html#distance-metric">hamming distance</a> Nearest-Neighbor search.
        Refer to <a href="https://blog.vespa.ai/billion-scale-knn/">billion-scale-knn</a> for example use.
      </p>
    </td>
  </tr>
  </tbody>
</table>


<h3 id="Inner-outer-products">Inner/outer products</h3>
<p>The following is a primer into inner/outer products and execution details:</p>
<table class="table">
  <thead>
  <tr>
    <th>tensor a</th>
    <th>tensor b</th>
    <th>product</th>
    <th>sum</th>
    <th>comment</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td style="white-space: nowrap;">tensor(x[3]):[1.0, 2.0, 3.0]</td>
    <td>tensor(x[3]):[4.0, 5.0, 6.0]</td>
    <td style="white-space: nowrap;">tensor(x[3]):[4.0, 10.0, 18.0]</td>
    <td>32</td>
    <td>
      <p id="inner-product">
        <a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEMSybIiFIAXA2gZywAnABQAPRAGYAugEo4iAIzEwAJmXTIrCAF9W20hmrlcDIgbaU0WugwBGLGpg5Qe-IWMmz5AFmUBWZQA2KU1HXQx9ViNME04zawp8aPJ6TlhzR3YGRgAqe2tw1EjDBNjCBwsk6whIVKhoAFdaAGMKzOdIPgaAW2Fc2xlQmkKdFCkQbSA">
        Playground example</a>.
        The dimension name and size is the same in both tensors - this is an inner product, with a scalar result.
      </p>
    </td>
  </tr>
  <tr>
    <td>tensor(x[3]):[1.0, 2.0, 3.0]</td>
    <td style="white-space: nowrap;">tensor(y[3]):[4.0, 5.0, 6.0]</td>
    <td>tensor(x[3],y[3]):[<br/>[4.0, 5.0, 6.0],<br/>[8.0, 10.0, 12.0],<br/>[12.0, 15.0, 18.0] ]</td>
    <td>90</td>
    <td>
      <p id="outer-product-same-dimensions">
        <a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEMSybIiFIAXA2gZywAnABQAPRAGYAugEo4iAIzEwAJmXTIrCAF9W20hmrlcDIgbaU0WugwBGLGpg5Qe-IWMmz5AFmUBWZQA2KU1HXQx9ViNME04zawp8aPJ6TlhzR3YGRgAqe2tw1EjDBNjCBwsk6whIVKhoAFdaAGMKzOdIPgaAW2Fc2xlQmkKdFCkQbSA">
        Playground example</a>.
        The dimension size is the same in both tensors, but dimensions have different names -&gt;
        this is an outer product, the result is a two-dimensional tensor.
      </p>
    </td>
  </tr>
  <tr>
    <td>tensor(x[3]):[1.0, 2.0, 3.0]</td>
    <td>tensor(x[2]):[4.0, 5.0]</td>
    <td>undefined</td>
    <td></td>
    <td>
      <p id="undefined">
        <a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEMSybIiFIAXA2gZywAnABQAPRAGYAugEo4iAIzEwAJmXTIrCAF9W20hmrlcDIgbaU0WugwBGLGpg5Qe-IWMQrZ8gCzKArFKajroY+qxGmCacZtYU+JHk9Jyw5o7sDIwAVPbWoajhhnHRhA4WCdYQkMlQ0ACutADGZenOkHx1ALbC2bYywTT5OihSINpAA">
        Playground example</a>.
        Two tensors in same dimension, but with different length -&gt; undefined.
      </p>
    </td>
  </tr>
  <tr>
    <td>tensor(x[3]):[1.0, 2.0, 3.0]</td>
    <td>tensor(y[2]):[4.0, 5.0]</td>
    <td>tensor(x[3],y[2]):[<br/>[4.0, 5.0],<br/>[8.0, 10.0],<br/>[12.0, 15.0] ]</td>
    <td>54</td>
    <td>
      <p id="outer-product-different-dimensions">
        <a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEMSybIiFIAXA2gZywAnABQAPRAGYAugEo4iAIzEwAJmXTIrCAF9W20hmrlcDIgbaU0WugwBGLGpg5Qe-IcICeiFbPkAWZQBWKU1HXQx9ViNME04zawp8aPJ6TlhzR3YGRgAqe2tw1EjDBNjCBwsk6whIVKhoAFdaAGMKzOdIPgaAW2Fc2xlQmkKdFCkQbSA">
        Playground example</a>.
        Two tensors with different names and dimensions -&gt;
        this is an outer product, the result is a two-dimensional tensor.
      </p>
    </td>
  </tr>
  </tbody>
</table>
<p>
  Inner product - observe optimized into <code>DenseDotProductFunction</code>
  with no temporary objects:
</p>
<pre>{% highlight json %}
[ {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::DenseDotProductFunction",
    "symbol": "vespalib::eval::(anonymous namespace)::my_cblas_double_dot_product_op(vespalib::eval::InterpretedFunction::State&, unsigned long)"
  } ]
{% endhighlight %}</pre>
<p>
  Outer product, parsed into a tensor multiplication (<code>DenseSimpleExpandFunction</code>),
  followed by a <code>Reduce</code> operation:
</p>
<pre>{% highlight json %}
[ {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::DenseSimpleExpandFunction",
    "symbol": "void vespalib::eval::(anonymous namespace)::my_simple_expand_op<double, double, double, vespalib::eval::operation::InlineOp2<vespalib::eval::operation::Mul>, true>(vespalib::eval::InterpretedFunction::State&, unsigned long)"
  },
  {
    "class": "vespalib::eval::tensor_function::Reduce",
    "symbol": "void vespalib::eval::instruction::(anonymous namespace)::my_full_reduce_op<double, vespalib::eval::aggr::Sum<double> >(vespalib::eval::InterpretedFunction::State&, unsigned long)"
  } ]
{% endhighlight %}</pre>
<p>
  Note that an inner product can also be run on mapped tensors
  (<a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEMSybIiFIAXA2gZywAnABQAPYAF8AlHGCiAjHHnEwogExw1K0QGY4OiZFYQJrCaQzVyuBkQttKaY3QYAjFjUwcoPfkLGSMnKKACwAdAAM2hoArJHaegBskYbOphjmrFaYNpx2zhT42eT0nACWtLQEgjiCWAAmAK4AxlwenoQMjABU7mlmKAC6IBJAA">Playground example</a>):
</p>
<pre>{% highlight json %}
[ {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::SparseFullOverlapJoinFunction",
    "symbol": "void vespalib::eval::(anonymous namespace)::my_sparse_full_overlap_join_op<double, vespalib::eval::operation::InlineOp2<vespalib::eval::operation::Mul>, true>(vespalib::eval::InterpretedFunction::State&, unsigned long)"
  } ]
{% endhighlight %}</pre>


<h3 id="mapped-lookups">Mapped lookups</h3>
<p>
  <code>sum(model_id * models, m_id)</code>
</p>
<table class="table">
  <thead>
  <tr><th>tensor name</th><th>tensor type</th></tr>
  </thead>
  <tbody>
  <tr><td>model_id</td><td><code>tensor(m_id{})</code></td></tr>
  <tr><td>models</td><td><code>tensor(m_id{}, x[3])</code></td></tr>
  </tbody>
</table>
<p>
  Using a mapped dimension to select an indexed tensor can be considered a
  <a href="../tensor-examples.html#using-a-tensor-as-a-lookup-structure">mapped lookup</a>.
  This is similar to creating a slice, but optimized into a single <code>MappedLookup</code> -
  see <a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gFssATAgGwH0BLFksmpCIJIAFwK0AzlgBOACkY8WwAL4BKOMEYAmOAEYAdAAYVkARBUCVpDNXK4GRG4MppzdBszbtJ-GpmEocSlZBSVVYgAPRABmAF0NLT04RAAWY2IwAFYMsAA2YzjMnRSAdlyADlyATkLTd0sMawE7TAcRJ3cKfFbyehFJAFdGBVYOJTAAKjAvDklipTU-f0IGIZHZrl4pmbGfBd4lhqtnCF6odtXTzFdziEh+qEl2bgBjTpXVkVHvSUnNxZaJRwHT1fyNVDNWxdS5CZbkW7ue6PJgAQxwOAILE47CwWAA1oMcJwZFjBu94YJApBSSxyQQfnN-nslMR1sRFIczOCrCg4iAVEA">
  Tensor Playground</a> example.
</p>
<pre>{% highlight json %}
[ {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::MappedLookup",
    "symbol": "void vespalib::eval::(anonymous namespace)::my_mapped_lookup_op<double>(vespalib::eval::InterpretedFunction::State&, unsigned long)"
  } ]
{% endhighlight %}</pre>


<h3 id="three-way-dot-product-mapped">Three-way dot product - mapped</h3>
<p>
  <code>sum(query(model_id) * model_weights * model_features)</code>
</p>
<table class="table">
  <thead>
  <tr><th>tensor name</th><th>tensor type</th></tr>
  </thead>
  <tbody>
  <tr><td>query(model_id)</td><td><code>tensor(model{})</code></td></tr>
  <tr><td>model_weights</td><td><code>tensor(model{}, feature{})</code></td></tr>
  <tr><td>model_features</td><td><code>tensor(feature{})</code></td></tr>
  </tbody>
</table>
<p>
  Three-way mapped (sparse) dot product:
  <a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEcBXAgJwE8AKAWywBMCAGwD6AS34BKEmRqQiCSABcCtAM5Y2AHmhCsAQyUA+XgOHAAvpLjAeABjgBGC5FkQLsi6QzVyuBkTecpRobnQMfIKiAO4EYgDmABZKajI0mApQKuqaOnqGJpHmXmDQBIbMbASW1sBgADq0tmZCcPbEpeVKlQRw0HYWTsSNzVFtdh1lFVV9znAATMNNRa08jpNdPX0DcADMS6PCbeud073QcwAsYC5hHhhesr6Y-oqBYRT4z+T0iisiU26VVSQXS8gY2Q02l0BmMXEBPRqNn6cAArJNHHAAGy3dL3VCPHwfV6ENLBL5hCCQX5QFjsbj-CSSMAAKjA-1iCWSalZ7JaAM2wLJYMyTFYnFMUXEUl5HLiSRSsv5CKFd08oNCYJJ4I1VJC33CijUzB4XDpEsZMrZcq5iutysFBDU0l1GQYxtN5oZ-KZSqlnIVPPtUpVTukaoeKAAuiALEA">
    Tensor Playground</a>
</p>
<pre>{% highlight json %}
[ {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::Sparse112DotProduct",
    "symbol": "void vespalib::eval::(anonymous namespace)::my_sparse_112_dot_product_op<float>(vespalib::eval::InterpretedFunction::State&, unsigned long)"
  } ]
{% endhighlight %}</pre>


<h3 id="three-way-dot-product-mixed">Three-way dot product - mixed</h3>
<p>
  <code>sum(query(model_id) * model_weights * model_features)</code>
</p>
<table class="table">
  <thead>
  <tr><th>tensor name</th><th>tensor type</th></tr>
  </thead>
  <tbody>
  <tr><td>query(model_id)</td><td><code>tensor(model{})</code></td></tr>
  <tr><td>model_weights</td><td><code>tensor(model{}, feature[2])</code></td></tr>
  <tr><td>model_features</td><td><code>tensor(feature[2])</code></td></tr>
  </tbody>
</table>
<p>
  Three-way mapped (mixed) dot product:
  <a href="https://docs.vespa.ai/playground/#N4KABGBEBmkFxgNrgmUrWQPYAd5QFNIAaFDSPBdDTAO30gEcBXAgJwE8AKAWywBMCAGwD6AS34BKEmRqQiCSABcCtAM5Y2AHmhCsAQyUA+XgOHAAvpLjAeABjgBGC5FkQLsi6QzVyuBkTecpRobnQMfIKiAO4EYgDmABZKajI0mApQKuqaOnqGJpHmXmDQBIbMbASIAEwAutbAYAA6tLZmQnD2xKXlSpUEcHYWTsSt7VFddj1lFVVOIzVjbUWdPI4zfQNDIwDMyxPCXRu9c4POcAAsYC5hHhhesr6Y-oqBYRT4z+T0iqsis36VVSQXS8gY2Q02l0BmMXEBA1qDTgiAArMQAGx1Vzpe6oR4+D6vQhpYJfMIQSC-KAsdjcf4SSRgABUYH+sQSyTULLZHQBW2BpLBmSYrE4pii4ikPPZcSSKRlfIRgrunlBoTBxPB6spIW+4UUamYPC4tPFDOlrNlnIVVqVAoIamkOoyDCNJrN9L5jMVko58u5dslysd0lVDxQdRAFiAA">
    Tensor Playground</a>
</p>
<pre>{% highlight json %}
[ {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::tensor_function::Inject",
    "symbol": "<inject_param>"
  },
  {
    "class": "vespalib::eval::Mixed112DotProduct",
    "symbol": "void vespalib::eval::(anonymous namespace)::my_mixed_112_dot_product_op<float>(vespalib::eval::InterpretedFunction::State&, unsigned long)"
  } ]
{% endhighlight %}</pre>

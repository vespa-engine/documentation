---
# Copyright Vespa.ai. All rights reserved.
title: "Container Tuning"
---

<p>
  A collection of configuration parameters to tune the Container as used in Vespa.
  Some configuration parameters have native
  <a href="../application-packages.html">services.xml</a> support
  while others are configured through
  <a href="../reference/config-files.html#generic-configuration-in-services-xml">
    generic config overrides</a>.
</p>



<h2 id="container-worker-threads">Container worker threads</h2>
<p>
  The container uses multiple thread pools for its operations.
  Most components including request handlers use the container's <em>default thread pool</em>,
  which is controlled by a shared executor instance.
  Any component can utilize the default pool by injecting an <code>java.util.concurrent.Executor</code> instance.
  Some built-in components have dedicated thread pools - such as the Jetty server, the
  <a href="../reference/services-search.html#threadpool">search handler</a> and
  <a href="../reference/services-docproc.html#threadpool">document-processing</a> chains.
  These thread pools are injected through special wiring in the config model and
  are not easily accessible from other components.
</p>
<p>
  The thread pools are by default scaled on the system resources as reported by the JVM
  (<code>Runtime.getRuntime().availableProcessors()</code>).
  It's paramount that the <code>-XX:ActiveProcessorCount</code>/<code>jvm_availableProcessors</code>
  configuration is correct for the container to work optimally.
  The default thread pool configuration can be overridden through services.xml.
  We recommend you keep the default configuration as it's tuned to work across a variety of workloads.
  Note that the default configuration and pool usage may change between minor versions.
</p>
<p>
  The container will pre-start the minimum number of worker threads,
  so even an idle container may report running several hundred threads.
  The <a href="../reference/services-search.html#threadpool">search handler</a> and
  <a href="../reference/services-docproc.html#threadpool">document processing handler</a>
  thread pools each pre-start the number of workers set in their configurations.
  Note that tuning the capacity upwards increases the risk of high GC pressure
  as concurrency becomes higher with more in-flight requests.
  The GC pressure is a function of number of in-flight requests, the time it takes to complete the request
  and the amount of garbage produced per request.
  Increasing the queue size will allow the application to handle shorter traffic bursts without rejecting requests,
  although increasing the average latency for those requests that are queued up.
  Large queues will also increase heap consumption in overload situations.
  For some thread pools, extra threads will be created once the queue is full (when
  <a href="../reference/services-search.html#threads.boost"> <code>boost</code></a> is specified), and are destroyed
  after an idle timeout. If all threads are occupied, requests are rejected with a 503 response.
</p>
<p>
  The effective thread pool configuration and utilization statistics can be observed through the
  <a href="/en/operations/metrics.html#container-metrics">Container Metrics</a>.
  See <a href="/en/operations/metrics.html#thread-pool-metrics">Thread Pool Metrics</a> for a list of metrics exported.
</p>
{% include note.html content=' If the queue size is set to 0 the metric measuring the queue size -
<code>jdisc.thread_pool.work_queue.size</code> - will instead switch to measure how many threads are active.'%}

<h3 id="recommendation">Recommendation</h3>
<p>
  A fixed size pool is preferable for stable latency during peak load,
  at a cost of a higher static memory load and increased context-switching overhead if excessive number of threads are configured.
  Variable size pool is mostly beneficial to minimize memory consumption during low-traffic periods, and in general if the size of peak load is somewhat unknown.
  The downside is that once all core threads are active,
  latency will increase as additional tasks are queued and launching extra threads is relatively expensive as it involves system calls to the OS.
</p>

<h3 id="container-worker-threads-example">Example</h3>
<pre>{% highlight xml %}
<container id="container" version="1.0">

    <search>
        <!-- Search handler thread pool -->
        <threadpool>
            <!-- Don't set 'boost' attribute or set equal to thread value for fixed pool size  (recommended) -->

            <!-- 40x vcpu (e.g 1000 threads if 25 vcpu) -->
            <threads>40</threads>

            <!-- 100 x threads (e.g 4000 if threads set to 40 and 25 vcpu) -->
            <queue>100</queue>
        </threadpool>
    </search>

    <document-processing>
        <!-- Docproc worker thread pool -->
        <threadpool>
            <!-- 4x vcpu permanent workers, e.g. 200 threads on 50 vcpu -->
            <threads>4</threads>
            <!-- Queue capacity scales with threads and vcpu -->
            <queue>25</queue>
        </threadpool>
    </document-processing>

    <!-- Default thread pool -->
    <config name="container.handler.threadpool">
        <!-- Set corePoolSize==maxthreads for fixed size pool (recommended) -->
        <!-- Note: absolute pool size -->
        <corePoolSize>1000</corePoolSize>
        <maxthreads>1000</maxthreads>
    </config>

</container>
{% endhighlight %}</pre>

<h2 id="container-memory-usage">Container memory usage</h2>
<blockquote>Help, my container nodes are using more than 70% memory!</blockquote>
<p>
  It's common to observe the container process utilizing its maximum configured heap size.
  This, by itself, is not necessarily an indication of a problem. The Java Virtual Machine (JVM)
  manages memory within the allocated heap, and it's designed to use as much of it as possible
  to reduce the frequency of garbage collection.
<p>
  To understand whether enough memory is allocated, look at the garbage collection activity.
  If GC is running frequently and using significant CPU or causing long pauses, it might
  indicate that the heap size is too small for the workload. In such cases, consider increasing
  the maximum heap size. However, if the garbage collector is running infrequently and efficiently,
  it's perfectly normal for the container to utilize most or all of its allocated heap, and even more
  (as some memory will also be allocated outside the heap; e.g. direct buffers for efficient data transfer).
<p>
  Vespa exports several metrics to allow you to monitor JVM GC performance, such as <a href="../reference/container-metrics-reference.html#jvm_gc_overhead">jvm.gc.overhead</a>
  - if this exceeds 8-10% you should consider increasing heap memory and/or tuning GC settings.

<h2 id="jvm-heap-size">JVM heap size</h2>
<p>
  Change the default JVM heap size settings used by Vespa to better suit
  the specific hardware settings or application requirements.
</p>
<p>
  By setting the relative size of the total JVM heap in
  <a href="../reference/services-container.html#nodes">percentage of available memory</a>,
  one does not know exactly what the heap size will be,
  but the configuration will be adaptable
  and ensure that the container can start
  even in environments with less available memory.
  The example below allocates 50% of available memory on the machine to the JVM heap:
</p>
<pre>{% highlight xml %}
<container id="container" version="1.0">
    <nodes>
        <jvm allocated-memory="50%" />
        <node hostalias="node0" />
    </nodes>
</container>
{% endhighlight %}</pre>



<h2 id="jvm-tuning">JVM Tuning</h2>
<p>
  Use <em>gc-options</em> for controlling GC related parameters
  and <em>options</em> for tuning other parameters.
  See <a href="../reference/services-container.html#nodes">reference documentation</a>.
  Example: Running with 4 GB heap using G1 garbage collector and using NewRatio = 1
  (equal size of old and new generation) and enabling verbose GC logging (logged to stdout to vespa.log file).
</p>
<pre>{% highlight xml %}
<container id="default" version="1.0">
    <nodes>
        <jvm options="-Xms4g -Xmx4g -XX:+PrintCommandLineFlags -XX:+PrintGC"
             gc-options="-XX:+UseG1GC -XX:MaxTenuringThreshold=15" />
        <node hostalias="node0" />
    </nodes>
</container>
{% endhighlight %}</pre>
<p>
  The default heap size with docker image is 1.5g which can for high throughput applications be on the low side,
  causing frequent garbage collection.
  By default, the G1GC collector is used.
</p>


<h3 id="config-server-and-config-proxy">Config Server and Config Proxy</h3>
<p>
  The config server and proxy are not executed based on the model in <em>services.xml</em>.
  On the contrary, they are used to bootstrap the services in that model.
  Consequently, one must use configuration variables
  to set the JVM parameters for the config server and config proxy.
  They also need to be restarted (<em>services</em> in the config proxy's case) after a change,
  but one does <em>not</em> need to <em>vespa prepare</em>
  or <em>vespa activate</em> first. Example:
</p>
<pre>
VESPA_CONFIGSERVER_JVMARGS      -Xlog:gc
VESPA_CONFIGPROXY_JVMARGS       -Xlog:gc -Xmx256m
</pre>
<p>
  Refer to <a href="/en/operations-selfhosted/files-processes-and-ports.html#environment-variables">Setting Vespa variables</a>.
</p>



<h2 id="container-warmup">Container warmup</h2>
<p>
  Some applications observe that the first queries made to a freshly started container
  take a long time to complete.
  This is typically due to some components performing lazy setup of data structures or connections.
  Lazy initialization should be avoided in favor of eager initialization in component constructor,
  but this is not always possible.
</p>
<p>
  A way to avoid problems with the first queries in such cases
  is to perform warmup queries at startup.
  This is done by issuing queries from the constructor of the Handler of regular queries.
  If using the default handler,
  <a href="https://github.com/vespa-engine/vespa/blob/master/container-search/src/main/java/com/yahoo/search/handler/SearchHandler.java">
    com.yahoo.search.handler.SearchHandler</a>,
  subclass this and configure your subclass as the handler of query requests in <em>services.xml</em>.
</p>
<p>
  Add a call to a warmupQueries() method as the last line of your handler constructor.
  The method can look something like this:
</p>
<pre>{% highlight java %}
private void warmupQueries() {
    String[] requestUris = new String[] {"warmupRequestUri1", "warmupRequestUri2"};
    int warmupIterations = 50;

    for (int i = 0; i < warmupIterations; i++) {
        for (String requestUri : requestUris) {
            handle(HttpRequest.createTestRequest(requestUri, com.yahoo.jdisc.http.HttpRequest.Method.GET));
        }
    }
}
{% endhighlight %}</pre>
<p>
  Since these queries will be executed before the container starts accepting external queries,
  they will cause the first external queries to observe a warmed up container instance.
</p>
<p>
  Use <a href="../reference/query-api-reference.html#metrics.ignore">metrics.ignore</a>
  in the warmup queries to eliminate them from being reported in metrics.
</p>

<h3 id="disabling-warmups">Disabling warmups</h3>
<p>
  Warmups can be disabled by adding the following container http config to the container section in services.xml:
</p>
<pre>{% highlight xml %}
<config name="container.core.container-http">
  <warmup>false</warmup>
</config>
{% endhighlight %}</pre>

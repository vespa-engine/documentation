---
#Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Accelerated OR search using the WAND algorithm with Vespa"
---

<p>
  This document describes how to use the Weak And algorithm for accelerated OR like search. 
<p>

<p>The WAND algorithm is described in detail in 
<a href="https://www.researchgate.net/profile/David-Carmel-3/publication/221613425_Efficient_query_evaluation_using_a_two-level_retrieval_process/links/02bfe50e6854500153000000/Efficient-query-evaluation-using-a-two-level-retrieval-process.pdf>
Efficient Query Evaluation using a Two-Level Retrieval Process (PDF)</a> by <em>Andrei Z. Broder et al</em></p>

<blockquote>
We have determined that our algorithm significantly reduces the total number of full evaluations by more
than 90%, almost without any loss in precision or recall.
At the heart of our approach there is an efficient implementation of a new Boolean construct called WAND or
Weak AND that might be of independent interest.
</blockquote>
<p>

Vespa has two different implementations of the WAND dynamic pruning algorithm. Both implementations
attempts to retrieve the best top-k scoring documents without exhaustive scoring all documents which matches any of the terms in the query. 

Consider a query example <b><em>is cdg airport in main paris</em>?</b> from the <a href="https://github.com/microsoft/MSMARCO-Passage-Ranking">MS Marco Passage Ranking</a> query set. 

If we run this query over the 8.8M passage documents using OR 
we retrieve and rank <em>7,926,256</em> documents out of <em>8,841,823</em> documents. That is in other words <a href="ranking.html">ranking</a> 89% of the total corpus size. 
It's close to brute force evaluating all documents (100%). 

If we instead change to the boolean retrieval logic to AND we only retrieve 2 documents and fail to retrieve the relevant document(s). </p>

<p>The WAND algorithm tries to address this problem by starting
the search for candidate documents using OR 
limiting the number of documents that are ranked, saving both latency and resource usage (cost) while still returning the same or almost the same top-k results as the brute force OR. For the example
using WAND with <em>K</em> or <em>targetHits</em> to 1000 we fully rank only 196,900 documents. That is a huge improvement over the exhaustive OR search which retrieves and ranks <em>7,926,256</em> documents 
and at the same time retrieving the same results as the exhaustive OR search.
</p>

<p>
So what is the catch? Why not use WAND algorithm all the time? The catch is that the inner wand scoring function can only 
be a linear sum of the ranking contribution from each of the query terms and one cannot override the score calculation (with a ranking expression, see <a href="ranking.html">ranking documentation</a>).

The inner scoring function of both WAND implementations in Vespa cannot be overridden like the ranking expression controlled by the <code>first-phase</code> ranking expression in a ranking profile. 
Users can only control the weight of the 
terms or generally features in the query and in the document document. The weights can be adjusted and both WAND implementations in Vespa attempt to 
calculate the top-k documents with highest maximum inner dot product. WAND could be looked at as performing the maximum inner product search in a sparse vector space without brute force calculating it all over all candidates exhaustively. 
</p>

<p>
Vespa has two WAND query operators, <code>weakAnd</code> and <code>wand</code> with different characteristics. 

<ul>
<li>
<code>WeakAnd</code> which is designed for single valued indexed string fields (or fieldset combining multiple indexed string fields). The <em>weakAnd</em>
  and integrates with linguistic processing (tokenization and stemming). It uses the per term inverted document frequency and query term 
weight in the inner scoring but does not use document term frequency in the scoring. </li>

<li>
The <code>wand</code> query operator which does not integrate with linguistic processing like tokenization, stemming and normalization and the user (you) must specify the query features and their weight and the document features and their weight. 
The features does not need to be string and it is recommended to map from string to numeric types. For example the pre-trained language model <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> uses a fixed vocabulary 
of about 30K tokens and text snippets are tokenized into a set of token ids. We can
then represent the document as a bag of BERT token ids (e.g using weightedset&lt;int&gt; data type) where each token id has a weight which is computed during document processing. For example using <a href="https://github.com/AdeDZY/DeepCT">DeepCT or HDCT</a> weighting. 
Similar approaches exist for other high dimensional sparse vector spaces which does not relate to text matching but where one wants to efficiently perform a maximum inner dot product search.
</li>
</ul>

<p>
If you are in doubt about you can use WAND algorithm to accelerate retrieval you can evaluate using a query set and perform the query exhaustive using brute force OR and compare the top-k results returned with when using the approximative
WAND. If top-k is as measured by Recall@K is high you could save compute resources (and lower latency) by  using WAND.</p>


In the following sections we discuss these two WAND implementations in detail:
</p>



<h2 id="weakand">weakAnd</h2>
<p>
The <a href="reference/query-language-reference.html#weakand">weakAnd query operator</a>
accepts terms searching over multiple fields and also logical conjuctions using OR/AND. It's designed to retrieve over indexed string fields and fieldsets (single-valued or multi-valued) and integrates
fully with linguistic processing like tokenization and stemming. 
</p>

<p>
When using <em>weakAnd</em> via YQL or a Java Searcher plugin,
specify the target for minimum number of hits the operator should produce per content node involved in the query.

The effect of tuning <code>targetHits</code> may not be very intuitive.
To ensure that you get the best hits possible with a <em>weakAnd</em>,
set the target number somewhat higher than the number of hits returned to the user;
setting it 10 times higher should be more than enough.

The reason for increasing the target number is that <em>weakAnd</em>
uses a very simple ranking function internally (inner product) and the hits which are evaluated by the weakAnd scorer is
also evaluated by the <code>first-phase</code> ranking expression.

Anything similar to classic vector vector ranking should correlate well with <em>weakAnd</em> inner product scoring. For example
<code>nativeFieldMatch</code> or <code>bm25</code> ranking features. 

Note that because <em>weakAnd</em> relies on feedback
identifying which hits are used for first phase ranking to increase its threshold for what's considered a good hit,
the special <code>unranked</code> rank profile (which turns off ranking completely)
may cause <em>weakAnd</em> queries to become slower than using a real ranking profile.
</p>

<p>Our query example expressed in 
<a href="query-language.html">YQL - Vespa Query Language</a> :
<pre>
select * from passages where (
    default contains "is" OR default contains "cdg" OR 
    default contains "airport" OR default contains "in" OR default contains "main" OR 
    default contains "paris" 
  );
</pre>

Alternatively using a combination of YQL and simple query language

<pre>
{
  "yql": "select * from passages where userQuery();",
  "query" : "is cdg airport in main paris?",
  "type": "any"
}
</pre>
Where type any means OR. 

Using the <em>weakAnd</em> query operator the query could look like:
<pre>
select * from passages where (
  [{"targetHits": 200}] 
    weakAnd(
      default contains "is", default contains "cdg", default contains "airport", 
      default contains "in", default contains "main", default contains "paris" 
    )
);
</pre>
We specify that the target number of hits (top k) should be 200 (Default 100)
and this number is used per content node if the content is distributed over more than one node.
</p>

<p><b>Weak and inner scoring</b>
The WeakAnd query operator uses the following ranking features when calculating the inner score dot product.
<ul>
  <li><a href="reference/rank-features.html#term(n).significance">term(n).significance</a></li>
  <li><a href="reference/rank-features.html#term(n).weight">term(n).weight</a></li>
</ul>

Note that the number of times the term occurs in the document is not used in the inner scoring.

Both term significance and weight features could be overridden in the query using
<a href="reference/query-language-reference.html#annotations">annotations</a>:

If the term significance is not overridden with the query, the significance is calculated from the indexed corpus 
using a formula loosely based on <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf>Inverse Document Frequency</a>.

Documents that could not potentially compete with any of the hits already in heap (size targetHits) of top hits are skipped,
while the weakAnd implementation still exposes the hits which were evaluated to the first phase ranking function,
and not only the top k hits. When Vespa is configured to use multiple threads per search each thread maintains a top-k scoring heap but communicates
score thresholds. 

</p>

<p>
Often times, for the performance reasons listed above, it is preferable to use weakAnd instead of OR. 
To enable this behavior, the query property <code><a href="reference/query-api-reference.html#weakAnd.replace">weakAnd.replace</a></code>
can be set to true. 
</p>


<h2 id="wand">wand</h2>
<p>
The <a href="reference/query-language-reference.html#wand">wand query operator</a>
works over a single <a href="reference/schema-reference.html#type:weightedset">weightedset field<a> which can be both string or numeric (int/long) and the weight is always int. 
Weighted sets of string must be configured with 
<code>match:word</code> or <code>match:exact</code>. See <a href="reference/schema-reference.html#match">match documentation</a>. There is no linguistic processing of strings for 
string wand features. Below is an example passage document type where we pre-process the text using a BERT tokenizer and map text to bert token ids and assign a weight to each unique token id.   

<pre>
  document passage {
    field text type string { 
      indexing: summary | index
    }
    field deep_ct_tokens type weightedset&lt;int&gt; {
      indexing: summary |attribute
      attribute:fast-search 
    }
  }
</pre> 

We can process text and tokenize the text with a BERT tokenizer and explicit set the weight per token id

<pre>
{ 
	"put":"id:msmarco:passage::8433854", 
	"fields": {
    "text": "Charles de Gaulle airport (CDG) is the main international airport for Paris",
		"deep_ct_tokens": {
		"2248": 12, "1996": 5, "3729": 9, "2003": 8, "2798": 1, 
    "2139": 1, "1006": 3, "1007": 1, "2290": 5, "28724": 6, 
    "3000": 3, "2005": 5, "2364": 1, "3199": 15
		}
	}
}
</pre>

The <code>wand</code> query operator allows full control over both query side weights and document side weights
and it's guaranteed that it will find the top k best hits ranked by the inner dot product  
between the sparse query vector and the sparse document vector.

</p><p>
The Vespa <a href="reference/query-language-reference.html#rank">rank query operator</a>
can be used to create a query tree where a bag of features is used in the <em>wand<em> for efficient retrieval 
with normal lexical query terms to produce matching ranking features like bm25 for the configurable first phase/second-phase ranking.

The example below uses the <em>rank</em> operator to also produce normal text matching features for those top-k documents which are 
retrieved by the inner product search performed by the wand operator.

The userQuery() does not impact recall,
but creates "normal" ranking features for first-phase or second-phase ranking.

Similar at query time we can use the same type of text to feature mapping (in this case all query terms have weight 1):

<pre>
{
  "yql":"select * from passages where rank(
    ([ {"targetHits": 25} ]
      wand(deep_ct_tokens, {"2003": 1, "3729": 1, "2290": 1, "3199": 1, "1999": 1, "2364": 1, "3000": 1})),
      userQuery());",
  "query": "is cdg airport in main paris?",
  "type":"any"
  "ranking.profile": "deep_bm25"
}
</pre>

With out ranking profile <em>deep_bm25</em> defined as  

<pre>
rank-profile deep {
  first-phase {
    expression: rawScore(deep_ct_tokens) + bm25(text)
  }
  summary-features {
    bm25(text)
    rawScore(deep_ct_tokens)
   }
}
</pre>

The <em>rawScore</em> ranking feature is the inner dot product calculated by the wand query operator and for those 25 documents with the highest score there is also a bm25(text) score which we combine with the inner product score. Note
that the bm25 is <b>only</b> produced for the top-k hits returned by the <em>wand</em> . 

In our example the inner product score is 41, and the bm25 of the text is 35.003 

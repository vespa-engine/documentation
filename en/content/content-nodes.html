---
# Copyright Vespa.ai. All rights reserved.
title: "Content nodes, states and metrics"
---

<img src="/assets/img/elastic-feed.svg" alt="Content cluster overview" width="540" height="auto"/>
<p>
  Content cluster processes are <em>distributor</em>, <em>proton</em> and <em>cluster controller</em>.
</p>
<p>
  The distributor calculates the correct content node using
  the distribution algorithm and the <a href="#cluster-state">cluster state</a>.
  With no known cluster state, the client library will send requests to a random node,
  which replies with the updated cluster state if the node was incorrect.
  Cluster states are versioned, such that clients hitting outdated distributors do not override
  updated states with old states.
</p>
<p>
  The <a href="#distributor">distributor</a> keeps track of
  which content nodes that stores replicas of each bucket (maximum one replica each),
  based on <a href="../reference/services-content.html#redundancy">redundancy</a>
  and information from the <em>cluster controller</em>.
  A bucket maps to one distributor only.
  A distributor keeps a bucket database with bucket metadata.
  The metadata holds which content nodes store replicas of the buckets,
  the checksum of the bucket content and the number of documents and meta entries within the bucket.
  Each document is algorithmically mapped to a bucket and forwarded to the correct content nodes.
  The distributors detect whether there are enough bucket replicas on the
  content nodes and add/remove as needed.
  Write operations wait for replies from every replica
  and fail if less than redundancy are persisted within timeout.
</p>
<p>
  The <a href="#cluster-controller">cluster controller</a>
  manages the state of the distributor and content nodes.
  This <em>cluster state</em> is used by the document processing chains
  to know which distributor to send documents to,
  as well as by the distributor to know which content nodes should have which bucket.
</p>



<h2 id="cluster-state">Cluster state</h2>
<p>
There are three kinds of state: <a href="../reference/cluster-v2.html#state-unit">unit state</a>,
  <a href="../reference/cluster-v2.html#state-user">user state</a> and
  <a href="../reference/cluster-v2.html#state-generated">generated state</a> (a.k.a. <em>cluster state</em>).
</p>
<p>
  For new cluster states, the cluster state version is incremented,
  and the new cluster state is broadcast to all nodes.
  There is a minimum time between each cluster state change.
  <!-- ToDo ref to config here -->
</p>
<p>
  It is possible to set a minimum capacity for the cluster state to be <code>up</code>.
  <!-- ToDo ref to config here -->
  If a cluster has so many nodes unavailable that it is considered down,
  the state of each node is irrelevant,
  and thus new cluster states will not be created and broadcast
  before enough nodes are back for the cluster to come back up.
  A cluster state indicating the entire cluster is down,
  may thus have outdated data on the node level.
</p>
<!-- ToDo: An example cluster state string is useful here -->



<h2 id="cluster-controller">Cluster controller</h2>
<p>
  The main task of the cluster controller is to maintain the <a href="#cluster-state">cluster state</a>.
  This is done by <em>polling</em> nodes for state,
  <em>generating</em> a cluster state,
  which is then <em>broadcast</em> to all the content nodes in the cluster.
  Note that clients do not interface with the cluster controller -
  they get the cluster state from the distributors - <a href="#distributor">details</a>.
</p>
<table class="table">
  <thead>
  <tr>
    <th>Task</th>
    <th>Description</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <th>Node state polling</th>
    <td>
      <p id="node-state-polling">
        The cluster controller polls nodes, sending the current cluster state.
        If the cluster state is no longer correct, the node returns correct information immediately.
        If the state is correct, the request lingers on the node,
        such that the node can reply to it immediately if its state changes.
        After a while, the cluster controller will send a new state request to the node,
        even with one pending.
        This triggers a reply to the lingering request and makes the new one linger instead.
        Hence, nodes have a pending state request.
      </p>
      <p>
        During a controlled node shutdown, it starts the shutdown process
        by responding to the pending state request that it is now stopping.
        {% include note.html content='As controlled restarts or shutdowns are implemented as TERM signals
        from the <a href="/en/operations-selfhosted/config-sentinel.html">config-sentinel</a>,
        the cluster controller is not able to differ between controlled and other shutdowns.' %}
      </p>
    </td>
  </tr><tr>
    <th>Cluster state generation</th>
    <td>
      <p id="cluster-state-generation">
        The cluster controller translates unit and user states
        into the generated <em>cluster state</em>
      </p>
    </td>
  </tr><tr>
    <th>Cluster state broadcast</th>
    <td>
      <p id="state-broadcast">
        When node unit states are received, a cluster controller internal cluster state is updated.
        New cluster states are distributed with a minimum interval between. <!-- ToDo link to config here -->
        A grace period per unit state too -
        e.g., distributors and content nodes that are on the same node often stop at the same time.
      </p>
      <p>
        The version number is incremented, and the new cluster state is broadcast.
      </p>
      <p>
        If cluster state version is <a href="../operations-selfhosted/admin-procedures.html#cluster-state">reset</a>,
        distributors and content node processes may have to be restarted in order for the system
        to converge to the new state. Nodes will reject lower cluster state versions to
        prevent race conditions caused by overlapping cluster controller leadership periods.
      </p>
    </td>
  </tr>
  </tbody>
</table>
<p>
  See <a href="../operations-selfhosted/admin-procedures.html#cluster-controller-configuration">
  cluster controller configuration</a>.
</p>


<h3 id="master-election">Master election</h3>
<p>
Vespa can be configured with one cluster controller.
Reads and writes will work well in case of cluster controller down,
but other changes to the cluster (like a content node going down) will not be handled.
It is hence recommended to configure a set of cluster controllers.
</p><p>
The cluster controller nodes elect a master,
which does the node polling and cluster state broadcast.
The other cluster controller nodes only exist to do master election
and potentially take over if the master dies.
</p><p>
All cluster controllers will vote for the cluster controller with the lowest index that says it is ready.
If a cluster controller has more than half of the votes, it will be elected master.
As a majority vote is required,
the number of cluster controllers should be an odd number of 3 or greater.
A fresh master will not broadcast states before a transition time is passed, <!-- ToDo config ref -->
allowing an old master to have some time to realize it is no longer the master.
</p>



<h2 id="distributor">Distributor</h2>
<p>
Buckets are mapped to distributors using the <a href="idealstate.html">ideal state algorithm</a>.
As the cluster state changes, buckets are re-mapped immediately.
The mapping does not overlap -
a bucket is owned by <span style="text-decoration: underline">one</span> distributor.
</p><p>
Distributors do not persist the bucket database,
the bucket-to-content-node mapping is kept in memory in the distributor.
Document count, persisted size and a metadata checksum per bucket is stored as well.
At distributor (re)start, content nodes are polled for bucket information,
and return which buckets are owned by this distributor (using the ideal state algorithm).
There is no centralized bucket directory node.
Likewise, at any distributor cluster state change,
content nodes are polled for bucket handover -
a distributor will then handle a new set of buckets.
</p><p>
Document operations are mapped to content nodes based on bucket locations -
each put/update/get/remove is mapped to a <a href="buckets.html">bucket</a>
and sent to the right content nodes.
To manage the document set as it grows and nodes change, buckets move between content nodes.
</p><p>
Document API clients (i.e. container nodes with
<a href="../reference/services-container.html#document-api">&lt;document-api&gt;</a>)
do not communicate directly with the cluster controller, and do not know the cluster state at startup.
Clients therefore start out by sending requests to a random distributor.
If the document operation hits the wrong distributor,
<code>WRONG_DISTRIBUTION</code> is returned, with the current cluster state in the response.
<code>WRONG_DISTRIBUTION</code> is hence expected and normal at cold start / state change events.
</p>


<h3 id="timestamps">Timestamps</h3>
<p>
<a href="../reads-and-writes.html">Write operations</a>
have a <em>last modified time</em> timestamp assigned when passing through the distributor.
The timestamp is guaranteed to be unique within the
<a href="buckets.html">bucket</a> where it is stored.
The timestamp is used by the content layer to decide which operation is newest.
These timestamps can be used when <a href="../visiting.html">visiting</a>,
to process/retrieve documents within a given time range.
To guarantee unique timestamps, they are in microseconds -
the microsecond part is generated to avoid conflicts with other documents.
</p><p>
If documents are migrated <em>between</em> clusters,
the target cluster will have new timestamps for their entries.
Also, when <a href="../document-processing.html">reprocessing documents</a> <em>within</em> a cluster,
documents will have new timestamps, even if not modified.
</p>


<h3 id="ordering">Ordering</h3>
<p>
The Document API uses the <a href="../documents.html#document-ids">document ID</a> to order operations.
A Document API client ensures that only one operation is pending at the same time.
This ensures that if a client sends multiple operations for the same document,
they will be processed in a defined order. This is done by queueing pending
operations <em>locally</em> at the client.
</p>
{% include note.html content='If sending two write operations to the same document,
and the first operation fails, the enqueued operation is sent. In other words, the
client does not assume there exists any kind of dependency between separate
operations to the same document. If you need to enforce this, use
<a href="../document-v1-api-guide.html#conditional-writes">test-and-set conditions</a>
for writes.' %}
<p>
If <em>different</em> clients have pending operations on the same document,
the order is unspecified.
</p>


<h3 id="maintenance-operations">Maintenance operations</h3>
<p>
Distributors track which content nodes have which buckets in their bucket database.
Distributors then use the <a href="idealstate.html">ideal state algorithm</a>
to generate bucket <em>maintenance operations</em>.
A stable system has all buckets located per the ideal state:
</p>
<ul>
  <li>If buckets have too few replicas, new are generated on other content nodes.</li>
  <li>If the replicas differ, a bucket merge is issued to get replicas consistent.</li>
  <li>If a buckets has too many replicas, superfluous are deleted.
      Buckets are merged, if inconsistent, before deletion.</li>
  <li>If two buckets exist, such that both may contain the same document,
      the buckets are split or joined to remove such overlapping buckets.
      Read more on <a href="buckets.html">inconsistent buckets</a>.</li> <!-- ToDo: change this to a proper anchor within buckets.html - this needs a dedicated section, relevant to point to from visiting -->
  <li>If buckets are too small/large, they will be joined or split.</li>
</ul>
<p>
The maintenance operations have different priorities.
If no maintenance operations are needed, the cluster is said to be in the <em>ideal state</em>.
The distributors synchronize maintenance load with user load,
e.g. to remap requests to other buckets after bucket splitting and joining.
</p>


<h3 id="distributor-restart">Restart</h3>
<p>
  When a distributor stops, it will try to respond to any pending cluster state request first.
  New incoming requests after shutdown is commenced will fail immediately,
  as the socket is no longer accepting requests.
  Cluster controllers will thus detect processes stopping almost immediately.
</p>
<p>
  The cluster state will be updated with the new state internally in the cluster controller.
  Then the cluster controller will wait for maximum
  <a href="https://github.com/vespa-engine/vespa/blob/master/configdefinitions/src/vespa/fleetcontroller.def">
    min_time_between_new_systemstates</a>
  before publishing the new cluster state - this to reduce short-term state fluctuations.
</p>
<p>
  The cluster controller has the option of setting states to make other
  distributors take over ownership of buckets, or mask the change, making the
  buckets owned by the distributor restarting unavailable for the time being.
</p>
<p>
  If the distributor transitions from <code>up</code> to <code>down</code>,
  other distributors will request metadata from the content nodes to take
  over ownership of buckets previously owned by the restarting distributor.
  Until the distributors have gathered this new metadata from all the content
  nodes, requests for these buckets can not be served, and will fail back to client.
  When the restarting node comes back up and is marked <code>up</code> in the cluster state again,
  the additional nodes will discard knowledge of the extra buckets they previously acquired.
</p>
<p>
  For requests with timeouts of several seconds,
  the transition should be invisible due to automatic client resending.
  Requests with a lower timeout might fail,
  and it is up to the application whether to resend or handle failed requests.
</p>
<p>
  Requests to buckets not owned by the restarting distributor will not be affected.
</p>



<h2 id="content-node">Content node</h2>
<p>
  The content node runs <em>proton</em>, which is the query backend.
</p>



<h3 id="content-node-restart">Restart</h3>
<p>
  When a content node does a controlled restart,
  it marks itself in the <code>stopping</code> state and rejects new requests.
  It will process its pending request queue before shutting down.
  Consequently, client requests are typically unaffected by content node restarts.
  The currently pending requests will typically be completed.
  New copies of buckets will be created on other nodes, to store new requests in appropriate redundancy.
  This happens whether node transitions through <code>down</code> or <code>maintenance</code> state.
  The difference being that if transitioning through <code>maintenance</code>,
  the distributor will not start any effort of synchronizing new copies with existing copies.
  They will just store the new requests until the maintenance node comes back up.
</p>
<p>
  When starting, content nodes will start with gathering information
  on what buckets it has data stored for.
  While this is happening, the service layer will expose that it is <code>down</code>.
</p>



<h2 id="metrics">Metrics</h2>
<table class="table">
  <thead>
  <tr>
    <th>Metric</th>
    <th>Description</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <th>.idealstate.idealstate_diff</th><td>
    This metric tries to create a single value indicating distance to the ideal state.
    A value of zero indicates that the cluster is in the ideal state.
    Graphed values of this metric gives a good indication for how
    fast the cluster gets back to the ideal state after changes.
    Note that some issues may hide other issues, so sometimes the graph
    may appear to stand still or even go a bit up again, as resolving one
    issue may have detected one or several others.</td>
  </tr><tr>
    <th>.idealstate.buckets_toofewcopies</th><td>
    Specifically lists how many buckets have too few copies.
    Compare to the <em>buckets</em> metric to see how big a portion of the cluster this is.</td>
  </tr><tr>
    <th>.idealstate.buckets_toomanycopies</th><td>
    Specifically lists how many buckets have too many copies.
    Compare to the <em>buckets</em> metric to see how big a portion of the cluster this is.</td>
  </tr><tr>
    <th>.idealstate.buckets</th><td>
    The total number of buckets managed. Used by other metrics reporting
    bucket counts to know how big a part of the cluster they relate to.</td>
  </tr><tr>
    <th>.idealstate.buckets_notrusted</th><td>
    Lists how many buckets have no trusted copies.
    Without trusted buckets operations against the bucket may have poor performance,
    having to send requests to many copies to try and create consistent replies.</td>
  </tr><tr>
    <th>.idealstate.delete_bucket.pending</th><td>
    Lists how many buckets that needs to be deleted.</td>
  </tr><tr>
    <th>.idealstate.merge_bucket.pending</th><td>
    Lists how many buckets there are,
    where we suspect not all copies store identical document sets.</td>
  </tr><tr>
    <th>.idealstate.split_bucket.pending</th><td>
    Lists how many buckets are currently being split.</td>
  </tr><tr>
    <th>.idealstate.join_bucket.pending</th><td>
    Lists how many buckets are currently being joined.</td>
  </tr><tr>
    <th>.idealstate.set_bucket_state.pending</th><td>
    Lists how many buckets are currently altered for active state.
    These are high priority requests which should finish fast,
    so these requests should seldom be seen as pending.</td>
  </tr>
</tbody>
</table>
<p>
  Example, using the <a href="../deploy-an-application-local.html">quickstart</a> -
  find the distributor port (look for HTTP):
</p>
<pre>
$ docker exec vespa vespa-model-inspect service distributor

distributor @ vespa-container : content
music/distributor/0
    tcp/vespa-container:19112 (MESSAGING)
    tcp/vespa-container:19113 (STATUS RPC)
    tcp/vespa-container:19114 (STATE STATUS HTTP)
</pre>
<p>
  Get the metric value:
</p>
<pre>
$ docker exec vespa curl -s http://localhost:19114/state/v1/metrics | jq . | \
  grep -A 10 idealstate.merge_bucket.pending

        "name": "vds.idealstate.merge_bucket.pending",
        "description": "The number of operations pending",
        "values": {
          "average": 0,
          "sum": 0,
          "count": 1,
          "rate": 0.016666,
          "min": 0,
          "max": 0,
          "last": 0
        },
</pre>



<h2 id="cluster-v2-API-examples">/cluster/v2 API examples</h2>
<p>
  Examples of state manipulation using the <a href="../reference/cluster-v2.html">/cluster/v2 API</a>.
</p>
<p>
  List content clusters:
</p>
<pre>$ curl http://localhost:19050/cluster/v2/</pre>
<pre>{% highlight json %}
{
    "cluster": {
        "music": {
            "link": "/cluster/v2/music"
        },
        "books": {
            "link": "/cluster/v2/books"
        }
    }
}
{% endhighlight %}</pre>
<p>
  Get cluster state and list service types within cluster:
</p>
<pre>$ curl http://localhost:19050/cluster/v2/music</pre>
<pre>{% highlight json %}
{
    "state": {
        "generated": {
            "state": "state-generated",
            "reason": "description"
        }
    }
    "service": {
        "distributor": {
            "link": "/cluster/v2/music/distributor"
        },
        "storage": {
            "link": "/cluster/v2/music/storage"
        }
    }
 }
{% endhighlight %}</pre>
<p>
  List nodes per service type for cluster:
</p>
<pre>$ curl http://localhost:19050/cluster/v2/music/storage</pre>
<pre>{% highlight json %}
{
    "node": {
        "0": {
            "link": "/cluster/v2/music/storage/0"
        },
        "1": {
            "link": "/cluster/v2/music/storage/1"
        }
    }
}
{% endhighlight %}</pre>
<p>
  Get node state:
</p>
<pre>
$ curl http://localhost:19050/cluster/v2/music/storage/0
</pre>
<pre>{% highlight json %}
{
    "attributes": {
        "hierarchical-group": "group0"
    },
    "state": {
        "generated": {
            "state": "up",
            "reason": ""
        },
        "unit": {
            "state": "up",
            "reason": ""
        },
        "user": {
            "state": "up",
            "reason": ""
        }
    },
    "metrics": {
        "bucket-count": 0,
        "unique-document-count": 0,
        "unique-document-total-size": 0
    }
}
{% endhighlight %}</pre>
<p>
  Get all nodes, including topology information (see <code>hierarchical-group</code>):
</p>
<pre>
$ curl http://localhost:19050/cluster/v2/music/?recursive=true
</pre>
<pre>{% highlight json %}
{
    "state": {
        "generated": {
            "state": "up",
            "reason": ""
        }
    },
    "service": {
        "storage": {
            "node": {
                "0": {
                    "attributes": {
                        "hierarchical-group": "group0"
                    },
                    "state": {
                        "generated": {
                            "state": "up",
                            "reason": ""
                        },
                        "unit": {
                            "state": "up",
                            "reason": ""
                        },
                        "user": {
                            "state": "up",
                            "reason": ""
                        }
                    },
                    "metrics": {
                        "bucket-count": 0,
                        "unique-document-count": 0,
                        "unique-document-total-size": 0
                    }
{% endhighlight %}</pre>

<p>
  Set node user state:
</p>
<pre>
curl -X PUT -H "Content-Type: application/json" --data '
  {
      "state": {
          "user": {
              "state": "retired",
              "reason": "This node will be removed soon"
          }
      }
  }' \
  http://localhost:19050/cluster/v2/music/storage/0
</pre>
<pre>{% highlight json %}
{
    "wasModified": true,
    "reason": "ok"
}
{% endhighlight %}</pre>



<h2 id="further-reading">Further reading</h2>
<ul>
  <li>
    Refer to <a href="../operations-selfhosted/admin-procedures.html">administrative procedures</a>
    for configuration and state monitoring / management.
  </li>
  <li>
    Try the <a href="https://github.com/vespa-engine/sample-apps/tree/master/examples/operations/multinode">
    Multinode testing and observability</a> sample app to get familiar with interfaces and behavior.
  </li>
</ul>

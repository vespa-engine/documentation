---
# Copyright Vespa.ai. All rights reserved.
title: "services.xml - 'content'"
---

<pre class="pre-config">
<a href="#content">content</a>
    <a href="#documents">documents</a>
        <a href="#document">document</a>
        <a href="#document-processing">document-processing</a>
    <a href="#min-redundancy">min-redundancy</a>
    <a href="#redundancy">redundancy</a>
    <a href="#coverage-policy">coverage-policy</a>
    <a href="services.html#nodes">nodes</a>
        <a href="#node">node</a>
    <a href="#group">group</a>
        <a href="#distribution">distribution</a>
        <a href="#node">node</a>
        <a href="#group">group</a>
    <a href="#engine">engine</a>
        <a href="#proton">proton</a>
            <a href="#searchable-copies">searchable-copies</a>
            <a href="#tuning-proton">tuning</a>
                <a href="#searchnode">searchnode</a>
                    <a href="#lidspace">lidspace</a>
                        <a href="#lidspace-max-bloat-factor">max-bloat-factor</a>
                    <a href="#requestthreads">requestthreads</a>
                        <a href="#requestthreads-search">search</a>
                        <a href="#requestthreads-persearch">persearch</a>
                        <a href="#requestthreads-summary">summary</a>
                    <a href="#flushstrategy">flushstrategy</a>
                        <a href="#flushstrategy-native">native</a>
                            <a href="#flushstrategy-native-total">total</a>
                                <a href="#flushstrategy-native-total-maxmemorygain">maxmemorygain</a>
                                <a href="#flushstrategy-native-total-diskbloatfactor">diskbloatfactor</a>
                            <a href="#flushstrategy-native-component">component</a>
                                <a href="#flushstrategy-native-component-maxmemorygain">maxmemorygain</a>
                                <a href="#flushstrategy-native-component-diskbloatfactor">diskbloatfactor</a>
                                <a href="#flushstrategy-native-component-maxage">maxage</a>
                            <a href="#flushstrategy-native-transactionlog">transactionlog</a>
                                <a href="#flushstrategy-native-transactionlog-maxsize">maxsize</a>
                            <a href="#flushstrategy-native-conservative">conservative</a>
                                <a href="#flushstrategy-native-conservative-memory-limit-factor">memory-limit-factor</a>
                                <a href="#flushstrategy-native-conservative-disk-limit-factor">disk-limit-factor</a>
                    <a href="#initialize">initialize</a>
                        <a href="#initialize-threads">threads</a>
                    <a href="#feeding">feeding</a>
                        <a href="#feeding-concurrency">concurrency</a>
                        <a href="#feeding-niceness">niceness</a>
                    <a href="#index">index</a>
                        <a href="#index-io">io</a>
                            <a href="#index-io-search">search</a>
                        <a href="#index-warmup">warmup</a>
                            <a href="#index-warmup-time">time</a>
                            <a href="#index-warmup-unpack">unpack</a>
                    <a href="#removed-db">removed-db</a>
                        <a href="#removed-db-prune">prune</a>
                            <a href="#removed-db-prune-age">age</a>
                            <a href="#removed-db-prune-interval">interval</a>
                    <a href="#summary">summary</a>
                        <a href="#summary-io">io</a>
                            <a href="#summary-io-read">read</a>
                        <a href="#summary-store">store</a>
                            <a href="#summary-store-cache">cache</a>
                                <a href="#summary-store-cache-maxsize">maxsize</a>
                                <a href="#summary-store-cache-maxsize-percent">maxsize-percent</a>
                                <a href="#summary-store-cache-compression">compression</a>
                                    <a href="#summary-store-cache-compression-type">type</a>
                                    <a href="#summary-store-cache-compression-level">level</a>
                            <a href="#summary-store-logstore">logstore</a>
                                <a href="#summary-store-logstore-maxfilesize">maxfilesize</a>
                                <a href="#summary-store-logstore-chunk">chunk</a>
                                    <a href="#summary-store-logstore-chunk-maxsize">maxsize</a>
                                    <a href="#summary-store-logstore-chunk-compression">compression</a>
                                        <a href="#summary-store-logstore-chunk-compression-type">type</a>
                                        <a href="#summary-store-logstore-chunk-compression-level">level</a>
            <a href="#sync-transactionlog">sync-transactionlog</a>
            <a href="#flush-on-shutdown">flush-on-shutdown</a>
            <a href="#resource-limits-proton">resource-limits</a>
                <a href="#disk">disk</a>
                <a href="#memory">memory</a>
    <a href="#search">search</a>
        <a href="#query-timeout">query-timeout</a>
        <a href="#visibility-delay">visibility-delay</a>
        <a href="#coverage">coverage</a>
            <a href="#minimum">minimum</a>
            <a href="#min-wait-after-coverage-factor">min-wait-after-coverage-factor</a>
            <a href="#max-wait-after-coverage-factor">max-wait-after-coverage-factor</a>
    <a href="#tuning">tuning</a>
        <a href="#bucket-splitting">bucket-splitting</a>
        <a href="#min-node-ratio-per-group">min-node-ratio-per-group</a>
        <a href="#distribution_type">distribution</a>
        <a href="#maintenance">maintenance</a>
        <a href="#max-document-size">max-document-size</a>
        <a href="#merges">merges</a>
        <a href="#persistence-threads">persistence-threads</a>
        <a href="#resource-limits">resource-limits</a>
        <a href="#visitors">visitors</a>
            <a href="#max-concurrent">max-concurrent</a>
        <a href="#dispatch-tuning">dispatch</a>
            <a href="#max-hits-per-partition">max-hits-per-partition</a>
            <a href="#dispatch-policy">dispatch-policy</a>
            <a href="#prioritize-availability">prioritize-availability</a>
            <a href="#min-active-docs-coverage">min-active-docs-coverage</a>
            <a href="#top-k-probability">top-k-probability</a>
        <a href="#cluster-controller">cluster-controller</a>
            <a href="#init-progress-time">init-progress-time</a>
            <a href="#transition-time">transition-time</a>
            <a href="#max-premature-crashes">max-premature-crashes</a>
            <a href="#stable-state-period">stable-state-period</a>
            <a href="#min-distributor-up-ratio">min-distributor-up-ratio</a>
            <a href="#min-storage-up-ratio">min-storage-up-ratio</a>
            <a href="#groups-allowed-down-ratio">groups-allowed-down-ratio</a>
</pre>



<h2 id="content">content</h2>
<p>
The root element of a Content cluster definition.
Creates a content cluster. A content cluster stores and/or indexes documents.
The xml file may have zero or more such tags.
</p><p>
Contained in <a href="services.html">services</a>.
</p>
<table class="table">
  <thead>
  <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
<tr><th>version</th>
  <td>required</td>
  <td>number</td>
  <td></td>
  <td>1.0 in this version of Vespa</td></tr>
<tr><th>id</th>
  <td>required for multiple clusters</td>
  <td>string</td>
  <td></td>
  <td>
    <p>
      Name of the content cluster.
      If none is supplied, the cluster name will be <code>content</code>.
      Cluster names must be unique within the application,
      if multiple clusters are configured, the name must be set for all but one at minimum.
    </p>
    {% include note.html content="
    Renaming a cluster is the same as dropping the current cluster and adding a new one.
    This makes data unavailable or lost, depending on hosting model.
    Deploying with a changed cluster id will therefore fail with a validation override requirement:
    <code>Content cluster 'music' is removed. This will cause loss of all data in this cluster.
    To allow this add &lt;allow until='yyyy-mm-dd'&gt;content-cluster-removal&lt;/allow&gt; to validation-overrides.xml,
    see https://docs.vespa.ai/en/reference/validation-overrides.html</code>."%}
  </td>
</tr>
</tbody>
</table>
<p>Subelements:</p>
<ul>
  <li><a href="#documents">documents</a> (required)</li>
  <li><a href="#min-redundancy">min-redundancy</a></li>
  <li><a href="#redundancy">redundancy</a></li>
  <li><a href="#coverage-policy">coverage-policy</a></li>
  <li><a href="services.html#nodes">nodes</a></li>
  <li><a href="#group">group</a></li>
  <li><a href="#engine">engine</a></li>
  <li><a href="#search">search</a></li>
  <li><a href="#tuning">tuning</a></li>
</ul>



<h2 id="documents">documents</h2>
<p>
Contained in <a href="#content">content</a>.
Defines which document types should be routed to this content cluster using the default route,
and what documents should be kept in the cluster if the garbage collector runs.
Read more on <a href="../documents.html#document-expiry">expiring documents</a>.
Also have some backend specific configuration for whether documents should be searchable or not.
</p>
<table class="table">
<thead>
<tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
</thead><tbody>
<tr><th>selection</th>
  <td>optional</td>
  <td>string</td>
  <td></td>
  <td>
    <p id="documents.selection">
    A <a href="document-select-language.html">document selection</a>,
    restricting documents that are routed to this cluster.
    Defaults to a selection expression matching everything.
    </p><p>
    This selection can be specified to match document identifier specifics
    that are <em>independent</em> of document types.
    For restrictions that apply only to a <em>specific</em> document type,
    this must be done within that particular document type's
    <a href="#document">document</a> element.
    Trying to use document type references in this selection makes an error during deployment.
    The selection given here will be merged with per-document
    type selections specified within document tags, if any,
    meaning that any document in the cluster must match <em>both</em> selections to be accepted and kept.
    </p><p>
    This feature is primarily used to <a href="../documents.html#document-expiry">expire documents</a>.
    </p>
  </td></tr>
<tr><th>garbage-collection</th>
  <td>optional</td>
  <td>true / false</td>
  <td>false</td>
  <td>
    <p id="documents.garbage-collection">
    If true, regularly verify the documents stored in the cluster to see if
    they belong in the cluster, and delete them if not.
    If false, garbage collection is not run.
    </p>
  </td></tr>
<tr><th style="white-space: nowrap">garbage-collection-interval</th>
  <td>optional</td>
  <td>integer</td>
  <td>3600</td>
  <td>
    <p id="documents.garbage-collection-interval">Time (in seconds) between garbage collection cycles.
      Note that the deletion of documents is spread over this interval, so more resources will be
      used for deleting a set of documents with a small interval than with a larger interval.</p>
  </td></tr>
</tbody>
</table>
<p>Subelements:</p>
<ul>
  <li><a href="#document">document</a> (required)</li>
  <li><a href="#document-processing">document-processing</a> (optional)</li>
</ul>



<h2 id="document">document</h2>
<p>
  Contained in <a href="#documents">documents</a>.
  The document type to be routed to this content cluster.
</p>
<table class="table">
  <thead>
    <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
    <tr><th>type</th>
      <td>required</td>
      <td>string</td>
      <td></td>
      <td>
        <p id="document.type"><a href="schema-reference.html#document">Document type name</a></p>
      </td></tr>
    <tr><th>mode</th>
      <td>required</td>
      <td style="white-space: nowrap">index /<br/>store-only /<br/>streaming</td>
      <td></td>
      <td>
        <p id="document.mode">
          The mode of storing and indexing.
          Refer to <a href="../streaming-search.html">streaming search</a> for <em>store-only</em>, as documents are stored
          the same way for both cases.
        </p><p>
          Changing mode requires an <em>indexing-mode-change</em>
          <a href="validation-overrides.html">validation override</a>,
          and documents must be re-fed.
        </p>
      </td></tr>
    <tr><th>selection</th>
      <td>optional</td>
      <td>string</td>
      <td></td>
      <td>
        <p id="document.selection">
        A <a href="document-select-language.html">document selection</a>,
        restricting documents that are routed to this cluster.
        Defaults to a selection expression matching everything.
        </p><p>
        This selection must apply to fields in <em>this document type only</em>.
        Selection will be merged together with selection for other types and global selection from
        <a href="#documents">documents</a> to form a full expression for what documents belong to this cluster.
        </p>
      </td></tr>
    <tr><th>global</th>
      <td>optional</td>
      <td>true / false</td>
      <td>false</td>
      <td>
        <p id="document.global">
          Set to <em>true</em> to distribute all documents of this type to all nodes
          in the content cluster it is defined.
        </p><p>
          Fields in global documents can be imported into documents to implement joins -
          read more in <a href="../parent-child.html">parent/child</a>.
          Vespa will detect when a new (or outdated) node is added to the cluster
          and prevent it from taking part in searches until it has received all global documents.
        </p><p>
          Changing from <em>false</em> to <em>true</em> or vice versa requires a <em>global-document-change</em>
          <a href="validation-overrides.html">validation override</a>.
          First, <a href="/en/operations-selfhosted/admin-procedures.html#vespa-start-stop-restart">stop services</a>
          on all content nodes.
          Then, deploy with the validation override.
          Finally, <a href="/en/operations-selfhosted/admin-procedures.html#vespa-start-stop-restart">start services</a>
          on all content nodes.
        </p><p>
          Note: <em>global</em> is only supported for <em>mode="index"</em>.
        </p>
      </td></tr>
  </tbody>
</table>



<h2 id="document-processing">document-processing</h2>
<p>
Contained in <a href="#documents">documents</a>.
Vespa Search specific configuration for which document processing cluster and chain to run index preprocessing.
</p>
<table class="table">
  <thead>
    <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
    <tr><th>cluster</th>
      <td>optional</td>
      <td>string</td>
      <td>Container cluster on content node</td>
      <td>
        <p id="document-processing.cluster">
        Name of a <a href="services-docproc.html">document-processing</a>
        container cluster that does index preprocessing.
        Use cluster to specify an alternative cluster, other than the default cluster on content nodes.
        </p>
      </td></tr>
    <tr><th>chain</th>
      <td>optional</td>
      <td>string</td>
      <td><code>indexing</code> chain</td>
      <td>
        <p id="document-processing.chain">
        A document processing chain in the container cluster specified by <em>cluster</em>
        to use for index preprocessing.
        The chain must inherit the <code>indexing</code> chain.
        </p>
      </td></tr>
  </tbody>
</table>
<p>
Example - the container cluster enables <a href="services-docproc.html">document-processing</a>,
referred to by the content cluster:
</p>
<pre>{% highlight xml %}
<container id="my-indexing-cluster" version="1.0">
    <document-api/>
    <document-processing/>
</container>
<content id="music" version="1.0">
    <documents>
        <document-processing cluster="my-indexing-cluster"/>
    </documents>
</content>
{% endhighlight %}</pre>
<p>
  To add document processors either before or after the indexer,
  declare a chain (inherit <em>indexing</em>) in a <em>document-processing</em> container cluster
  and add document processors.
  Annotate document processors with <code>before=indexingStart</code> or <code>after=indexingEnd</code>.
  Configure this cluster and chain as the indexing chain in the content cluster - example:
</p>
<pre>{% highlight xml %}
<container id="my-indexing-cluster" version="1.0">
    <document-api/>
    <document-processing>
        <chain id="my-document-processors"
               inherits="indexing">
            <documentprocessor id="MyDocproc">
                <before>indexingStart</before>
            </documentprocessor>
            <documentprocessor id="MyOtherDocproc">
                <after>indexingEnd</after>
            </documentprocessor>
        </chain>
    </document-processing>
</container>
<content id="music" version="1.0">
    <documents>
        <document-processing cluster="my-indexing-cluster"
                             chain="my-document-processors" />
    </documents>
</content>
{% endhighlight %}</pre>
{% include important.html content='
Note the <a href="services-container.html#document-api">document-api</a> configuration.
Set up this API on the same nodes as <code>document-processing</code> -
find details in <a href="../indexing.html">indexing</a>.' %}


<h2 id="min-redundancy">min-redundancy</h2>

<p>Contained in <a href="#content">content</a>.
The minimum total data copies the cluster will maintain.
This can be set instead of (or in addition to) redundancy to ensure that a
minimum number of copies are always maintained regardless of other configuration.
</p>
<p>
Example: If <em>min-redundancy</em> is 2 and there is 1 content group, there will be 2
data copies in the group (2 copies for the cluster). If the number of groups is
changed to 2 there will be 1 data copy in each group (still 2 copies for the cluster).
</p>
<p>
  Read more about the actual number of replicas when using <a href="#group">groups</a>
  in <a href="/en/elasticity.html#changing-topology">topology change</a>.
</p>
<p><code>min-redundancy</code> can be changed without node restart - replicas will be added or removed automatically.</p>



<h2 id="redundancy">redundancy</h2>
<p>Contained in <a href="#content">content</a>.</p>
{% include note.html content='
Use <a href="#min-redundancy">min-redundancy</a> instead of <code>redundancy</code>.' %}
<p>Vespa Cloud: The number of data copies <em>per group</em>.</p>
<p>Self-managed: The total data copies the cluster will maintain to avoid data loss.</p>
<p>
  Example: with a redundancy of 2, the system tolerates 1 node failure before data becomes unavailable
  (until the system has managed to create new replicas on other online nodes).
</p>
<p>Redundancy can be changed without node restart - replicas will be added or removed automatically.</p>



<h2 id="coverage-policy">coverage-policy</h2>
<p>Contained in <a href="#content">content</a>.</p>
<p>
  Specifies the coverage policy for the content cluster. Valid values are <code>group</code> or <code>node</code>.
  The default value is <code>group</code>.</p>
<p>
  If the policy is <code>group</code> coverage is maintained per group, meaning that when doing maintenance,
  upgrades etc. one group is allowed to be down at a time. If there is only one group in the cluster,
  coverage will be the same as policy <code>node</code>.</p>
<p>
  If the policy is <code>node</code> coverage is maintained on a node level, meaning that when doing
  maintenance, upgrades etc. coverage will be maintained on a node level,  so in practice 1 node in the whole
  cluster is allowed to be down at a time.
</p>
<p>
  When having several groups the common reason for changing policy away from the default <code>group</code> policy
  is when the load added to the remaining groups will increase too much when a whole group is allowed to
  go down. In that case it will be better to use the <code>node</code> policy, as taking one node at a time
  will give just a minor increase in load.
</p>


<h2 id="node">node</h2>

<p>Contained in <a href="services.html#nodes">nodes</a> or <a href="#group">group</a>.
Configures a content node to the cluster, see <a href="services.html#node">node</a>
in the general services.xml documentation.</p>

<p>Additional node attributes for content nodes:</p>

<table class="table">
<thead>
<tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
</thead>
<tbody>
<tr>
  <th style="white-space: nowrap">distribution-key</th>
  <td>required</td>
  <td>integer</td>
  <td></td>
  <td>
    <p id="node.distribution-key">
    The unique data distribution id of this node. This <b>must</b> remain unchanged for the host's lifetime.
    Distribution keys of a fresh system should be contiguous and start from zero.</p>

    <p>Distribution keys are used to identify nodes and groups for the
    <a href="../content/idealstate.html">distribution algorithm</a>.
    If a node changes distribution key, the distribution algorithm regards it as a new node,
    so buckets are redistributed.</p>
  </td>
</tr>
<tr>
  <th>capacity</th>
  <td>optional</td>
  <td>double</td>
  <td>1</td>
  <td>
    <p id="node.capacity">
      {% include deprecated.html content="Capacity of this node, relative to other nodes.
      A node with capacity 2 will get double the data and feed requests of a node with capacity 1.
      This feature is deprecated and expert mode only. Don't use in production, Vespa assumes
      homogenous cluster capacity."%}
    </p>
  </td>
</tr>
<tr>
  <th>baseport</th>
  <td>optional</td>
  <td>integer</td>
  <td></td>
  <td>
      <p id="node.baseport">baseport
      The first port in the port range allocated by this node.
      </p>
  </td>
</tr>
</tbody>
</table>



<h2 id="group">group</h2>

<p>Contained in <a href="#content">content</a> or
<a href="#group">group</a> - groups can be nested.
Defines the <a href="../elasticity.html#grouped-distribution">hierarchical structure</a> of the cluster.
Can not be used in conjunction with the <a href="services.html#nodes">nodes</a> element.
Groups can contain other groups or nodes, but not both.
There can only be a single level of leaf groups under the top group.</p>

<table class="table">
<thead>
<tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
</thead><tbody>
<tr><th style="white-space: nowrap">distribution-key</th>
  <td>required</td>
  <td>integer</td>
  <td></td>
  <td>
    <p id="group.distribution-key">
      Sets the distribution key of a group. It is not allowed to change this for a given group.
      Group distribution keys only need to be unique among groups that share the same parent group.
    </p>
  </td></tr>
<tr><th>name</th>
  <td>required</td>
  <td>string</td>
  <td></td>
  <td><p id="group.name">The name of the group, used for access from status pages and the like.</p>
  </td></tr>
</tbody>
</table>
{% include important.html       content="
There is no deployment-time verification that the distribution key remains unchanged for any given node or group.
Consequently, take great care when modifying the set of nodes in a content cluster.
Assigning a new distribution key to an existing node is undefined behavior;
Best case, the existing data will be temporarily unavailable until the error has been corrected.
Worst case, risk crashes or data loss."%}
<p>
See <a href="../performance/sizing-search.html">Vespa Serving Scaling Guide</a>
for when to consider using grouped distribution
and <a href="../performance/sizing-examples.html">Examples</a> for example deployments
using flat and grouped distribution.
</p>



<h2 id="distribution">distribution (in group)</h2>
<p>
Contained in <a href="#group">group</a>.
Defines the data distribution to subgroups of this group.
<em>distribution</em> should not be in the lowest level group containing storage nodes,
as here the ideal state algorithm is used directly.
In higher level groups, <em>distribution</em> is mandatory.
</p>
<table class="table">
  <thead>
  <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
<tr><th style="white-space: nowrap">partitions</th>
  <td>required if there are subgroups in the group</td>
  <td>string</td>
  <td></td>
  <td>
    String conforming to the partition specification:
    <table class="table">
      <thead>
      <tr><th>Partition specification</th><th>Description</th></tr>
      </thead><tbody>
        <tr><th>*</th><td>Distribute all copies over 1 of N groups</td></tr>
        <tr><th>1|*</th><td>Distribute all copies over 2 of N groups</td></tr>
        <tr><th>1|1|*</th><td>Distribute all copies over 3 of N groups</td></tr>
    </tbody>
    </table>
  </td></tr>
</tbody>
</table>
<p>
  The partition specification is used to evenly distribute content copies across groups.
  Set a number or <code>*</code> per group separated by pipes (e.g. <code>1|*</code> for two groups).
  See <a href="../performance/sizing-examples.html">sample deployment configurations</a>.
</p>



<h2 id="engine">engine</h2>
<p>
Contained in <a href="#content">content</a>.
Specify the content engine to use, and/or adjust tuning parameters for the engine.
Allowed engines are <code>proton</code> and <code>dummy</code>,
the latter being used for debugging purposes. If no engine is given, proton is used.
Sub-element: <a href="#proton">proton</a>.
</p>



<h2 id="proton">proton</h2>
<p>
Contained in <a href="#engine">engine</a>.
If specified, the content cluster will use the Proton content engine.
This engine supports storage, indexed search and secondary indices.
Optional sub-elements are <a href="#searchable-copies">searchable-copies</a>,
<a href="#tuning-proton">tuning</a>,
<a href="#sync-transactionlog">sync-transactionlog</a>,
<a href="#flush-on-shutdown">flush-on-shutdown</a>, and
<a href="#resource-limits-proton">resource-limits (in proton)</a>.
</p>



<h2 id="searchable-copies">searchable-copies</h2>
<p>
Contained in <a href="#proton">proton</a>.
Default value is 2, or <a href="#redundancy">redundancy</a>, if lower.
If set to less than redundancy, only some of the stored copies are ready for searching at any time.
This means that node failures causes temporary data unavailability
while the alternate copies are being indexed for search.
The benefit is using less memory, trading off availability during transitions.
Refer to <a href="../proton.html#bucket-move">bucket move</a>.
</p><p>
If updating documents or using <a href="#documents">document selection</a> for garbage collection,
consider setting <a href="schema-reference.html#attribute">fast-access</a>
on the subset of attribute fields used for this to make sure that these attributes are always kept
in memory for fast access.
Note that this is only useful if <code>searchable-copies</code> is less than <code>redundancy</code>.
Read more in <a href="../proton.html">proton</a>.
</p><p>
  <code>searchable-copies</code> can be changed without node restart. Note that when reducing
  <code>searchable-copies</code> resource usage will not be reduced until content nodes are restarted.
</p>



<h2 id="tuning-proton">tuning</h2>
<p>
Contained in <a href="#proton">proton</a>, optional.
Tune settings for the search nodes in a content cluster - sub-element:
</p>
<table class="table">
    <thead>
    <tr>
        <th>Element</th><th>Required</th><th>Quantity</th>
    </tr>
    </thead><tbody>
    <tr>
        <td><a href="#searchnode">searchnode</a></td>
        <td class="required">No</td>
        <td>Zero or one</td>
    </tr>
    </tbody>
</table>



<h2 id="searchnode">searchnode</h2>
<p>
Contained in <a href="#tuning-proton">tuning</a>, optional.
Tune settings for search nodes in a content cluster - sub-elements:
</p>
<table class="table">
    <thead>
    <tr>
        <th>Element</th><th>Required</th><th>Quantity</th>
    </tr>
    </thead><tbody>
    <tr>
        <td><a href="#lidspace">lidspace</a></td>
        <td class="required">No</td>
        <td>Zero or one</td>
    </tr><tr>
    <tr>
        <td><a href="#requestthreads">requestthreads</a></td>
        <td class="required">No</td>
        <td>Zero or one</td>
    </tr><tr>
        <td><a href="#flushstrategy">flushstrategy</a></td>
        <td class="required">No</td>
        <td>Zero or one</td>
    </tr><tr>
        <td><a href="#initialize">initialize</a></td>
        <td class="required">No</td>
        <td>Zero or one</td>
    </tr><tr>
        <td><a href="#feeding">feeding</a></td>
        <td class="required">No</td>
        <td>Zero or one</td>
    </tr><tr>
        <td><a href="#index">index</a></td>
        <td class="required">No</td>
        <td>Zero or one</td>
    </tr><tr>
        <td><a href="#summary">summary</a></td>
        <td class="required">No</td>
        <td>Zero or one</td>
    </tr>
    </tbody>
</table>
<pre>{% highlight xml %}
<tuning>
    <searchnode>
        <lidspace></lidspace>
        <requestthreads></requestthreads>
        <flushstrategy></flushstrategy>
        <initialize></initialize>
        <feeding></feeding>
        <index></index>
        <summary></summary>
    </searchnode>
</tuning>
{% endhighlight %}</pre>



<h2 id="requestthreads">requestthreads</h2>
<p>
  Contained in <a href="#searchnode">searchnode</a>, optional.
  Tune the number of request threads used on a content node,
  see <a href="../performance/sizing-search.html#thread-configuration">thread-configuration</a> for details.
  Sub-elements:
</p>
<table class="table">
  <thead>
  <tr>
    <th>Element</th><th>Required</th><th>Default</th><th>Description</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <th>search</th>
    <td>Optional</td>
    <td>64</td>
    <td><p id="requestthreads-search">Number of search threads.</p></td>
  </tr>
  <tr>
    <th>persearch</th>
    <td>Optional</td>
    <td>1</td>
    <td><p id="requestthreads-persearch">
      Number of search threads.
      Number of search threads used per search,
      see the <a href="../performance/sizing-search.html">Vespa serving scaling guide</a>
      for an introduction of using multiple threads per search per node to reduce query latency.
      Number of threads per search can be adjusted down per <em>rank-profile</em>
      using <a href="schema-reference.html#num-threads-per-search">num-threads-per-search</a>.</p></td>
  </tr>
  <tr>
    <th>summary</th>
    <td>Optional</td>
    <td>16</td>
    <td><p id="requestthreads-summary">Number of summary threads.</p></td>
  </tr>
  </tbody>
</table>
<pre>{% highlight xml %}
<requestthreads>
    <search>64</search>
    <persearch>1</persearch>
    <summary>16</summary>
</requestthreads>
{% endhighlight %}</pre>



<h2 id="flushstrategy">flushstrategy</h2>
<p>
Contained in <a href="#searchnode">searchnode</a>, optional.
Tune the <em>native</em>-strategy for flushing components to disk -
a smaller number means more frequent flush:
</p>
<ul>
  <li><em>Memory gain</em> is how much memory can be freed by flushing a component</li>
  <li><em>Disk gain</em> is how much disk space can be freed by flushing a
    component (typically by using compaction)</li>
</ul>
<p>
Refer to <a href="../proton.html#proton-maintenance-jobs">Proton maintenance jobs</a>.
Optional sub-elements:
</p>
<ul>
    <li id="flushstrategy-native"><code>native</code>:
    <ul>
        <li id="flushstrategy-native-total"><code>total</code>
        <ul>
            <li id="flushstrategy-native-total-maxmemorygain"><code>maxmemorygain</code>:
            The total maximum memory gain (in bytes) for <em>all</em> components
            before running flush, default 4294967296 (4 GB)
            </li>
            <li id="flushstrategy-native-total-diskbloatfactor"><code>diskbloatfactor</code>:
            Trigger flush if the total disk gain (in bytes) for <em>all</em> components is larger
            than the factor times current total disk usage, default 0.25
            </li>
        </ul>
        </li><li id="flushstrategy-native-component"><code>component</code>
        <ul>
            <li id="flushstrategy-native-component-maxmemorygain"><code>maxmemorygain</code>:
            The maximum memory gain (in bytes) by <em>a single</em> component
            before running flush, default 1073741824 (1 GB)
            </li>
            <li id="flushstrategy-native-component-diskbloatfactor"><code>diskbloatfactor</code>:
            Trigger flush if the disk gain (in bytes) by <em>a single</em> component is larger than
            the given factor times the current disk usage by that component, default 0.25
            </li>
            <li id="flushstrategy-native-component-maxage"><code>maxage</code>:
            The maximum age (in seconds) of unflushed content for a single component
            before running flush, default 111600 (31h)
            </li>
        </ul>
        </li><li id="flushstrategy-native-transactionlog"><code>transactionlog</code>
        <ul>
            <li id="flushstrategy-native-transactionlog-maxsize"><code>maxsize</code>:
            The total maximum size (in bytes) of <a href="../proton.html#transaction-log">transaction logs</a>
            for all document types before running flush, default 21474836480 (20 GB)
            </li>
        </ul>
        </li><li id="flushstrategy-native-conservative"><code>conservative</code>
        <ul>
            <li id="flushstrategy-native-conservative-memory-limit-factor"><code>memory-limit-factor</code>:
            When <a href="#resource-limits-proton">resource-limits (in proton)</a> for memory is reached,
            flush more often by downscaling <code>total.maxmemorygain</code> and
            <code>component.maxmemorygain</code>, default 0.5
            </li>
            <li id="flushstrategy-native-conservative-disk-limit-factor"><code>disk-limit-factor</code>:
            When <a href="#resource-limits-proton">resource-limits (in proton)</a> for disk is reached,
            flush more often by downscaling <code>transactionlog.maxsize</code>, default 0.5
            </li>
        </ul>
        </li>
    </ul>
    </li>
</ul>
<pre>{% highlight xml %}
<flushstrategy>
    <native>
        <total>
            <maxmemorygain>4294967296</maxmemorygain>
            <diskbloatfactor>0.2</diskbloatfactor>
        </total>
        <component>
            <maxmemorygain>1073741824</maxmemorygain>
            <diskbloatfactor>0.2</diskbloatfactor>
            <maxage>111600</maxage>
        </component>
        <transactionlog>
            <maxsize>21474836480</maxsize>
        </transactionlog>
        <conservative>
            <memory-limit-factor>0.5</memory-limit-factor>
            <disk-limit-factor>0.5</disk-limit-factor>
        </conservative>
    </native>
</flushstrategy>
{% endhighlight %}</pre>



<h2 id="initialize">initialize</h2>
<p>
Contained in <a href="#searchnode">searchnode</a>, optional.
Tune settings related to how the search node (proton) is initialized. Optional sub-elements:
</p>
<ul>
  <li id="initialize-threads"><code>threads</code>:
    The number of initializer threads used for loading structures from disk at proton startup.
    The threads are shared between document databases when the value is larger than 0.
    Default value is the number of document databases + 1.
    <ul>
      <li>When set to larger than 1, document databases are initialized in parallel</li>
      <li>When set to 1, document databases are initialized in sequence</li>
      <li>When set to 0, 1 separate thread is used per document database,
      and they are initialized in parallel.</li>
    </ul>
  </li>
</ul>
<pre>{% highlight xml %}
<initialize>
    <threads>2</threads>
</initialize>
{% endhighlight %}</pre>

<h2 id="lidspace">lidspace</h2>
<p>
Contained in <a href="#searchnode">searchnode</a>, optional.
Tune settings related to how lidspace is managed. Optional sub-elements:
</p>
<ul>
  <li id="lidspace-max-bloat-factor"><code>max-bloat-factor</code>:
    Maximum bloat allowed before lidspace compaction is started. Compaction is moving a document
    from a high lid to a lower lid. Cost is similar to feeding a document and removing it.
    Also see description in <a href="../proton.html#lid-space-compaction">lidspace compaction maintenance job</a>.
    Default value is 0.01 or 1% of total lidspace. Will be increased to target of 0.50 or 50%.
  </li>
</ul>
<pre>{% highlight xml %}
<lidspace>
    <max-bloat-factor>0.5</max-bloat-factor>
</lidspace>
{% endhighlight %}</pre>



<h2 id="feeding">feeding</h2>
<p>
Contained in <a href="#searchnode">searchnode</a>, optional.
Tune <a href="../proton.html">proton</a> settings for feed operations. Optional sub-elements:
</p>
<ul>
  <li id="feeding-concurrency"><code>concurrency</code>:
    A number between 0.0 and 1.0 that specifies the concurrency when handling feed operations, default 0.5.
    When set to 1.0, all cores on the cpu can be used for feeding. Changing this value requires restart of
    node to take effect.
  </li>
  <li id="feeding-niceness"><code>niceness</code>:
    A number between 0.0 and 1.0 that specifies the niceness of the feeding threads, default 0.0 => not any nicer than anyone else.
    Increasing this number will reduce priority of feeding compared to search. The real world effect is hard to predict as the magic
    exists in the OS level scheduler. Changing this value requires restart of node to take effect.
  </li>
</ul>
<pre>{% highlight xml %}
<feeding>
    <concurrency>0.8</concurrency>
    <niceness>0.5</niceness>
</feeding>
{% endhighlight %}</pre>



<h2 id="index">index</h2>
<p>
Contained in <a href="#searchnode">searchnode</a>, optional.
Tune various aspect with the handling of disk and memory indexes. Optional sub-elements:
</p>
<ul>
    <li id="index-io"><code>io</code>
    <ul>
        <li id="index-io-search"><code>search</code>:
        Controls io read options used during search,
        values={mmap,populate}, default <code>mmap</code>. Using <code>populate</code> will eagerly touch all pages when index is loaded (after re-start or after index fusion is complete).
        </li>
    </ul>
    </li>
    <li id="index-warmup"><code>warmup</code>
    <ul>
        <li id="index-warmup-time">
          <p>
            <code>time</code>:
            Specifies in seconds how long the index shall be warmed up before being switched in for serving.
            During warmup, it will receive queries and posting lists will be iterated, but results ignored
            as they are duplicates of the live index. This will pull in the most important ones in the cache.
            However, as warming up an index will occupy more memory, do not turn it on unless you suspect you need it.
            And always benchmark to see if it is worth it.
          </p>
          <p>
            It's only potentially relevant for fields with indexing setting <a href="../schemas.html#document-fields">index</a>,
            which have regular disk based indexes,
            and where the disk indexes are merged/fused in the background.
            When switching the index, warmup can be used.
            Also note that <a href="state-v1.html#state-v1-health">state-v1-health</a>
            is independent of <code>warmup</code> - the node can be "up" before warmup.
          </p>
        </li>
        <li id="index-warmup-unpack"><code>unpack</code>:
        Controls whether all posting features are pulled in to the cache, or only the most important.
        values={true, false}, default false.
        </li>
    </ul>
    </li>
</ul>
<pre>{% highlight xml %}
<index>
    <io>
        <search>mmap</search>
    </io>
    <warmup>
        <time>60</time>
        <unpack>true</unpack>
    </warmup>
</index>
{% endhighlight %}</pre>


<h2 id="removed-db">removed-db</h2>
<p>
Contained in <a href="#searchnode">searchnode</a>, optional.
Tune various aspect of the db of removed documents. Optional sub-elements:
</p>
<ul>
    <li id="removed-db-prune"><code>prune</code>
    <ul>
        <li id="removed-db-prune-age"><code>age</code>:
        Specifies how long (in seconds) we must remember removed documents before we can prune them away.
        Default is 2 weeks.
        This sets the upper limit on how long a node can be down and still be accepted back in the system,
        without having the index wiped.
        There is no point in having this any higher than the age of the documents.
        If corpus is re-fed every day, there is no point in having this longer than 24 hours.
        </li>
        <li id="removed-db-prune-interval"><code>interval</code>:
        Specifies how often (in seconds) to prune old documents. Default is 3.36 hours (prune age / 100).
        No need to change default. Exposed here for reference and for testing.
        </li>
    </ul>
    </li>
</ul>
<pre>{% highlight xml %}
<removed-db>
    <prune>
        <age>86400</age>
    </prune>
</removed-db>
{% endhighlight %}</pre>



<h2 id="summary">summary</h2>
<p>
Contained in <a href="#searchnode">searchnode</a>, optional.
Tune various aspect with the handling of document summary. Optional sub-elements:
</p>
<ul>
  <li id="summary-io"><code>io</code>
    <ul>
      <li id="summary-io-read"><code>read</code>:
        Controls io read options used during reading of stored documents.
        Values are <code>directio</code> <code>mmap</code> <code>populate</code>.
        Default is <code>mmap</code>. <code>populate</code> will do an eager mmap and touch all pages.</li>
    </ul>
  </li>
  <li id="summary-store"><code>store</code>
    <ul>
      <li id="summary-store-cache"><code>cache</code>: Used to tune the cache used by the document store.
        Enabled by default, using up to 5% of available memory.
        <ul>
          <li id="summary-store-cache-maxsize"><code>maxsize</code>:
          The maximum size of the cache in bytes.
          If set, it takes precedence over <a href="#summary-store-cache-maxsize-percent">maxsize-percent</a>.
          Default is unset. </li>
          <li id="summary-store-cache-maxsize-percent"><code>maxsize-percent</code>:
          The maximum size of the cache in percent of available memory. Default is 5%.</li>
          <li id="summary-store-cache-compression"><code>compression</code>
            <ul>
              <li id="summary-store-cache-compression-type"><code>type</code>:
                The compression type of the documents while in the cache.
                Possible values are , <code>none</code> <code>lz4</code> <code>zstd</code>.
                Default is <code>lz4</code></li>
              <li id="summary-store-cache-compression-level"><code>level</code>:
                The compression level of the documents while in cache.
                Default is 6</li>
            </ul>
          </li>
        </ul>
      </li>
      <li id="summary-store-logstore"><code>logstore</code>:
        Used to tune the actual document store implementation (log-based).
        <ul>
          <li id="summary-store-logstore-maxfilesize"><code>maxfilesize</code>:
            The maximum size (in bytes) per summary file on disk. Default value is 1GB.
            <a href="../proton.html#document-store-compaction">document-store-compaction</a></li>
          <li id="summary-store-logstore-chunk"><code>chunk</code>
            <ul>
              <li id="summary-store-logstore-chunk-maxsize"><code>maxsize</code>:
                Maximum size (in bytes) of a chunk. Default value is 64KB.</li>
              <li id="summary-store-logstore-chunk-compression"><code>compression</code>
                <ul>
                  <li id="summary-store-logstore-chunk-compression-type"><code>type</code>:
                    Compression type for the documents, <code>none</code> <code>lz4</code> <code>zstd</code>.
                    Default is <code>zstd</code>.</li>
                  <li id="summary-store-logstore-chunk-compression-level"><code>level</code>:
                    Compression level for the documents. Default is 3.</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li> <!-- summary-store-logstore -->
    </ul>
  </li> <!-- summary-store -->
</ul>
<pre>{% highlight xml %}
<summary>
    <io>
        <read>directio</read>
    </io>
    <store>
        <cache>
            <maxsize-percent>5</maxsize-percent>
            <compression>
                <type>none</type>
            </compression>
        </cache>
        <logstore>
            <chunk>
                <maxsize>16384</maxsize>
                <compression>
                    <type>zstd</type>
                    <level>3</level>
                </compression>
            </chunk>
        </logstore>
    </store>
</summary>
{% endhighlight %}</pre>



<h2 id="flush-on-shutdown">flush-on-shutdown</h2>
<p>
Contained in <a href="#proton">proton</a>. Default value is true.
If set to true, search nodes will flush a set of components (e.g. memory index, attributes) to disk
before shutting down such that the time it takes to flush these components
plus the time it takes to replay the <a href="../proton.html#transaction-log">transaction log</a>
after restart is as low as possible.
The time it takes to replay the transaction log depends on the amount of data to replay,
so by flushing, some components before restart the transaction log will be pruned,
and we reduce the replay time significantly.
Refer to <a href="../proton.html#proton-maintenance-jobs">Proton maintenance jobs</a>.
</p>

<h2 id="sync-transactionlog">sync-transactionlog</h2>
<p>
  Contained in <a href="#proton">proton</a>. Default value is true.
  If true, the transactionlog is synced to disk after every write.
  This enables the transactionlog to survive power failures and kernel panic.
  The sync cost is amortized over multiple feed operations.
  The faster you feed the more operations it is amortized over.
  So with a local disk this is not known to be a performance issue.
  However, if using NAS (Network Attached Storage) like EBS on AWS one can see significant
  feed performance impact. For one particular case, turning off sync-transactionlog for EBS gave a 60x improvement.
</p>
<p>
    With sync-transactionlog turned off, the risk of losing data depends on the kernel's
    <a href="https://www.kernel.org/doc/html/latest/admin-guide/sysctl/vm.html#dirty-background-bytes">sysctl settings.</a>
    For example, this is a common default:
</p>
<pre>
# sysctl -a
...
vm.dirty_expire_centisecs = 3000
vm.dirty_ratio = 20
vm.dirty_writeback_centisecs = 500
...
</pre>
<p>
    With this configuration, the worse case scenario is to lose 35 seconds worth of transactionlog, but no more than 1/20
    of the free memory. Because kernel flusher threads wake up every 5s (dirty_writeback_centisecs) and write data
    older than 30s (dirty_expire_centisecs) from memory to disk. But if un-synced data exceeds 1/20 of the free memory,
    the Vespa process will sync it (dirty_ratio).
</p>
<p>
    The above also assumes that all copies of the data are lost at the same time <b>and</b> that kernels on all these nodes
    flush at the same time: realistic scenario only with one copy.
</p>
<p>
    Adjust these <a href="https://www.kernel.org/doc/html/latest/admin-guide/sysctl/vm.html#dirty-background-bytes">sysctl settings</a>
    to manage the trade-off between data loss and performance. You'll see more in those kernel docs: for example,
    thresholds can be expressed in bytes.
</p>


<h2 id="resource-limits-proton">resource-limits (in proton)</h2>
<p>
Contained in <a href="#proton">proton</a>.
Specifies resource limits used by proton to reject both external and internal write operations (on this content node) when a limit is reached.
</p>
{% include warning.html    content="
These proton limits should almost never be changed directly.
Instead, change <a href='#resource-limits'>resource-limits</a>
that controls when external write operations are blocked in the entire content cluster.
Be aware of the risks of tuning resource limits as seen in the link."%}
<p>
The local proton limits are derived from the cluster limits if not specified, using this formula:
</p>
<p class="equation-container"><!-- depends on mathjax -->
$${L_{proton}} = {L_{cluster}} + \frac{1-L_{cluster}}{2}$$
</p>
<table class="table">
  <thead>
    <tr><th>Element</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
    <tr><th>disk</th>
      <td>optional</td>
      <td>float<br/>[0, 1]</td>
      <td>0.875</td>
      <td>
        <p id="disk-proton">
          Fraction of total space on the disk partition used before put and update operations are rejected
        </p>
      </td></tr>
    <tr><th>memory</th>
      <td>optional</td>
      <td>float<br/>[0, 1]</td>
      <td>0.9</td>
      <td>
        <p id="memory-proton">
          Fraction of physical memory that can be resident memory in anonymous mapping by proton
          before put and update operations are rejected
        </p>
      </td></tr>
  </tbody>
</table>
<p>Example:</p>
<pre>{% highlight xml %}
<proton>
    <resource-limits>
        <disk>0.83</disk>
        <memory>0.82</memory>
{% endhighlight %}</pre>



<h2 id="search">search</h2>
<p>
Contained in <a href="#content">content</a>, optional.
Declares search configuration for this content cluster. Optional sub-elements are
<a href="#query-timeout">query-timeout</a>,
<a href="#visibility-delay">visibility-delay</a> and
<a href="#coverage">coverage</a>.
</p>



<h2 id="query-timeout">query-timeout</h2>
<p>
Contained in <a href="#search">search</a>.
Specifies the query timeout in seconds for queries against the search interface on the content nodes.
The default is 0.5 (500ms), the max is 600.0.
For query timeout also see the request parameter <a href="query-api-reference.html#timeout">timeout</a>.
</p>
{% include note.html content='One can not override this value using the
<a href="query-api-reference.html#timeout">timeout</a> request parameter.' %}



<h2 id="visibility-delay">visibility-delay</h2>
<p>Contained in <a href="#search">search</a>. Default 0, max 1, seconds.</p>
<p>
This setting controls the TTL caching for <a href="../parent-child.html">parent-child</a> imported fields.
See <a href="../performance/feature-tuning.html#parent-child-and-search-performance">feature tuning</a>.
</p>



<h2 id="coverage">coverage</h2>
<p>Contained in <a href="#search">search</a>.
Declares search coverage configuration for this content cluster. Optional sub-elements are
<a href="#minimum">minimum</a>,
<a href="#min-wait-after-coverage-factor">min-wait-after-coverage-factor</a> and
<a href="#max-wait-after-coverage-factor">max-wait-after-coverage-factor</a>.
Search coverage configuration controls how many nodes the query dispatcher process
should wait for, trading search coverage versus search performance.
</p>


<h2 id="minimum">minimum</h2>
<p>
Contained in <a href="#coverage">coverage</a>.
Declares the minimum search coverage required before returning the results of a query.
This number is in the range <code>[0, 1]</code>, with 0 being no coverage and 1 being full coverage.
</p><p>
The default is 1; unless configured otherwise a query will not return
until all search nodes have responded within the specified timeout.
</p>



<h2 id="min-wait-after-coverage-factor">min-wait-after-coverage-factor</h2>
<p>
Contained in <a href="#coverage">coverage</a>.
Declares the minimum time for a query to wait for full coverage once the declared
<a href="#minimum">minimum</a> has been reached. This number is a factor that is
multiplied with the time remaining at the time of reaching minimum coverage.
</p><p>
The default is 0; unless configured otherwise a query will return as soon as the
minimum coverage has been reached, and the remaining search nodes appear to be lagging.
</p>



<h2 id="max-wait-after-coverage-factor">max-wait-after-coverage-factor</h2>
<p>
Contained in <a href="#coverage">coverage</a>.
Declares the maximum time for a query to wait for full coverage once the declared
<a href="#minimum">minimum</a> has been reached.
This number is a factor that is multiplied with the time remaining
at the time of reaching minimum coverage.
</p><p>
The default is 1; unless configured otherwise a query is allowed to wait its full
timeout for full coverage even after reaching the minimum.
</p>


<h2 id="tuning">tuning</h2>
<p>
Contained in <a href="#content">content</a>, optional. Optional tuning parameters are:
<a href="#bucket-splitting">bucket-splitting</a>,
<a href="#min-node-ratio-per-group">min-node-ratio-per-group</a>,
<a href="#cluster-controller">cluster-controller</a>,
<a href="#dispatch-tuning">dispatch</a>,
<a href="#distribution_type">distribution</a>,
<a href="#maintenance">maintenance</a>,
<a href="#max-document-size">max-document-size</a>,
<a href="#merges">merges</a>,
<a href="#persistence-threads">persistence-threads</a> and
<a href="#visitors">visitors</a>.
</p>



<h2 id="bucket-splitting">bucket-splitting</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
The <a href="../content/buckets.html">bucket</a> is the fundamental unit of distribution
and management in a content cluster.
Buckets are auto-split, no need to configure for most applications.
</p>
<table class="table">
  <thead>
    <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
    <tr><th style="white-space: nowrap">max-documents</th>
      <td>optional</td>
      <td>integer</td>
      <td>1024</td>
      <td>
        <p id="max-documents">
        Maximum number of documents per content bucket.
        Buckets are split in two if they have more documents than this.
        Keep this value below 16K.
        </p>
      </td></tr>
    <tr><th>max-size</th>
      <td>optional</td>
      <td>integer</td>
      <td>32MiB</td>
      <td>
        <p id="max-size">
        Maximum size (in bytes) of a bucket.
        This is the sum of the serialized size of all documents kept in the bucket.
        Buckets are split in two if they have a larger size than this.
        Keep this value below 100 MiB.
        </p>
      </td></tr>
    <tr><th>minimum-bits</th>
      <td>optional</td>
      <td>integer</td>
      <td></td>
      <td>
        <p id="minimum-bits">
        Override the ideal distribution bit count configured for this cluster.
        Prefer to use the <a href="#distribution_type">distribution type</a>
        setting instead if the default distribution bit count does not fit the cluster.
        This variable is intended for testing and to work around possible distribution bit issues.
        Most users should not need this option.
        </p>
      </td></tr>
  </tbody>
</table>



<h2 id="min-node-ratio-per-group">min-node-ratio-per-group</h2>
{% include important.html content='This is configuration for the cluster controller.
Most users are normally looking for
<a href="#min-active-docs-coverage">min-active-docs-coverage</a>
which controls how many nodes can be down before query load is routed to other groups.' %}
<p>
Contained in <a href="#tuning">tuning</a>.
States a lower bound requirement on the ratio of nodes within <em>individual</em> <a href="#group">groups</a>
that must be online and able to accept traffic before the entire group is automatically taken out of service.
Groups are automatically brought back into service when the availability
of its nodes has been restored to a level equal to or above this limit.
</p><p>
Elastic content clusters are often configured to use multiple groups
for the sake of horizontal traffic scaling and/or data availability.
The content distribution system will try to ensure a configured number of replicas is always present
within a group in order to maintain data redundancy.
If the number of available nodes in a group drops too far,
it is possible for the remaining nodes in the group to not have sufficient capacity to take over
storage and serving for the replicas they now must assume responsibility for.
Such situations are likely to result in increased latencies and/or feed rejections caused by resource exhaustion.
Setting this tuning parameter allows the system to instead automatically take down the remaining nodes in the group,
allowing feed and query traffic to fail completely over to the remaining groups.
</p><p>
Valid parameter is a decimal value in the range [0, 1].
Default is 0, which means that the automatic group out-of-service functionality will <em>not</em> automatically take effect.
</p><p>
Example: assume a cluster has been configured with <em>n</em> groups of 4 nodes each
and the following tuning config:
</p>
<pre>{% highlight xml %}
<tuning>
    <min-node-ratio-per-group>0.75</min-node-ratio-per-group>
</tuning>
{% endhighlight %}</pre>
<p>
This tuning allows for 1 node in a group to be down. If 2 or more nodes go down,
all nodes in the group will be marked as down, letting the <em>n-1</em> remaining groups handle all the traffic.
</p><p>
This configuration can be changed live as the system is running and altered limits will take effect immediately.
</p>



<h2 id="distribution_type">distribution (in tuning)</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
Tune the distribution algorithm used in the cluster.
</p>
<table class="table">
  <thead>
  <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
<tr><th>type</th>
  <td>optional</td>
  <td>loose | strict | legacy</td>
  <td>loose</td>
  <td>
    <p>When the number of a nodes configured in a system changes over certain limits, the system will
      automatically trigger major redistributions of documents. This is to ensure that
      the number of buckets is appropriate for the number of nodes in the cluster. This enum
      value specifies how aggressive the system should be in triggering such distribution changes.</p>
    <p>The default of <code>loose</code> strikes a balance between rarely altering the distribution
      of the cluster and keeping the skew in document distribution low. It is recommended that you
      use the default mode unless you have empirically observed that it causes too much skew in load
      or document distribution.</p>
    <p>Note that specifying <code>minimum-bits</code> under <a href="#bucket-splitting">bucket-splitting</a>
      overrides this setting and effectively "locks" the distribution in place.</p>
  </td></tr>
</tbody>
</table>



<h2 id="maintenance">maintenance</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
Controls the running time of the bucket maintenance process.
Bucket maintenance verifies bucket content for corruption.
Most users should not need to tweak this.
</p>
<table class="table">
  <thead>
  <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
<tr><th>start</th>
  <td>required</td>
  <td>HH.MM</td>
  <td></td>
  <td>
    Start of daily maintenance window, e.g. 02:00
  </td></tr>
<tr><th>stop</th>
  <td>required</td>
  <td>HH.MM</td>
  <td></td>
  <td>
    End of daily maintenance window, e.g. 05:00
  </td></tr>
<tr><th>high</th>
  <td>required</td>
  <td>HH.MM</td>
  <td></td>
  <td>
    Day of week for starting full file verification cycle, e.g. monday.
    The full cycle is more costly than partial file verification
  </td></tr>
</tbody>
</table>


<h2 id="max-document-size">max-document-size</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
Specifies max document size in the content cluster, measured as the uncompressed size
of a document operation arriving over the wire by the distributor service. The limit will
be used for all document types. A document larger than this limit will be rejected by the
distributor. Note that some document operations that don't contain the entire document, like
<a href="../document-api-guide.html#document-updates">document updates</a>
might increase the size of a document above this limit.
</p>
<p>
Valid values are numbers including a unit (e.g. <em>10MiB</em>) and the value must be between 1Mib and 2048 Mib (inclusive).
Values will be rounded to nearest MiB, so using MiB as a unit is preferrable.
It is strongly recommended to make sure this is not set too high, 10 MiB is a reasonable setting for most use cases,
setting it above 100 MiB is not recommended, as allowing large documents might impact operations, e.g. when
restarting nodes, moving documents between nodes etc. Default value is 128 MiB.
</p>
<p>
  Example:
</p>
<pre>{% highlight xml %}
<tuning>
    <max-document-size>10MiB</max-document-size>
</tuning>
{% endhighlight %}</pre>

<h2 id="merges">merges</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
Defines throttling parameters for bucket merge operations.
</p>
<table class="table">
  <thead>
  <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
<tr><th>max-per-node</th>
  <td>optional</td>
  <td>number</td>
  <td></td>
  <td>
    Maximum number of parallel active bucket merge operations.
  </td></tr>
<tr><th>max-queue-size</th>
  <td>optional</td>
  <td>number</td>
  <td></td>
  <td>
    Maximum size of the merge bucket queue, before reporting BUSY back to the distributors.
  </td></tr>
</tbody>
</table>


<h2 id="persistence-threads">persistence-threads</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
Defines the number of persistence threads per partition on each content node.
A content node executes bucket operations against the persistence engine synchronously in each of these threads.
8 threads are used by default. Override with the <strong>count</strong> attribute.
</p>



<h2 id="visitors">visitors</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
Tuning parameters for visitor operations.
Might contain <a href="#max-concurrent">max-concurrent</a>.
</p>
<table class="table">
  <thead>
  <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
<tr><th>thread-count</th>
  <td>optional</td>
  <td>number</td>
  <td></td>
  <td>
    The maximum number of threads in which to execute visitor operations.
    A higher number of threads may increase performance, but may use more memory.
  </td></tr>
<tr><th>max-queue-size</th>
  <td>optional</td>
  <td>number</td>
  <td></td>
  <td>
    Maximum size of the pending visitor queue, before reporting BUSY back to the distributors.
  </td></tr>
</tbody>
</table>



<h2 id="max-concurrent">max-concurrent</h2>
<p>
Contained in <a href="#visitors">visitors</a>.
Defines how many visitors can be active concurrently on each storage node.
The number allowed depends on priority - lower priority visitors should not block higher priority visitors completely.
To implement this, specify a fixed and a variable number.
The maximum active is calculated by adjusting the variable component using the priority,
and adding the fixed component.
</p>
<table class="table">
  <thead>
    <tr><th>Attribute</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
    <tr><th>fixed</th>
      <td>optional</td>
      <td>number</td>
      <td><a href="https://github.com/vespa-engine/vespa/blob/master/storage/src/vespa/storage/visiting/stor-visitor.def">16</a></td>
      <td>The fixed component of the maximum active count</td></tr>
    <tr><th>variable</th>
      <td>optional</td>
      <td>number</td>
      <td><a href="https://github.com/vespa-engine/vespa/blob/master/storage/src/vespa/storage/visiting/stor-visitor.def">64</a></td>
      <td>The variable component of the maximum active count</td></tr>
  </tbody>
</table>



<h2 id="resource-limits">resource-limits</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
Specifies resource limits used to decide whether external write operations should be blocked in the entire content cluster,
based on the reported resource usage by content nodes.
See <a href="../operations/feed-block.html">feed block</a> for more details.
</p>
<p>
<strong>Warning:</strong> The content nodes require resource headroom to handle
extra documents as part of re-distribution during node failure,
and spikes when running
<a href="../proton.html#proton-maintenance-jobs">maintenance jobs</a>.
Tuning these limits should be done with extreme care,
and setting them too high might lead to permanent data loss.
They are best left untouched, using the defaults, and cannot be set in Vespa Cloud.
</p>
<table class="table">
  <thead>
    <tr><th>Element</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
    <tr><th>disk</th>
      <td>optional</td>
      <td>float<br/>[0, 1]</td>
      <td>0.75</td>
      <td>
        <p id="disk">Fraction of total space on the disk partition used on a content node before feed is blocked</p>
      </td></tr>
    <tr><th>memory</th>
      <td>optional</td>
      <td>float<br/>[0, 1]</td>
      <td>0.8/0.75</td>
      <td>
        <p id="memory">
          Fraction of physical memory that can be resident memory in anonymous mapping on a content node before feed is blocked.
          Total physical memory is sampled as the minimum of <code>sysconf(_SC_PHYS_PAGES) * sysconf(_SC_PAGESIZE)</code>
          and the cgroup (v1 or v2) memory limit. Nodes with 8 Gib or less memory in Vespa Cloud has a limit of 0.75.
        </p>
      </td></tr>
  </tbody>
</table>
Example - in the content tag:
<pre>{% highlight xml %}
<tuning>
    <resource-limits>
        <disk>0.78</disk>
        <memory>0.77</memory>
    </resource-limits>
</tuning>
{% endhighlight %}</pre>



<h2 id="dispatch-tuning">dispatch</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
Tune the query dispatch behavior - child elements:
</p>
<table class="table">
  <thead>
    <tr><th>Element</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
    <tr><th>max-hits-per-partition</th>
      <td>optional</td>
      <td>Integer</td>
      <td>No capping: Return all</td>
      <td><p id="max-hits-per-partition">
        Maximum number of hits to return from a content node.
        By default, a query returns the requested number of hits +
        offset from every content node to the container.
        The container orders the hits globally according to the query,
        then discards all hits beyond the number requested.
        </p><p>
        In a system with a large fan-out,
        this consumes network bandwidth and the container nodes easily network saturated.
        Containers will also sort and discard more hits than optimal.
        </p><p>
        When there are sufficiently many search nodes,
        assuming an even distribution of the hits,
        it suffices to only return a fraction of the request number of hits from each node.
        Note that changing this number will have global ordering impact.
        See <em>top-k-probability</em> below for improving performance with fewer hits.</p></td></tr>
    <tr><th>dispatch-policy</th>
      <td>optional</td>
      <td>best-of-random-2 / adaptive</td>
      <td>adaptive</td>
      <td>
        <p id="dispatch-policy">
        Configure policy for choosing which group shall receive the next query request.
        However, multiphase requests that either requires or benefits from hitting the same group
        in all phases are always hashed.
        Relevant only for <a href="../performance/sizing-search.html#data-distribution">grouped distribution</a>:
        </p>
        <table class="table">
          <tr>
            <th style="white-space: nowrap">best-of-random-2</th>
            <td>Selects 2 random groups and selects the one with the lowest latency.</td>
          </tr><tr>
            <th>adaptive</th>
            <td>measures latency, preferring lower latency groups, selecting group with probability latency/(sum latency over all groups)</td>
          </tr>
        </table>
    <tr><th style="white-space: nowrap">prioritize-availability</th>
        <td>optional</td>
        <td>Boolean</td>
        <td>true</td>
        <td>
            <p id="prioritize-availability">
                With <a href="../performance/sizing-search.html#data-distribution">grouped distribution</a>:
                If true, or by default, all groups that are within min-active-docs-coverage of the <b>median</b>
                of the document count of other groups will be used to service queries. If set to false, only
                groups within min-active-docs-coverage of the <b>max</b> document count will be used,
                with the consequence that full coverage is prioritized over availability when
                multiple groups are lacking content, since the remaining groups may not be able
                to service the full query load.
            </p></td></tr>
    <tr><th style="white-space: nowrap">min-active-docs-coverage</th>
      <td>optional</td>
      <td>A float percentage</td>
      <td>97</td>
      <td>
        <p id="min-active-docs-coverage">
        With <a href="../performance/sizing-search.html#data-distribution">grouped distribution</a>:
        The percentage of active documents a group must have, relative to the median across all groups 
		in the content cluster, to be considered active for serving queries.
        Because of measurement timing differences, it is not advisable to tune this above 99 percent.
        </p></td></tr>
    <tr><th>top-k-probability</th>
        <td>optional</td>
        <td>Double</td>
        <td>0.9999</td>
        <td><p id="top-k-probability">
            Probability that the top K hits will be the globally best.
            Based on this probability, the dispatcher will fetch enough hits from each node to achieve this.
            The only way to guarantee a probability of 1.0 is to fetch K hits from each partition.
            However, by reducing the probability from 1.0 to 0.99999, one can significantly reduce number of hits fetched
            and save both bandwidth and latency.
            The number of hits to fetch from each partition is computed as:
        </p><p class="equation-container"><!-- depends on mathjax -->
            $${q}={\frac{k}{n}}+{qT}({p},{30})&times;{\sqrt{ {k}&times;{\frac{1}{n}}&times;({1}-{\frac{1}{n}}) }}$$
        </p><p>
            where qT is a Student's t-distribution.
            With n=10 partitions, k=200 hits and p=0.99999, only 45 hits per partition is needed,
            as opposed to 200 when p=1.0.
        </p><p>
            Use this option to reduce network and container cpu/memory in clusters with many nodes per group -
            see <a href="../performance/sizing-search.html">Vespa Serving Scaling Guide</a>.</p></td></tr>
  </tbody>
</table>



<h2 id="cluster-controller">cluster-controller</h2>
<p>
Contained in <a href="#tuning">tuning</a>.
Tuning parameters for the cluster controller managing this cluster - child elements:
<table class="table">
  <thead>
    <tr><th>Element</th><th>Required</th><th>Value</th><th>Default</th><th>Description</th></tr>
  </thead><tbody>
    <tr><th>init-progress-time</th>
      <td>optional</td>
      <td></td>
      <td></td>
      <td>
        <p id="init-progress-time">
        If the initialization progress count have not been altered for this amount of seconds,
        the node is assumed to have deadlocked and is set down.
        Note that initialization may actually be prioritized lower now,
        so setting a low value here might cause false positives.
        Though if it is set down for wrong reason,
        when it will finish initialization and then be set up again.
        </p>
      </td></tr>
    <tr><th>transition-time</th>
      <td>optional</td>
      <td></td>
      <td><a href="https://github.com/vespa-engine/vespa/blob/master/configdefinitions/src/vespa/fleetcontroller.def">
        storage_transition_time</a>
        <a href="https://github.com/vespa-engine/vespa/blob/master/configdefinitions/src/vespa/fleetcontroller.def">
        distributor_transition_time</a></td>
      <td>
        <p id="transition-time">
        The transition time states how long (in seconds) a node will be in maintenance mode
        during what looks like a controlled restart.
        Keeping a node in maintenance mode during a restart allows a restart
        without the cluster trying to create new copies of all the data immediately.
        If the node has not started or got back up within the transition time,
        the node is set down, in which case, new full bucket copies will be created.
        Note separate defaults for distributor and storage (i.e. search) nodes.
        </p>
      </td></tr>
    <tr><th style="white-space: nowrap">max-premature-crashes</th>
      <td>optional</td>
      <td></td>
      <td><a href="https://github.com/vespa-engine/vespa/blob/master/configdefinitions/src/vespa/fleetcontroller.def">
        max_premature_crashes</a></td>
      <td>
        <p id="max-premature-crashes">
        The maximum number of crashes allowed before a content node is permanently
        set down by the cluster controller.
        If the node has a stable up or down state for more than the <em>stable-state-period</em>,
        the crash count is reset.
        However, resetting the count will not re-enable the node again if it has been disabled -
        restart the cluster controller to reset.
        </p>
      </td></tr>
    <tr><th>stable-state-period</th>
      <td>optional</td>
      <td></td>
      <td><a href="https://github.com/vespa-engine/vespa/blob/master/configdefinitions/src/vespa/fleetcontroller.def">
        stable_state_time_period</a></td>
      <td>
        <p id="stable-state-period">
        If a content node's state doesn't change for this many seconds,
        it's state is considered <em>stable</em>, clearing the premature crash count.
        </p>
      </td></tr>
    <tr><th>min-distributor-up-ratio</th>
      <td>optional</td>
      <td></td>
      <td><a href="https://github.com/vespa-engine/vespa/blob/master/configdefinitions/src/vespa/fleetcontroller.def">
        min_distributor_up_ratio</a></td>
      <td>
        <p id="min-distributor-up-ratio">
        The minimum ratio of distributors that are required to be <em>up</em> for the
        cluster state to be <em>up</em>.
        </p>
        </td></tr>
    <tr><th>min-storage-up-ratio</th>
      <td>optional</td>
      <td></td>
      <td><a href="https://github.com/vespa-engine/vespa/blob/master/configdefinitions/src/vespa/fleetcontroller.def">
        min_storage_up_ratio</a></td>
      <td>
        <p id="min-storage-up-ratio">
        The minimum ratio of content nodes that are required to be <em>up</em> for the
        cluster state to be <em>up</em>.
        </p>
    </td></tr>
    <tr><th>groups-allowed-down-ratio</th>
      <td>optional</td>
      <td></td>
      <td><a href="https://github.com/vespa-engine/vespa/blob/master/configdefinitions/src/vespa/fleetcontroller.def">
        groups-allowed-down-ratio</a></td>
      <td>
        <p id="groups-allowed-down-ratio">
        A ratio for the number of content groups that are allowed to be
        down simultaneously. A value of 0.5 means that 50% of the groups are
        allowed to be down. The default is to allow only one group to be down
	at a time.
        </p>
      </td></tr>
  </tbody>
</table>

---
# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "ConfigServer Metrics"
---

<table class="table">
  <thead>
      <tr><th>Name</th><th>Unit</th><th>Description</th></tr>
  </thead>
  <tbody>
    <tr>
      <td><p id="configserver_requests">configserver.requests</p></td>
      <td>request</td>
      <td>Number of requests processed</td>
    </tr>
    <tr>
      <td><p id="configserver_failedRequests">configserver.failedRequests</p></td>
      <td>request</td>
      <td>Number of requests that failed</td>
    </tr>
    <tr>
      <td><p id="configserver_latency">configserver.latency</p></td>
      <td>millisecond</td>
      <td>Time to complete requests</td>
    </tr>
    <tr>
      <td><p id="configserver_cacheConfigElems">configserver.cacheConfigElems</p></td>
      <td>item</td>
      <td>Time to complete requests</td>
    </tr>
    <tr>
      <td><p id="configserver_cacheChecksumElems">configserver.cacheChecksumElems</p></td>
      <td>item</td>
      <td>Number of checksum elements in the cache</td>
    </tr>
    <tr>
      <td><p id="configserver_hosts">configserver.hosts</p></td>
      <td>node</td>
      <td>The number of nodes being served configuration from the config server cluster</td>
    </tr>
    <tr>
      <td><p id="configserver_tenants">configserver.tenants</p></td>
      <td>instance</td>
      <td>The number of tenants being served configuration from the config server cluster</td>
    </tr>
    <tr>
      <td><p id="configserver_applications">configserver.applications</p></td>
      <td>instance</td>
      <td>The number of applications being served configuration from the config server cluster</td>
    </tr>
    <tr>
      <td><p id="configserver_delayedResponses">configserver.delayedResponses</p></td>
      <td>response</td>
      <td>Number of delayed responses</td>
    </tr>
    <tr>
      <td><p id="configserver_sessionChangeErrors">configserver.sessionChangeErrors</p></td>
      <td>session</td>
      <td>Number of session change errors</td>
    </tr>
    <tr>
      <td><p id="configserver_unknownHostRequests">configserver.unknownHostRequests</p></td>
      <td>request</td>
      <td>Config requests from unknown hosts</td>
    </tr>
    <tr>
      <td><p id="configserver_newSessions">configserver.newSessions</p></td>
      <td>session</td>
      <td>New config sessions</td>
    </tr>
    <tr>
      <td><p id="configserver_preparedSessions">configserver.preparedSessions</p></td>
      <td>session</td>
      <td>Prepared config sessions</td>
    </tr>
    <tr>
      <td><p id="configserver_activeSessions">configserver.activeSessions</p></td>
      <td>session</td>
      <td>Active config sessions</td>
    </tr>
    <tr>
      <td><p id="configserver_inactiveSessions">configserver.inactiveSessions</p></td>
      <td>session</td>
      <td>Inactive config sessions</td>
    </tr>
    <tr>
      <td><p id="configserver_addedSessions">configserver.addedSessions</p></td>
      <td>session</td>
      <td>Added config sessions</td>
    </tr>
    <tr>
      <td><p id="configserver_removedSessions">configserver.removedSessions</p></td>
      <td>session</td>
      <td>Removed config sessions</td>
    </tr>
    <tr>
      <td><p id="configserver_rpcServerWorkQueueSize">configserver.rpcServerWorkQueueSize</p></td>
      <td>item</td>
      <td>Number of elements in the RPC server work queue</td>
    </tr>
    <tr>
      <td><p id="maintenanceDeployment_transientFailure">maintenanceDeployment.transientFailure</p></td>
      <td>operation</td>
      <td>Number of maintenance deployments that failed with a transient failure</td>
    </tr>
    <tr>
      <td><p id="maintenanceDeployment_failure">maintenanceDeployment.failure</p></td>
      <td>operation</td>
      <td>Number of maintenance deployments that failed with a permanent failure</td>
    </tr>
    <tr>
      <td><p id="maintenance_successFactorDeviation">maintenance.successFactorDeviation</p></td>
      <td>fraction</td>
      <td>Configserver: Maintenance Success Factor Deviation</td>
    </tr>
    <tr>
      <td><p id="maintenance_duration">maintenance.duration</p></td>
      <td>millisecond</td>
      <td>Configserver: Maintenance Duration</td>
    </tr>
    <tr>
      <td><p id="configserver_zkConnectionLost">configserver.zkConnectionLost</p></td>
      <td>connection</td>
      <td>Number of ZooKeeper connections lost</td>
    </tr>
    <tr>
      <td><p id="configserver_zkReconnected">configserver.zkReconnected</p></td>
      <td>connection</td>
      <td>Number of ZooKeeper reconnections</td>
    </tr>
    <tr>
      <td><p id="configserver_zkConnected">configserver.zkConnected</p></td>
      <td>node</td>
      <td>Number of ZooKeeper nodes connected</td>
    </tr>
    <tr>
      <td><p id="configserver_zkSuspended">configserver.zkSuspended</p></td>
      <td>node</td>
      <td>Number of ZooKeeper nodes suspended</td>
    </tr>
    <tr>
      <td><p id="configserver_zkZNodes">configserver.zkZNodes</p></td>
      <td>node</td>
      <td>Number of ZooKeeper nodes present</td>
    </tr>
    <tr>
      <td><p id="configserver_zkAvgLatency">configserver.zkAvgLatency</p></td>
      <td>millisecond</td>
      <td>Average latency for ZooKeeper requests</td>
    </tr>
    <tr>
      <td><p id="configserver_zkMaxLatency">configserver.zkMaxLatency</p></td>
      <td>millisecond</td>
      <td>Max latency for ZooKeeper requests</td>
    </tr>
    <tr>
      <td><p id="configserver_zkConnections">configserver.zkConnections</p></td>
      <td>connection</td>
      <td>Number of ZooKeeper connections</td>
    </tr>
    <tr>
      <td><p id="configserver_zkOutstandingRequests">configserver.zkOutstandingRequests</p></td>
      <td>request</td>
      <td>Number of ZooKeeper requests in flight</td>
    </tr>
    <tr>
      <td><p id="orchestrator_lock_acquire-latency">orchestrator.lock.acquire-latency</p></td>
      <td>second</td>
      <td>Time to acquire zookeeper lock</td>
    </tr>
    <tr>
      <td><p id="orchestrator_lock_acquire-success">orchestrator.lock.acquire-success</p></td>
      <td>operation</td>
      <td>Number of times zookeeper lock has been acquired successfully</td>
    </tr>
    <tr>
      <td><p id="orchestrator_lock_acquire-timedout">orchestrator.lock.acquire-timedout</p></td>
      <td>operation</td>
      <td>Number of times zookeeper lock couldn't be acquired within timeout</td>
    </tr>
    <tr>
      <td><p id="orchestrator_lock_acquire">orchestrator.lock.acquire</p></td>
      <td>operation</td>
      <td>Number of attempts to acquire zookeeper lock</td>
    </tr>
    <tr>
      <td><p id="orchestrator_lock_acquired">orchestrator.lock.acquired</p></td>
      <td>operation</td>
      <td>Number of times zookeeper lock was acquired</td>
    </tr>
    <tr>
      <td><p id="orchestrator_lock_hold-latency">orchestrator.lock.hold-latency</p></td>
      <td>second</td>
      <td>Time zookeeper lock was held before it was released</td>
    </tr>
    <tr>
      <td><p id="nodes_active">nodes.active</p></td>
      <td>node</td>
      <td>The number of active nodes in a cluster</td>
    </tr>
    <tr>
      <td><p id="nodes_nonActive">nodes.nonActive</p></td>
      <td>node</td>
      <td>The number of non-active nodes in a cluster</td>
    </tr>
    <tr>
      <td><p id="nodes_nonActiveFraction">nodes.nonActiveFraction</p></td>
      <td>node</td>
      <td>The fraction of non-active nodes vs total nodes in a cluster</td>
    </tr>
    <tr>
      <td><p id="nodes_exclusiveSwitchFraction">nodes.exclusiveSwitchFraction</p></td>
      <td>fraction</td>
      <td>The fraction of nodes in a cluster on exclusive network switches</td>
    </tr>
    <tr>
      <td><p id="nodes_emptyExclusive">nodes.emptyExclusive</p></td>
      <td>node</td>
      <td>The number of exclusive hosts that do not have any nodes allocated to them</td>
    </tr>
    <tr>
      <td><p id="nodes_expired_deprovisioned">nodes.expired.deprovisioned</p></td>
      <td>node</td>
      <td>The number of deprovisioned nodes that have expired</td>
    </tr>
    <tr>
      <td><p id="nodes_expired_dirty">nodes.expired.dirty</p></td>
      <td>node</td>
      <td>The number of dirty nodes that have expired</td>
    </tr>
    <tr>
      <td><p id="nodes_expired_inactive">nodes.expired.inactive</p></td>
      <td>node</td>
      <td>The number of inactive nodes that have expired</td>
    </tr>
    <tr>
      <td><p id="nodes_expired_provisioned">nodes.expired.provisioned</p></td>
      <td>node</td>
      <td>The number of provisioned nodes that have expired</td>
    </tr>
    <tr>
      <td><p id="nodes_expired_reserved">nodes.expired.reserved</p></td>
      <td>node</td>
      <td>The number of reserved nodes that have expired</td>
    </tr>
    <tr>
      <td><p id="cluster_cost">cluster.cost</p></td>
      <td>dollar_per_hour</td>
      <td>The cost of the nodes allocated to a certain cluster, in $/hr</td>
    </tr>
    <tr>
      <td><p id="cluster_load_ideal_cpu">cluster.load.ideal.cpu</p></td>
      <td>fraction</td>
      <td>The ideal cpu load of a certain cluster</td>
    </tr>
    <tr>
      <td><p id="cluster_load_ideal_memory">cluster.load.ideal.memory</p></td>
      <td>fraction</td>
      <td>The ideal memory load of a certain cluster</td>
    </tr>
    <tr>
      <td><p id="cluster_load_ideal_disk">cluster.load.ideal.disk</p></td>
      <td>fraction</td>
      <td>The ideal disk load of a certain cluster</td>
    </tr>
    <tr>
      <td><p id="cluster_load_peak_cpu">cluster.load.peak.cpu</p></td>
      <td>fraction</td>
      <td>The peak cpu load in the period considered of a certain cluster</td>
    </tr>
    <tr>
      <td><p id="cluster_load_peak_memory">cluster.load.peak.memory</p></td>
      <td>fraction</td>
      <td>The peak memory load in the period considered of a certain cluster</td>
    </tr>
    <tr>
      <td><p id="cluster_load_peak_disk">cluster.load.peak.disk</p></td>
      <td>fraction</td>
      <td>The peak disk load in the period considered of a certain cluster</td>
    </tr>
    <tr>
      <td><p id="zone_working">zone.working</p></td>
      <td>binary</td>
      <td>The value 1 if zone is considered healthy, 0 if not. This is decided by considering the number of non-active nodes vs the number of active nodes in a zone</td>
    </tr>
    <tr>
      <td><p id="cache_nodeObject_hitRate">cache.nodeObject.hitRate</p></td>
      <td>fraction</td>
      <td>The fraction of cache hits vs cache lookups for the node object cache</td>
    </tr>
    <tr>
      <td><p id="cache_nodeObject_evictionCount">cache.nodeObject.evictionCount</p></td>
      <td>item</td>
      <td>The number of cache elements evicted from the node object cache</td>
    </tr>
    <tr>
      <td><p id="cache_nodeObject_size">cache.nodeObject.size</p></td>
      <td>item</td>
      <td>The number of cache elements in the node object cache</td>
    </tr>
    <tr>
      <td><p id="cache_curator_hitRate">cache.curator.hitRate</p></td>
      <td>fraction</td>
      <td>The fraction of cache hits vs cache lookups for the curator cache</td>
    </tr>
    <tr>
      <td><p id="cache_curator_evictionCount">cache.curator.evictionCount</p></td>
      <td>item</td>
      <td>The number of cache elements evicted from the curator cache</td>
    </tr>
    <tr>
      <td><p id="cache_curator_size">cache.curator.size</p></td>
      <td>item</td>
      <td>The number of cache elements in the curator cache</td>
    </tr>
    <tr>
      <td><p id="wantedRestartGeneration">wantedRestartGeneration</p></td>
      <td>generation</td>
      <td>Wanted restart generation for tenant node</td>
    </tr>
    <tr>
      <td><p id="currentRestartGeneration">currentRestartGeneration</p></td>
      <td>generation</td>
      <td>Current restart generation for tenant node</td>
    </tr>
    <tr>
      <td><p id="wantToRestart">wantToRestart</p></td>
      <td>binary</td>
      <td>One if node wants to restart, zero if not</td>
    </tr>
    <tr>
      <td><p id="wantedRebootGeneration">wantedRebootGeneration</p></td>
      <td>generation</td>
      <td>Wanted reboot generation for tenant node</td>
    </tr>
    <tr>
      <td><p id="currentRebootGeneration">currentRebootGeneration</p></td>
      <td>generation</td>
      <td>Current reboot generation for tenant node</td>
    </tr>
    <tr>
      <td><p id="wantToReboot">wantToReboot</p></td>
      <td>binary</td>
      <td>One if node wants to reboot, zero if not</td>
    </tr>
    <tr>
      <td><p id="retired">retired</p></td>
      <td>binary</td>
      <td>One if node is retired, zero if not</td>
    </tr>
    <tr>
      <td><p id="wantedVespaVersion">wantedVespaVersion</p></td>
      <td>version</td>
      <td>Wanted vespa version for the node, in the form MINOR.PATCH. Major version is not included here</td>
    </tr>
    <tr>
      <td><p id="currentVespaVersion">currentVespaVersion</p></td>
      <td>version</td>
      <td>Current vespa version for the node, in the form MINOR.PATCH. Major version is not included here</td>
    </tr>
    <tr>
      <td><p id="wantToChangeVespaVersion">wantToChangeVespaVersion</p></td>
      <td>binary</td>
      <td>One if node want to change Vespa version, zero if not</td>
    </tr>
    <tr>
      <td><p id="hasWireguardKey">hasWireguardKey</p></td>
      <td>binary</td>
      <td>One if node has a WireGuard key, zero if not</td>
    </tr>
    <tr>
      <td><p id="wantToRetire">wantToRetire</p></td>
      <td>binary</td>
      <td>One if node wants to retire, zero if not</td>
    </tr>
    <tr>
      <td><p id="wantToDeprovision">wantToDeprovision</p></td>
      <td>binary</td>
      <td>One if node wants to be deprovisioned, zero if not</td>
    </tr>
    <tr>
      <td><p id="failReport">failReport</p></td>
      <td>binary</td>
      <td>One if there is a fail report for the node, zero if not</td>
    </tr>
    <tr>
      <td><p id="suspended">suspended</p></td>
      <td>binary</td>
      <td>One if the node is suspended, zero if not</td>
    </tr>
    <tr>
      <td><p id="suspendedSeconds">suspendedSeconds</p></td>
      <td>second</td>
      <td>The number of seconds the node has been suspended</td>
    </tr>
    <tr>
      <td><p id="activeSeconds">activeSeconds</p></td>
      <td>second</td>
      <td>The number of seconds the node has been active</td>
    </tr>
    <tr>
      <td><p id="numberOfServicesUp">numberOfServicesUp</p></td>
      <td>instance</td>
      <td>The number of services confirmed to be running on a node</td>
    </tr>
    <tr>
      <td><p id="numberOfServicesNotChecked">numberOfServicesNotChecked</p></td>
      <td>instance</td>
      <td>The number of services supposed to run on a node, that has not checked</td>
    </tr>
    <tr>
      <td><p id="numberOfServicesDown">numberOfServicesDown</p></td>
      <td>instance</td>
      <td>The number of services confirmed to not be running on a node</td>
    </tr>
    <tr>
      <td><p id="someServicesDown">someServicesDown</p></td>
      <td>binary</td>
      <td>One if one or more services has been confirmed to not run on a node, zero if not</td>
    </tr>
    <tr>
      <td><p id="numberOfServicesUnknown">numberOfServicesUnknown</p></td>
      <td>instance</td>
      <td>The number of services the config server does not know is running on a node</td>
    </tr>
    <tr>
      <td><p id="nodeFailerBadNode">nodeFailerBadNode</p></td>
      <td>binary</td>
      <td>One if the node is failed due to being bad, zero if not</td>
    </tr>
    <tr>
      <td><p id="downInNodeRepo">downInNodeRepo</p></td>
      <td>binary</td>
      <td>One if the node is registered as being down in the node repository, zero if not</td>
    </tr>
    <tr>
      <td><p id="numberOfServices">numberOfServices</p></td>
      <td>instance</td>
      <td>Number of services supposed to run on a node</td>
    </tr>
    <tr>
      <td><p id="lockAttempt_acquireMaxActiveLatency">lockAttempt.acquireMaxActiveLatency</p></td>
      <td>second</td>
      <td>Maximum duration for keeping a lock, ending during the metrics snapshot, or still being kept at the end or this snapshot period</td>
    </tr>
    <tr>
      <td><p id="lockAttempt_acquireHz">lockAttempt.acquireHz</p></td>
      <td>operation_per_second</td>
      <td>Average number of locks acquired per second the snapshot period</td>
    </tr>
    <tr>
      <td><p id="lockAttempt_acquireLoad">lockAttempt.acquireLoad</p></td>
      <td>operation</td>
      <td>Average number of locks held concurrently during the snapshot period</td>
    </tr>
    <tr>
      <td><p id="lockAttempt_lockedLatency">lockAttempt.lockedLatency</p></td>
      <td>second</td>
      <td>Longest lock duration in the snapshot period</td>
    </tr>
    <tr>
      <td><p id="lockAttempt_lockedLoad">lockAttempt.lockedLoad</p></td>
      <td>operation</td>
      <td>Average number of locks held concurrently during the snapshot period</td>
    </tr>
    <tr>
      <td><p id="lockAttempt_acquireTimedOut">lockAttempt.acquireTimedOut</p></td>
      <td>operation</td>
      <td> Number of locking attempts that timed out during the snapshot period</td>
    </tr>
    <tr>
      <td><p id="lockAttempt_deadlock">lockAttempt.deadlock</p></td>
      <td>operation</td>
      <td>Number of lock grab deadlocks detected during the snapshot period</td>
    </tr>
    <tr>
      <td><p id="lockAttempt_errors">lockAttempt.errors</p></td>
      <td>operation</td>
      <td>Number of other lock related errors detected during the snapshot period</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_totalCapacityCpu">hostedVespa.docker.totalCapacityCpu</p></td>
      <td>vcpu</td>
      <td>Total number of VCPUs on tenant hosts managed by hosted Vespa in a zone</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_totalCapacityMem">hostedVespa.docker.totalCapacityMem</p></td>
      <td>gigabyte</td>
      <td>Total amount of memory on tenant hosts managed by hosted Vespa in a zone</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_totalCapacityDisk">hostedVespa.docker.totalCapacityDisk</p></td>
      <td>gigabyte</td>
      <td>Total amount of disk space on tenant hosts managed by hosted Vespa in a zone</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_freeCapacityCpu">hostedVespa.docker.freeCapacityCpu</p></td>
      <td>vcpu</td>
      <td>Total number of free VCPUs on tenant hosts managed by hosted Vespa in a zone</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_freeCapacityMem">hostedVespa.docker.freeCapacityMem</p></td>
      <td>gigabyte</td>
      <td>Total amount of free memory on tenant hosts managed by hosted Vespa in a zone</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_freeCapacityDisk">hostedVespa.docker.freeCapacityDisk</p></td>
      <td>gigabyte</td>
      <td>Total amount of free disk space on tenant hosts managed by hosted Vespa in a zone</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_allocatedCapacityCpu">hostedVespa.docker.allocatedCapacityCpu</p></td>
      <td>vcpu</td>
      <td>Total number of allocated VCPUs on tenant hosts managed by hosted Vespa in a zone</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_allocatedCapacityMem">hostedVespa.docker.allocatedCapacityMem</p></td>
      <td>gigabyte</td>
      <td>Total amount of allocated memory on tenant hosts managed by hosted Vespa in a zone</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_allocatedCapacityDisk">hostedVespa.docker.allocatedCapacityDisk</p></td>
      <td>gigabyte</td>
      <td>Total amount of allocated disk space on tenant hosts managed by hosted Vespa in a zone</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_pendingRedeployments">hostedVespa.pendingRedeployments</p></td>
      <td>task</td>
      <td>The number of hosted Vespa re-deployments pending</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_docker_skew">hostedVespa.docker.skew</p></td>
      <td>fraction</td>
      <td>A number in the range 0..1 indicating how well allocated resources are balanced with availability on hosts</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_activeHosts">hostedVespa.activeHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "active"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_breakfixedHosts">hostedVespa.breakfixedHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "breakfixed"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_deprovisionedHosts">hostedVespa.deprovisionedHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "deprovisioned"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_dirtyHosts">hostedVespa.dirtyHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "dirty"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_failedHosts">hostedVespa.failedHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "failed"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_inactiveHosts">hostedVespa.inactiveHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "inactive"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_parkedHosts">hostedVespa.parkedHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "parked"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_provisionedHosts">hostedVespa.provisionedHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "provisioned"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_readyHosts">hostedVespa.readyHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "ready"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_reservedHosts">hostedVespa.reservedHosts</p></td>
      <td>host</td>
      <td>The number of managed hosts that are in state "reserved"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_activeNodes">hostedVespa.activeNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "active"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_breakfixedNodes">hostedVespa.breakfixedNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "breakfixed"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_deprovisionedNodes">hostedVespa.deprovisionedNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "deprovisioned"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_dirtyNodes">hostedVespa.dirtyNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "dirty"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_failedNodes">hostedVespa.failedNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "failed"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_inactiveNodes">hostedVespa.inactiveNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "inactive"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_parkedNodes">hostedVespa.parkedNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "parked"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_provisionedNodes">hostedVespa.provisionedNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "provisioned"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_readyNodes">hostedVespa.readyNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "ready"</td>
    </tr>
    <tr>
      <td><p id="hostedVespa_reservedNodes">hostedVespa.reservedNodes</p></td>
      <td>host</td>
      <td>The number of managed nodes that are in state "reserved"</td>
    </tr>
    <tr>
      <td><p id="overcommittedHosts">overcommittedHosts</p></td>
      <td>host</td>
      <td>The number of hosts with over-committed resources</td>
    </tr>
    <tr>
      <td><p id="spareHostCapacity">spareHostCapacity</p></td>
      <td>host</td>
      <td>The number of spare hosts</td>
    </tr>
    <tr>
      <td><p id="throttledHostFailures">throttledHostFailures</p></td>
      <td>host</td>
      <td>Number of host failures stopped due to throttling</td>
    </tr>
    <tr>
      <td><p id="throttledNodeFailures">throttledNodeFailures</p></td>
      <td>host</td>
      <td>Number of node failures stopped due to throttling</td>
    </tr>
    <tr>
      <td><p id="nodeFailThrottling">nodeFailThrottling</p></td>
      <td>binary</td>
      <td>Metric indicating when node failure throttling is active. The value 1 means active, 0 means inactive</td>
    </tr>
    <tr>
      <td><p id="clusterAutoscaled">clusterAutoscaled</p></td>
      <td>operation</td>
      <td>Number of times a cluster has been rescaled by the autoscaler</td>
    </tr>
    <tr>
      <td><p id="clusterAutoscaleDuration">clusterAutoscaleDuration</p></td>
      <td>second</td>
      <td>The currently predicted duration of a rescaling of this cluster</td>
    </tr>
    <tr>
      <td><p id="deployment_prepareMillis">deployment.prepareMillis</p></td>
      <td>millisecond</td>
      <td>Duration of deployment preparations</td>
    </tr>
    <tr>
      <td><p id="deployment_activateMillis">deployment.activateMillis</p></td>
      <td>millisecond</td>
      <td>Duration of deployment activations</td>
    </tr>
    <tr>
      <td><p id="throttledHostProvisioning">throttledHostProvisioning</p></td>
      <td>binary</td>
      <td>Value 1 if host provisioning is throttled, 0 if not</td>
    </tr>
  </tbody>
</table>

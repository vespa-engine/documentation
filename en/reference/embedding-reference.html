---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Embedding Reference"
redirect_from:
- /documentation/reference/embedding-reference.html
---

<p>Reference configuration for <a href="../embedding.html">embedders</a>.</p>

<h2 id="model-config-reference">Model config reference</h2>
<p>
  Embedder models uses the <a href="config-files.html#model">model</a> type configuration.
  The <em>model</em> type configuration accepts attributes <code>model-id</code>, <code>url</code> or <code>path</code>,
  and multiple of these can be specified as a single config value, where one is used depending on the deployment environment:
</p>
  <ul>
    <li>If a <code>model-id</code> is specified and the application is deployed on Vespa Cloud, the <code>model-id</code> is used.</li>
    <li>Otherwise, if a <code>url</code> is specified, it is used</li>
    <li>Otherwise, <code>path</code> is used.</li>
  </ul>
<p>
  When using <code>path</code>, the model files must be supplied in the
  Vespa <a href="../application-packages.html#deploying-remote-models">application package</a>.
</p>

<h2 id="huggingface-embedder">Huggingface Embedder</h3>
<p>
  An embedder using any <a href="https://huggingface.co/docs/tokenizers/index">Huggingface tokenizer</a>,
  including multilingual tokenizers,
  to produce tokens which is then input to a supplied transformer model in ONNX model format.
</p>
<p>
  The Huggingface embedder is configured in <a href="services.html">services.xml</a>,
  within the <code>container</code> tag:
</p>
<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="hf-embedder" type="hugging-face-embedder">
        <transformer-model path="my-models/model.onnx"/>
        <tokenizer-model path="my-models/tokenizer.json"/>
    </component>
    ...
</container>
{% endhighlight %}</pre>

<h3 id="huggingface-embedder-reference-config">Huggingface embedder reference config</h3>
<p>In addition to <a href="#embedder-onnx-reference-config">embedder ONNX parameters</a>:</p>
<table class="table">
  <thead>
    <tr>
      <th>Name</th>
      <th>Occurrence</th>
      <th>Description</th>
      <th>Type</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>transformer-model</td>
      <td>One</td>
      <td>Use to point to the transformer ONNX model file</td>
      <td><a href="#model-config-reference">model-type</a></td>
      <td>N/A</td>
    </tr>
    <tr>
      <td>tokenizer-model</td>
      <td>One</td>
      <td>Use to point to the <code>tokenizer.json</code> Huggingface tokenizer configuration file</td>
      <td><a href="#model-config-reference">model-type</a></td>
      <td>N/A</td>
    </tr>
    <tr>
      <td>max-tokens</td>
      <td>One</td>
      <td>The maximum number of tokens accepted by the transformer model</td>
      <td>numeric</td>
      <td>512</td>
    </tr>
    <tr>
      <td>transformer-input-ids</td>
      <td>One</td>
      <td>The name or identifier for the transformer input IDs</td>
      <td>string</td>
      <td>input_ids</td>
    </tr>
    <tr>
      <td>transformer-attention-mask</td>
      <td>One</td>
      <td>The name or identifier for the transformer attention mask</td>
      <td>string</td>
      <td>attention_mask</td>
    </tr>
    <tr>
      <td>transformer-token-type-ids</td>
      <td>One</td>
      <td>The name or identifier for the transformer token type IDs.  If the model does not use <code>token_type_ids</code> use <code>&lt;transformer-token-type-ids/&gt;</code></td>
      <td>string</td>
      <td>token_type_ids</td>
    </tr>
    <tr>
      <td>transformer-output</td>
      <td>One</td>
      <td>The name or identifier for the transformer output</td>
      <td>string</td>
      <td>last_hidden_state</td>
    </tr>
    <tr>
      <td>pooling-strategy</td>
      <td>One</td>
      <td>How the output vectors of the ONNX model is pooled to obtain a single vector representation. Valid values are <code>mean</code> and <code>cls</code></td>
      <td>string</td>
      <td>mean</td>
    </tr>
    <tr>
      <td>normalize</td>
      <td>One</td>
      <td>A boolean indicating whether to normalize the output embedding vector to unit length (length 1). Useful for <code>prenormalized-angular</code>
      <a href="reference/schema-reference.html#distance-metric">distance-metric</a></td>
      <td>boolean</td>
      <td>false</td>
    </tr>
  </tbody>
</table>



<h2 id="bert-embedder">Bert embedder</h2>
<p>
  The Bert embedder is configured in <a href="services.html">services.xml</a>,
  within the <code>container</code> tag:
</p>
<pre>{% highlight xml %}
<container version="1.0">
  <component id="myBert" type="bert-embedder">
    <transformer-model path="models/e5-small-v2.onnx"/>
    <tokenizer-vocab url="https://huggingface.co/intfloat/e5-small-v2/raw/main/vocab.txt"/>
  </component>
</container>
{% endhighlight %}</pre>

<h3 id="bert-embedder-reference-config">Bert embedder reference config</h3>
<p>In addition to <a href="#embedder-onnx-reference-config">embedder ONNX parameters</a>:</p>
<table class="table">
  <thead>
    <tr>
      <th>Name</th>
      <th>Occurrence</th>
      <th>Description</th>
      <th>Type</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>transformer-model</td>
      <td>One</td>
      <td>Use to point to the transformer ONNX model file</td>
      <td><a href="#model-config-reference">model-type</a></td>
      <td>N/A</td>
    </tr>
    <tr>
      <td>tokenizer-vocab</td>
      <td>One</td>
      <td>Use to point to the Huggingface <code>vocab.txt</code> tokenizer file with valid wordpiece tokens. Does not support <code>tokenizer.json</code> format.</td>
      <td><a href="#model-config-reference">model-type</a></td>
      <td>N/A</td>
    </tr>
    <tr>
      <td>max-tokens</td>
      <td>One</td>
      <td>The maximum number of tokens allowed in the input</td>
      <td>integer</td>
      <td>384</td>
    </tr>
    <tr>
      <td>transformer-input-ids</td>
      <td>One</td>
      <td>The name or identifier for the transformer input IDs</td>
      <td>string</td>
      <td>input_ids</td>
    </tr>
    <tr>
      <td>transformer-attention-mask</td>
      <td>One</td>
      <td>The name or identifier for the transformer attention mask</td>
      <td>string</td>
      <td>attention_mask</td>
    </tr>
    <tr>
      <td>transformer-token-type-ids</td>
      <td>One</td>
      <td>The name or identifier for the transformer token type IDs.  If the model does not use <code>token_type_ids</code> use <code>&lt;transformer-token-type-ids/&gt;</code></td>
      <td>string</td>
      <td>token_type_ids</td>
    </tr>
    <tr>
      <td>transformer-output</td>
      <td>One</td>
      <td>The name or identifier for the transformer output</td>
      <td>string</td>
      <td>output_0</td>
    </tr>
    <tr>
      <td>transformer-start-sequence-token</td>
      <td>One</td>
      <td>The start of sequence token</td>
      <td>numeric</td>
      <td>101</td>
    </tr>
    <tr>
      <td>transformer-end-sequence-token</td>
      <td>One</td>
      <td>The start of sequence token</td>
      <td>numeric</td>
      <td>102</td>
    </tr>
    <tr>
      <td>pooling-strategy</td>
      <td>One</td>
      <td>How the output vectors of the ONNX model is pooled to obtain a single vector representation. Valid values are <code>mean</code> and <code>cls</code></td>
      <td>string</td>
      <td>mean</td>
    </tr>
  </tbody>
</table>

<h2 id="huggingface-tokenizer-embedder">Huggingface tokenizer embedder</h3>
  <p>
    The Huggingface tokenizer embedder is configured in <a href="services.html">services.xml</a>,
    within the <code>container</code> tag:
  </p>
<pre>{% highlight xml %}
  <container version="1.0">
    <component id="tokenizer" type="hugging-face-tokenizer">
      <model url="https://huggingface.co/bert-base-uncased/raw/main/tokenizer.json"/>
    </component>
  </container>
{% endhighlight %}</pre>

<h3 id="huggingface-tokenizer-reference-config">Huggingface tokenizer reference config</h3>
<table class="table">
  <thead>
    <tr>
      <th>Name</th>
      <th>Occurrence</th>
      <th>Description</th>
      <th>Type</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>model</td>
      <td>One To Many</td>
      <td>Use to point to the <code>tokenizer.json</code> Huggingface tokenizer configuration file.
        Also supports <code>language</code>, which is only relevant if one wants to tokenize differently based on the document language.
        Use "unknown" for a model to be used for any language (i.e. by default).</td>
      <td><a href="#model-config-reference">model-type</a></td>
      <td>N/A</td>
    </tr>
  </tbody>
</table>


<h2 id="embedder-onnx-reference-config">Embedder ONNX reference config</h2>
<p>
  Vespa uses <a href="https://onnxruntime.ai/">ONNX Runtime</a> to accelerate inference of embedding models.
  These parameters are valid for both <a href="#bert-embedder">Bert embedder</a> and
  <a href="#huggingface-embedder">Huggingface embedder</a>.
</p>
<table class="table">
  <thead>
    <tr>
      <th>Name</th>
      <th>Occurrence</th>
      <th>Description</th>
      <th>Type</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>onnx-execution-mode</td>
      <td>One</td>
      <td>Low level ONNX execution model. Valid values are <code>parallel</code> or <code>sequential</code>.
         Only relevant for inference on CPU.
         See <a href="https://onnxruntime.ai/docs/performance/tune-performance/threading.html">ONNX runtime documentation</a> on threading.
        </td>
      <td>string</td>
      <td>sequential</td>
    </tr>
    <tr>
      <td>onnx-interop-threads</td>
      <td>One</td>
      <td>Low level ONNX setting.Only relevant for inference on CPU.</td>
      <td>numeric</td>
      <td>1</td>
    </tr>
    <tr>
      <td>onnx-intraop-threads</td>
      <td>One</td>
      <td>Low level ONNX setting. Only relevant for inference on CPU.</td>
      <td>numeric</td>
      <td>4</td>
    </tr>
    <tr>
      <td>onnx-gpu-device</td>
      <td>One</td>
      <td>The GPU device to run the model on. 
        See <a href="../vespa-gpu-container.html">configuring GPU for Vespa container image</a>. Use <code>-1</code> to not use GPU for the model, even
        if the instance has available GPUs.
      </td>
      <td>numeric</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<h2 id="sentencepiece-embedder">SentencePiece embedder</h2>
  <p>
    A native Java implementation of <a href="https://github.com/google/sentencepiece">SentencePiece</a>.
    SentencePiece breaks text into chunks independent of spaces,
    which is robust to misspellings and works with CJK languages.
    Prefer the <a href="#huggingface-tokenizer-embedder">Huggingface tokenizer embedder</a> over this
    for better compatibility with Huggingface models.
  </p>
  <p>
    This is suitable to use in conjunction with <a href="../jdisc/container-components.html">custom components</a>,
    or the resulting tensor can be used in <a href="../ranking.html">ranking</a>.
  </p>
  <p>
    To use the
    <a href="https://github.com/vespa-engine/vespa/blob/master/linguistics-components/src/main/java/com/yahoo/language/sentencepiece/SentencePieceEmbedder.java">
    SentencePiece embedder</a>, add it to <a href="reference/services.html">services.xml</a>:
  </p>
  <pre>{% highlight xml %}
  <container version="1.0">
    <component id="mySentencePiece"
             class="com.yahoo.language.sentencepiece.SentencePieceEmbedder"
             bundle="linguistics-components">
      <config name="language.sentencepiece.sentence-piece">;
          <model>
              <item>
                <language>unknown</language>
                <path>model/en.wiki.bpe.vs10000.model</path>
              </item>
          </model>
        </config>
    </component>
  </container>
  {% endhighlight %}</pre>
  <p>
    See the options available for configuring SentencePiece in
    <a href="https://github.com/vespa-engine/vespa/blob/master/linguistics-components/src/main/resources/configdefinitions/language.sentencepiece.sentence-piece.def">
    the full configuration definition</a>.
  </p>
  
  
  <h2 id="wordpiece-embedder">WordPiece embedder</h2>
  <p>
    A native Java implementation of
    <a href="https://github.com/google-research/bert#tokenization">WordPiece</a>,
    which is commonly used with BERT models.
    Prefer the <a href="#huggingface-tokenizer-embedder">Huggingface tokenizer embedder</a>
    over this for better compatibility with Huggingface models.
  </p>
  <p>
    This is suitable to use in conjunction with <a href="../jdisc/container-components.html">custom components</a>,
    or the resulting tensor can be used in <a href="../ranking.html">ranking</a>.
  </p>
  <p>
    To use the
    <a href="https://github.com/vespa-engine/vespa/blob/master/linguistics-components/src/main/java/com/yahoo/language/wordpiece/WordPieceEmbedder.java">
    WordPiece embedder</a>,
    add it to <a href="services.html">services.xml</a> within the <code>container</code> tag:
  </p>
  <pre>{% highlight xml %}
  <container version="1.0">
    <component id="myWordPiece">
             class="com.yahoo.language.wordpiece.WordPieceEmbedder"
             bundle="linguistics-components">
      <config name="language.wordpiece.word-piece">
        <model>
          <item>
            <language>unknown</language>
            <path>models/bert-base-uncased-vocab.txt</path>
          </item>
        </model>
      </config>
    </component>
  </container>
  {% endhighlight %}</pre>
  <p>
    See the options available for configuring WordPiece in
    <a href="https://github.com/vespa-engine/vespa/blob/master/linguistics-components/src/main/resources/configdefinitions/language.wordpiece.word-piece.def">
    the full configuration definition</a>.
  </p>
  <p>
    WordPiece is suitable to use in conjunction with custom components,
    or the resulting tensor can be used in <a href="../ranking.html">ranking</a>.
  </p>

<h2 id="using-an-embedder-from-java">Using an embedder from Java</h2>
<p>
  When writing custom Java components (such as <a href="../searcher-development.html">Searchers</a>
  or <a href="../document-processing.html#document-processors">Document processors</a>),
  use embedders you have configured by
  <a href="../jdisc/injecting-components.html">having them injected in the constructor</a>,
  just as any other component:
</p>
<pre>{% highlight java %}
class MyComponent {
  @Inject
  public MyComponent(ComponentRegistry<Embedder> embedders) {
    // embedders contains all the embedders configured in your services.xml
  }
}
{% endhighlight %}</pre>

<h2 id="custom-embedders">Custom Embedders</h2>
<p>Vespa provides a Java interface for defining components which can provide embeddings of text:
<a href="https://github.com/vespa-engine/vespa/blob/master/linguistics/src/main/java/com/yahoo/language/process/Embedder.java">
com.yahoo.language.process.Embedder</a>.</p>

<p>To define a custom embedder in an application and make it usable by Vespa (see <a href="#embedding-a-query-text">above</a>),
implement this interface and add it as a <a href="../developer-guide.html#developing-components">component</a> to
<a href="services-container.html">services.xml</a>:</p>

<pre>{% highlight xml %}
<container version="1.0">
    <component id="myEmbedder"
      class="com.example.MyEmbedder"
      bundle="the name in artifactId in pom.xml">
        <config name='com.example.my-embedder'>
            <model model-id="minilm-l6-v2"/>
            <vocab path="files/vocab.txt"/>
            <myValue>foo</myValue>
        </config>
    </component>
</container>
{% endhighlight %}</pre>

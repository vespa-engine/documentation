---
# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Ranking"
redirect_from:
- /documentation/ranking.html
---

<p><i>Ranking</i> is where Vespa does computing, or <i>inference</i> over documents.
The computations to be done are expressed in functions called
<a href="ranking-expressions-features.html">ranking expressions</a>,
bundled into <a href="#rank-profiles">rank profiles</a> defined in <a href="schemas.html">schemas</a>.
These can range from simple math expressions combining some <a href="reference/rank-features.html">rank features</a>,
to <a href="tensor-examples.html">tensor expressions</a> or large machine-learned <a href="onnx.html">Onnx models</a>.



<h2 id="two-phase-ranking">Two-phase ranking</h2>

<p>Rank profiles can define two <i>phases</i> that are evaluated locally on content nodes, which means that
no data needs to be transferred to container nodes to make inferences over data:</p>

<pre>
schema myapp {

    rank-profile my-rank-profile {

        num-threads-per-search:4

        first-phase {
            expression {
                attribute(quality) * freshness(timestamp)
            }
        }

        second-phase {
            expression: sum(onnx(my_onnx_model))
            rerank-count: 50
        }

    }
}
</pre>

<p>The first phase is executed for all matching documents while the second is executed for the
best rerank-count documents per content node according to the first-phase function. This
is useful to direct more computation towards the most promising candidate documents, see
<a href="phased-ranking.html">phased ranking</a>.</p>

<h2 id="global-phase-ranking">Global-phase ranking</h2>
<p>It's also possible to define an additional phase that runs on the
stateless container nodes after merging hits from the content nodes.
Please read the <a href="phased-ranking.html#global-phase">global-phase as part of phased ranking</a>
documentation for more details.
This can be more efficient use of CPU (especially with many content nodes)
and can be used instead of second-phase, or in addition to a moderately
expensive second-phase as in the example below.
</p>

<pre>
schema myapp {
    rank-profile my-rank-profile {
        first-phase {
            expression: attribute(quality) * freshness(timestamp)
        }
        second-phase {
            expression {
                my_combination_of(fieldMatch(title), bm25(abstract), attribute(quality), freshness(timestamp))
            }
        }
        global-phase {
            expression: sum(onnx(my_onnx_model))
            rerank-count: 100
        }
    }
}
</pre>

<h2 id="machine-learned-model-inference">Machine-Learned model inference</h2>

<p>Vespa supports ML models in these formats:</p>

<ul>
    <li><a href="onnx.html">ONNX</a></li>
    <li><a href="xgboost.html">XGBoost</a></li>
    <li><a href="lightgbm.html">LightGBM</a></li>
</ul>

<p>As these are exposed as rank features, it is possible to rank using a <em>model ensemble</em>.
Deploy multiple model instances and write a ranking expression that combines the results:</p>

<pre>
schema myapp {

    onnx-model my_model_1 {
        ...
    }
    onnx-model my_model_2 {
        ...
    }

    rank-profile my-rank-profile {
    ...
        second-phase {
            expression: max( sum(onnx(my_model_1), sum(onnx(my_model_2) )
        }
    }
}
</pre>

<!-- ToDo: Not exclusive list, we can represent lots of model, logistic regression etc. Also mention PyTorch through ONNX. -->


<h2 id="model-training-and-deployment">Model Training and Deployment</h2>

<p>Models are deployed in <a href="application-packages.html">application packages</a>.
Read more on how to automate training, deployment and re-training in a closed loop using
<a href="https://cloud.vespa.ai/">Vespa Cloud</a>.</p>



<h2 id="rank-profiles">Rank profiles</h2>

<p>Ranking expressions are defined in <a href="reference/schema-reference.html#rank-profile">rank profiles</a> -
either inside the schema or equivalently in their own files in the
<a href="reference/application-packages-reference.html">application package</a>, named
<code>schemas/[schema-name]/[profile-name].profile</code>.</p>

<p>One schema can have any number of rank profiles for implementing e.g. different use cases
or <a href="testing.html#feature-switches-and-bucket-tests">bucket testing</a> variations.
If no profile is specified, the <a href="#text-ranking">default text ranking</a>
profile is used.</p>

<p>Rank profiles can <em>inherit</em> other profiles. This makes it possible to define complex profiles and variants
without duplication.</p>

<p>Queries select a rank profile using the
<a href="reference/query-api-reference.html#ranking.profile">ranking.profile</a> argument
in requests or a <a href="query-profiles.html">query profiles</a>,
or equivalently in <a href="searcher-development.html">Searcher</a> code, by</p>

<pre>
query.getRanking().setProfile("my-rank-profile");
</pre>

<p>If no profile is specified in the query, the one called <code>default</code> is used.
This profile is available also if not defined explicitly.</p>

<p>Another special rank profile called <code>unranked</code> is also always available.
Specifying this boosts performance in queries which do not need ranking because random order is fine or
<a href="reference/sorting.html">explicit sorting</a> is used.</p>


<h3 id="evaluation">Evaluation</h3>
<p>Rank profiles are <span style="text-decoration: underline">not</span> evaluated lazily. Example:</p>
<pre>
function inline foo(tensor, defaultVal) {
    expression: if (count(tensor) == 0, defaultValue, sum(tensor))
}

function bar() {
    expression: foo(tensor, sum(tensor1 * tensor2))
}
</pre>
<p>
  <em>Will the <code>sum</code> in the <code>bar</code> function be computed lazily,
    meaning only if <code>tensor</code> is empty?</em>
</p>
<p>
  No, this would require lambda arguments.
  Only doubles and tensors are passed between functions.
</p>



<h2 id="text-ranking">Text ranking</h2>
<p>
  The default ranking is the first-phase function <code>nativeRank</code>,
  that is a function returning the value of the <a href="nativerank.html">nativeRank rank feature</a>,
  and no second-phase.
</p>
<p>
  A good simple alternative to <code>nativeRank</code> for text ranking is using the
  <a href="reference/bm25.html">BM25</a> rank feature.
</p>
<p>
  If the expression is written manually, it might be most convenient to
  stick with using the <code>fieldMatch(<em>name</em>)</code> feature for each field.
  This feature combines the more basic fieldMatch features in a reasonable way.
  A good way to combine the fieldMatch score of each field is to use a weighted average as explained above.
  Another way is to combine the field match scores
  using the <code>fieldMatch(<em>name</em>).weight/significance/importance</code> features
  which takes term weight or rareness or both into account
  and allows a normalized score to be produced by simply summing the product of this feature
  and any other normalized per-field score for each field.
  In addition, some attribute value(s) must usually be included
  to determine the a priori quality of each document.
</p>
<p>
  For example, assuming the title field is more important than the body field,
  create a ranking expression which gives more weight to that field, as in the example above.
  Vespa contains some built-in convenience support for this -
  weights can be set in the individual fields by <code>weight: &lt;number&gt;</code>
  and the feature <code>match</code> can be used to get a weighted average
  of the fieldMatch scores of each field.
  The overall ranking expression might contain other ranking dimensions than just text match,
  like freshness, the quality of the document, or any other property of the document or query.
</p>


<h3 id="weight-significance-and-connectedness">Weight, significance and connectedness</h3>
<p>
  Modify the values of the match features from the query
  by sending <em>weight</em>, <em>significance</em> and <em>connectedness</em> with the query:
</p>
<table class="table">
<thead>
  <tr>
    <th>Feature input</th><th>Description</th>
  </tr>
</thead>
  <tbody>
<tr><th>Weight</th><td>
  <p>
    Set query term <a href="reference/query-language-reference.html#weight">weight</a>.
    Example: <code>... where (title contains ({weight:200}"heads") AND title contains "tails")</code>
    specifies that <code>heads</code> is twice as important for the final rank score than <code>tails</code>
    (the default weight is 100).
  </p>
  <p>
    Weight is used in <a href="reference/rank-features.html#fieldMatch(name).weight">
    fieldMatch(<em>name</em>).weight</a>, which can be multiplied with
    <a href="reference/rank-features.html#fieldMatch(name)">
        fieldMatch(<em>name</em>)</a> to yield a weighted score for the field,
    and in <a href="reference/rank-features.html#fieldMatch(name).weightedOccurrence">
    fieldMatch(<em>name</em>).weightedOccurrence</a>
    to get an occurrence score which is higher if higher weighted terms occurs most.
  </p>
  <p>
    Configure static field weights in the <a href="reference/schema-reference.html#weight">schema</a>.
  </p>
</td>
</tr><tr><th>Significance</th><td>
  <p>
    Significance is an indication of how rare a term is in the corpus of the language,
    used by a number of text matching <a href="reference/rank-features.html">rank features</a>.
    This can be set explicitly for each term in
    <a href="reference/query-language-reference.html#significance">the query</a>,
    or by calling item.setSignificance() in a <a href="searcher-development.html">Searcher</a>.
  </p>
  <p>
    With <em>indexed search</em>, default significance values are calculated automatically during indexing.
    However, unless the indexed corpus is representative of the word frequencies in the user's language,
    relevance can be improved by passing significances derived from a representative corpus.
    Relative significance is accessible in ranking through the
    <a href="reference/rank-features.html#fieldMatch(name).significance">
        fieldMatch(<em>name</em>).significance</a> feature.
    Weight and significance are also averaged into
    <a href="reference/rank-features.html#fieldMatch(name).importance">
        fieldMatch(<em>name</em>).importance</a> for convenience.
  </p>
  <p>
    <em>Streaming search</em> does not compute term significance, queries should pass this with the query terms.
    <a href="streaming-search.html">Read more</a>.
  </p>
</td>
</tr><tr><th>Connectedness</th><td>
  <p>
    Signify the degree of connection between adjacent terms in the query -
    set query term <a href="reference/query-language-reference.html#connectivity">connectivity</a> to another term.
  </p>
  <p>
    For example, the query <code>new york newspaper</code> should have a higher connectedness
    between the terms "new" and "york" than between "york" and "newspaper" to rank documents higher
    if they contain "new york" as a phrase.
  </p>
  <p>
    Term connectedness is taken into account by
    <a href="reference/rank-features.html#fieldMatch(name).proximity">
        fieldMatch(<em>name</em>).proximity</a>,
    which is also an important contribution to
    <a href="reference/rank-features.html#fieldMatch(name)">fieldMatch(<em>name</em>)</a>.
    Connectedness is a normalized value which is 0.1 by default.
    It must be set by a custom Searcher,
    looking up connectivity information from somewhere - there is no query syntax for it.
  </p>
</td>
    <!-- ToDo: Use connectivity and id query annotations to do this? -->
</tr>
</tbody>
</table>

---
# Copyright Vespa.ai. All rights reserved.
title: Using machine-learned models from Vespa Cloud
category: cloud
---

<p>Vespa Cloud provides a set of machine-learned models that you can use
in your applications. These models will always be available on Vespa Cloud and are
<a href="https://blog.vespa.ai/tailoring-frozen-embeddings-with-vespa/">frozen models</a>. You
can also bring your own embedding model, by deploying it in the Vespa application package.</p>

<p>You specify to use a model provided by Vespa Cloud by setting the <code>model-id</code>
attribute where you specify a model config. For example, when configuring the
<a href="/en/embedding.html#huggingface-embedder">Huggingface embedder</a>
provided by Vespa, you can write:</p>

<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="e5" type="hugging-face-embedder">
        <transformer-model model-id="e5-small-v2"/>
    </component>
    ...
</container>
{% endhighlight %}</pre>

<p>With this, your application will have support for
<a href="/en/embedding.html#embedding-a-query-text">text embedding</a>
inference for both queries and documents. Nodes that have been provisioned with GPU acceleration, will automatically
use GPU for embedding inference.</p>



<h2 id="vespa-cloud-embedding-models">Vespa Cloud Embedding Models</h2>
<p>Models on Vespa model hub are selected open-source embedding models with
great performance. See the <a href="https://huggingface.co/spaces/mteb/leaderboard">Massive Text Embedding Benchmark (MTEB) Leaderboard</a> for details.
These embedding models are useful for retrieval (semantic search), re-ranking, clustering, classification, and more.</p>


<h3 id="hugging-face-embedder">Huggingface Embedder</h3>
<p>These models are available for the Huggingface Embedder <code>type="hugging-face-embedder"</code>.
All these models supports both mapping from <code>string</code> or <code>array&lt;string&gt;</code> to tensor representations.
The output tensor <a href="/en/performance/feature-tuning.html#cell-value-types">cell-precision</a>
can be <code>&lt;float&gt; </code> or <code>&lt;bfloat16&gt;</code>.</p>
<table class="table">
    <thead>
    </thead>
    <tbody>

    <tr><th colspan="2" class="divider"><h4 id="nomic-ai-modernbert">nomic-ai-modernbert</h4></th></tr>
    <tr><td colspan="2"><p>
	 Trained from ModernBERT-base on the Nomic Embed datasets, bringing the new advances of ModernBERT to embeddings.
    </p></td></tr>
    <tr><td>Model id</td><td><code>nomic-ai-modernbert</code></td></tr>
    <tr><td>Tensor definition</td><td><code>tensor&lt;float&gt;(x[768])</code> (supports Matryoshka, so <code>x[256]</code> is also possible)</td></tr>
    <tr><td><a href="/en/reference/schema-reference.html#distance-metric">distance-metric</a></td><td><code>angular</code></td></tr>
    <tr><td>License</td><td><a href="https://www.apache.org/licenses/LICENSE-2.0">apache-2.0</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/nomic-ai/modernbert-embed-base">https://huggingface.co/nomic-ai/modernbert-embed-base</a> @ 92168cb</td></tr>
    <tr><td>Language</td><td>English</td></tr>
    <tr><td>Component declaration</td><td>
    <div class="pre-parent">
      <button class="d-icon d-duplicate pre-copy-button" onclick="copyPreContent(this)"></button>
<pre>{% highlight xml %}
    <component id="my-embedder-id" type="hugging-face-embedder">
        <transformer-model model-id="nomic-ai-modernbert"/>
        <transformer-output>token_embeddings</transformer-output>
        <max-tokens>8192</max-tokens>
        <prepend>
            <query>search_query:</query>
            <document>search_document:</document>
        </prepend>
    </component>
{% endhighlight %}</pre>
    </div>
    </td></tr>

    <tr><th colspan="2" class="divider"><h4 id="lightonai-modernbert-large">lightonai-modernbert-large</h4></th></tr>
    <tr><td colspan="2"><p>
	 Trained from ModernBERT-large on the Nomic Embed datasets, bringing the new advances of ModernBERT to embeddings.
    </p></td></tr>
    <tr><td>Model id</td><td><code>lightonai-modernbert-large</code></td></tr>
    <tr><td>Tensor definition</td><td><code>tensor&lt;float&gt;(x[1024])</code> </td></tr>
    <tr><td><a href="/en/reference/schema-reference.html#distance-metric">distance-metric</a></td><td><code>angular</code></td></tr>
    <tr><td>License</td><td><a href="https://www.apache.org/licenses/LICENSE-2.0">apache-2.0</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/lightonai/modernbert-embed-large">https://huggingface.co/lightonai/modernbert-embed-large</a> @ b3a781f</td></tr>
    <tr><td>Language</td><td>English</td></tr>
    <tr><td>Component declaration</td><td>
    <div class="pre-parent">
      <button class="d-icon d-duplicate pre-copy-button" onclick="copyPreContent(this)"></button>
<pre>{% highlight xml %}
    <component id="my-embedder-id" type="hugging-face-embedder">
        <transformer-model model-id="lightonai-modernbert-large"/>
        <max-tokens>8192</max-tokens>
        <prepend>
            <query>search_query:</query>
            <document>search_document:</document>
        </prepend>
    </component>
{% endhighlight %}</pre>
    </div>
    </td></tr>

    <tr><th colspan="2" class="divider"><h4 id="alibaba-gte-modernbert">alibaba-gte-modernbert</h4></th></tr>
    <tr><td colspan="2">
    <p>GTE (General Text Embedding) model trained from ModernBERT-base.</p>
    </td></tr>
    <tr><td>Model id</td><td><code>alibaba-gte-modernbert</code></td></tr>
    <tr><td>Tensor definition</td><td><code>tensor&lt;float&gt;(x[768])</code> (supports Matryoshka, so <code>x[256]</code> is also possible)</td></tr>
    <tr><td><a href="/en/reference/schema-reference.html#distance-metric">distance-metric</a></td><td><code>angular</code></td></tr>
    <tr><td>License</td><td><a href="https://www.apache.org/licenses/LICENSE-2.0">apache-2.0</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/Alibaba-NLP/gte-modernbert-base">https://huggingface.co/Alibaba-NLP/gte-modernbert-base</a> @ 3ab3f8c</td></tr>
    <tr><td>Language</td><td>English</td></tr>
    <tr><td>Component declaration</td><td>
    <div class="pre-parent">
      <button class="d-icon d-duplicate pre-copy-button" onclick="copyPreContent(this)"></button>
<pre>{% highlight xml %}
    <component id="my-embedder-id" type="hugging-face-embedder">
        <transformer-model model-id="alibaba-gte-modernbert"/>
        <max-tokens>8192</max-tokens>
        <pooling-strategy>cls</pooling-strategy>
    </component>
{% endhighlight %}</pre>
    </div>
    </td></tr>

    <tr><th colspan="2" class="divider"><h4 id="e5-small-v2">e5-small-v2</h4></th></tr>
    <tr><td colspan="2">
    <p>The smallest and most cost-efficient model from the <em>E5</em> family.</p>
    </td></tr>
    <tr><td>Model-id</td><td>e5-small-v2</td></tr>
    <tr><td>Tensor definition</td><td><code>tensor&lt;float&gt;(x[384])</code> or <code>tensor&lt;float&gt;(p{},x[384])</code></td></tr>
    <tr><td><a href="/en/reference/schema-reference.html#distance-metric">distance-metric</a></td><td><code>angular</code></td></tr>
    <tr><td>License</td><td><a href="https://github.com/microsoft/unilm/blob/master/LICENSE">MIT</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/intfloat/e5-small-v2">https://huggingface.co/intfloat/e5-small-v2</a></td></tr>
    <tr><td>Language</td><td>English</td></tr>
    <tr><td>Comment</td><td>See <a href="#using-e5-models">using E5 models</a></td></tr>

    <tr><th colspan="2" class="divider"><h4 id="e5-base-v2">e5-base-v2</h4></th></tr>
    <tr><td colspan="2">
    <p>The base model of the <em>E5</em> family.</p>
    </td></tr>
    <tr><td>Model-id</td><td>e5-base-v2</td></tr>
    <tr><td>Tensor definition</td><td><code>tensor&lt;float&gt;(x[768])</code> or <code>tensor&lt;float&gt;(p{},x[768])</code></td></tr>
    <tr><td><a href="/en/reference/schema-reference.html#distance-metric">distance-metric</a></td><td><code>angular</code></td></tr>
    <tr><td>License</td><td><a href="https://github.com/microsoft/unilm/blob/master/LICENSE">MIT</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/intfloat/e5-base-v2">https://huggingface.co/intfloat/e5-base-v2</a></td></tr>
    <tr><td>Language</td><td>English</td></tr>
    <tr><td>Comment</td><td>See <a href="#using-e5-models">using E5 models</a></td></tr>


    <tr><th colspan="2" class="divider"><h4 id="e5-large-v2">e5-large-v2</h4></th></tr>
    <tr><td colspan="2">
    <p>The largest model of the <em>E5</em> family, at time of writing, this is the best performing
    embedding model on the MTEB benchmark.</p>
    </td></tr>
    <tr><td>Model-id</td><td>e5-large-v2</td></tr>
    <tr><td>Tensor definition</td><td><code>tensor&lt;float&gt;(x[1024])</code> or <code>tensor&lt;float&gt;(p{},x[1024])</code></td></tr>
    <tr><td><a href="/en/reference/schema-reference.html#distance-metric">distance-metric</a></td><td><code>angular</code></td></tr>
    <tr><td>License</td><td><a href="https://github.com/microsoft/unilm/blob/master/LICENSE">MIT</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/intfloat/e5-large-v2">https://huggingface.co/intfloat/e5-large-v2</a></td></tr>
    <tr><td>Language</td><td>English</td></tr>
    <tr><td>Comment</td><td>See <a href="#using-e5-models">using E5 models</a></td></tr>


    <tr><th colspan="2" class="divider"><h4 id="multilingual-e5-base">multilingual-e5-base</h4></th></tr>
    <tr><td colspan="2">
    <p>The multilingual model of the <em>E5</em> family. Use this model for multilingual queries and documents.</p>
    </td></tr>
    <tr><td>Model-id</td><td>multilingual-e5-base</td></tr>
    <tr><td>Tensor definition</td><td><code>tensor&lt;float&gt;(x[768])</code> or <code>tensor&lt;float&gt;(p{},x[768])</code></td></tr>
    <tr><td><a href="/en/reference/schema-reference.html#distance-metric">distance-metric</a></td><td><code>angular</code></td></tr>
    <tr><td>License</td><td><a href="https://github.com/microsoft/unilm/blob/master/LICENSE">MIT</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/intfloat/multilingual-e5-base">https://huggingface.co/intfloat/multilingual-e5-base</a></td></tr>
    <tr><td>Language</td><td>Multilingual</td></tr>
    <tr><td>Comment</td><td>See <a href="#using-e5-models">using E5 models</a></td></tr>

    </tbody>
</table>

<h4 id="using-e5-models">Using E5 Models</h4>
<p>The E5 family uses keywords with the input to differentiate query and document side embedding.</p>
<p id="e5-queries">
  The query text should be prefixed with <em>"query: "</em>.
  In this example the original user query is <em>how to format e5 queries</em>.</p>
<pre>{% highlight json %}
{
  "yql": "select doc_id from doc where ({targetHits:10}nearestNeighbor(embeddings,e))",
  "input.query(e)": "embed(e5, \"query: how to format e5 queries\")"
}
{% endhighlight %}</pre>
<p>
The same technique also must be applied for document side embedding inference.
The input text should be prefixed with <em>"passage: "</em></p>
<pre>{% highlight raw %}
field embeddings type tensor<float>(p{}, x[768]) {
  indexing {
    input chunks |
	  for_each {
	    "passage: " . (input title || "") . " " . ( _ || "")
	  } | embed e5 | attribute
  }
}
{% endhighlight %}</pre>
<p>
The above example reads a <code>chunks</code> field of type <code>array&lt;string&gt;</code>,
and prefixes each item with "passage: ", followed by the concatenation
of the title and the item chunk (<em>_</em>).
See <a href="/en/indexing.html#execution-value-example">execution value example</a>.</p>


<h3 id="bert-embedder">Bert Embedder</h3>
<p>These models are available for the <a href="/en/embedding.html#bert-embedder">Bert Embedder</a>
<code>type="bert-embedder"</code>:</p>

<pre>{% highlight xml %}
<container id="default" version="1.0">
    <component id="mini" type="bert-embedder">
        <transformer-model model-id="minilm-l6-v2"/>
        <tokenizer-vocab model-id="bert-base-uncased"/>
    </component>
    ...
</container>
{% endhighlight %}</pre>
<p>Note bert-embedder requires both <code>transformer-model</code> and <code>tokenizer-vocab</code>.</p>
<table class="table">
    <thead>
    </thead>
    <tbody>
    <tr><th colspan="2" class="divider"><h4 id="minilm-l6-v2">minilm-l6-v2</h4></th></tr>
    <tr><td colspan="2">
    <p>A small, fast sentence-transformer model.</p>
    </td></tr>
    <tr><td>Model-id</td><td>minilm-l6-v2</td></tr>
    <tr><td>Tensor definition</td><td><code>tensor&lt;float&gt;(x[384])</code> or <code>tensor&lt;float&gt;(p{},x[384])</code></td></tr>
    <tr><td><a href="/en/reference/schema-reference.html#distance-metric">distance-metric</a></td><td><code>angular</code></td></tr>
    <tr><td>License</td><td><a href="https://www.apache.org/licenses/LICENSE-2.0">apache-2.0</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></td></tr>
    <tr><td>Language</td><td>English</td></tr>

    <tr><th colspan="2" class="divider"><h4 id="mpnet-base-v2">mpnet-base-v2</h4></th></tr>
    <tr><td colspan="2">
    <p>A larger, but better than <strong>minilm-l6-v2</strong> sentence-transformer model.</p>
    </td></tr>
    <tr><td>Model-id</td><td>mpnet-base-v2</td></tr>
    <tr><td>Tensor definition</td><td><code>tensor&lt;float&gt;(x[768])</code> or <code>tensor&lt;float&gt;(p{},x[768])</code></td></tr>
    <tr><td><a href="/en/reference/schema-reference.html#distance-metric">distance-metric</a></td><td><code>angular</code></td></tr>
    <tr><td>License</td><td><a href="https://www.apache.org/licenses/LICENSE-2.0">apache-2.0</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2">https://huggingface.co/sentence-transformers/all-mpnet-base-v2</a></td></tr>
    <tr><td>Language</td><td>English</td></tr>
    </tbody>
</table>


<h3 id="tokenization-embedders">Tokenization Embedders</h3>
<p>
  These are embedder implementations that tokenize text and embed string to the vocabulary identifiers.
  These are most useful for creating the tensor inputs to re-ranking models that take both the query and document token identifiers as input.
  Find examples in the
  <a href="https://github.com/vespa-engine/sample-apps/blob/master/README.md#vector-search-hybrid-search-and-embeddings">sample applications</a>.
</p>
<table class="table">
    <thead>
    </thead>
    <tbody>
    <tr><th colspan="2" class="divider"><h4 id="bert-base-uncased">bert-base-uncased</h4></th></tr>
    <tr><td colspan="2">
        A vocabulary text (<em>vocab.txt</em>) file on the format expected by
        <a href="/en/reference/embedding-reference#wordpiece-embedder">WordPiece</a>:
        A text token per line.
    </td></tr>
    <tr><td>Model-id</td><td>bert-base-uncased</td></tr>
    <tr><td>License</td><td><a href="https://www.apache.org/licenses/LICENSE-2.0">apache-2.0</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></td></tr>

    <tr><th colspan="2" class="divider"><h4 id="e5-base-v2-vocab">e5-base-v2-vocab</h4></th></tr>
    <tr><td colspan="2">
        A <em>tokenizer.json</em> configuration file on the format expected by
        <a href="/en/reference/embedding-reference#huggingface-tokenizer-embedder">HF tokenizer</a>.
        This tokenizer configuration can be used with <code>e5-base-v2</code>, <code>e5-small-v2</code> and <code>e5-large-v2</code>.
    </td></tr>
    <tr><td>Model-id</td><td>e5-base-v2-vocab</td></tr>
    <tr><td>License</td><td><a href="https://github.com/microsoft/unilm/blob/master/LICENSE">MIT</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/intfloat/e5-base-v2">https://huggingface.co/intfloat/e5-base-v2</a></td></tr>
    <tr><td>Language</td><td>English</td></tr>


    <tr><th colspan="2" class="divider"><h4 id="multilingual-e5-base-vocab">multilingual-e5-base-vocab</h4></th></tr>
    <tr><td colspan="2">
        A <em>tokenizer.json</em> configuration file on the format expected by
        <a href="/en/reference/embedding-reference#huggingface-tokenizer-embedder">HF tokenizer</a>.
        This tokenizer configuration can be used with <code>multilingual-e5-base-vocab</code>.
    <tr><td>Model-id</td><td>multilingual-e5-base-vocab</td></tr>
    <tr><td>License</td><td><a href="https://github.com/microsoft/unilm/blob/master/LICENSE">MIT</a></td></tr>
    <tr><td>Source</td><td><a href="https://huggingface.co/intfloat/multilingual-e5-base">https://huggingface.co/intfloat/multilingual-e5-base</a></td></tr>
    <tr><td>Language</td><td>Multilingual</td></tr>
  </tbody>
</table>


<h3 id="significance-models">Significance models</h3>
<p>
  These are <a href="/en/significance.html#significance-models-in-servicesxml">global significance models</a>
  that can be added to <a href="/en/reference/services-search.html#significance">significance element in services.xml</a>.
</p>
<table class="table">
    <thead>
    </thead>
    <tbody>
    <tr><th colspan="2" class="divider"><h4 id="significance-en-wikipedia-v1">significance-en-wikipedia-v1</h4></th></tr>
    <tr><td colspan="2">
        This significance model was generated from <a href="https://dumps.wikimedia.org/enwiki/20240801/" data-proofer-ignore>English Wikipedia dump data from 2024-08-01</a>.
        Available in Vespa as of version 8.426.8.
    </td></tr>
    <tr><td>Model-id</td><td>significance-en-wikipedia-v1</td></tr>
    <tr><td>License</td><td><a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en">Creative Commons Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0) License</a>.</td></tr>
    <tr><td>Source</td><td><a href="https://data.vespa-cloud.com/significance_models/significance-en-wikipedia-v1.json.zst">https://data.vespa-cloud.com/significance_models/significance-en-wikipedia-v1.json.zst</a></td></tr>
    <tr><td>Language</td><td>English</td></tr>
    </tbody>
</table>



<h2 id="creating-applications-working-both-self-hosted-and-on-vespa-cloud">Creating applications working both self-hosted and on Vespa Cloud</h2>

<p>You can also specify both a <code>model-id</code>, which will be used on Vespa Cloud,
and a url/path, which will be used on self-hosted deployments:</p>

<pre>{% highlight xml %}
<transformer-model model-id="minilm-l6-v2" path="myAppPackageModels/myModel.onnx"/>
{% endhighlight%}</pre>

<p>This can be useful for example to create an application package which uses models from Vespa Cloud
for production and a scaled-down or dummy model for self-hosted development.</p>



<h2 id="using-vespa-cloud-models-with-any-config">Using Vespa Cloud models with any config</h2>

<p>Specifying a model-id can be done for any
<a href="/en/configuring-components.html#adding-files-to-the-component-configuration">config field of type <code>model</code></a>,
whether the config is from Vespa or defined by you.</p>

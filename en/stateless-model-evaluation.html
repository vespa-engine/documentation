---
# Copyright Yahoo. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Stateless Model Evaluation"
redirect_from:
- /documentation/stateless-model-evaluation.html
---

<p>
Vespa's speciality is evaluating machine-learned models quickly over large numbers of data points.
However, it can also be used to evaluate models once on request in stateless containers.
By enabling a feature in <a href="reference/services.html">services.xml</a>,
all machine-learned models -
<a href="tensorflow.html">TensorFlow</a>,
<a href="onnx.html">Onnx</a>,
<a href="xgboost.html">XGBoost</a>,
<a href="lightgbm.html">LightGBM</a> and
<a href="reference/stateless-model-reference.html">Vespa stateless models</a> -
added to the <code>models/</code> directory of the
<a href="reference/application-packages-reference.html">application package</a>,
are made available through both a REST API and a Java API
where you can compute inferences from your own code.
</p><p>
An example application package can be found at in the
<a href="https://github.com/vespa-engine/system-test/tree/master/tests/container/model_evaluation/app">
model-evaluation system test</a>.
</p>


<h3 id="add-the-model-evaluation-tag">The model evaluation tag</h3>

<p>To enable both the REST API and the Java API, add the <code>model-evaluation</code> tag
inside the <a href="jdisc/">container</a> clusters where
it is needed in <a href="reference/services.html">services.xml</a>:
</p>
<pre>
&lt;container&gt;
    ...
    <span class="pre-hilite">&lt;model-evaluation/&gt;</span>
    ...
&lt;/container&gt;
</pre>
<p>
The <code>model-evaluation</code> section can optionally contain inference session options for
ONNX models. See <a href="#onnx-inference-options">ONNX inference options</a>.
</p>



<h2 id="model-inference-using-the-rest-api">Model inference using the REST API</h2>

<p>The simplest way to evaluate the model is to use the REST API.
After enabling it as above, a new API path is made available:
<code>/model-evaluation/v1/</code>.
To discover and find information about the models
(including expected input parameters to the model) in your application package,
simply follow the links from this root.
To evaluate a model add <code>/eval</code> to the query path:
</p>
<pre>
http://host:port/model-evaluation/v1/&lt;model-name&gt;/&lt;function&gt;/eval?&lt;param1=...&gt;&amp;...
</pre>
<p>
Here &lt;model-name&gt; signifies which model to evaluate as you can deploy
multiple models in your application package.
The &lt;function&gt; specifies
which signature and output to evaluate as a model might have multiple
signatures and outputs you can evaluate.
If a model only has one function, this can be omitted.
Inputs to the model are specified as query parameters for GET requests,
and they can also be in the body part of the request for POST requests.
The expected format for input parameters are tensors as specified
with the <a href="reference/tensor.html#tensor-literal-form">literal form</a>.
</p>

<p>
See the
<a href="https://github.com/vespa-engine/sample-apps/tree/master/model-inference">
model-inference sample app</a> for an example of this.
</p>

<h3>Model evaluation REST API parameters</h3>

<p>Model evaluation requests accepts these request parameters:</p>

<table class="table">
    <thead>
    <tr>
        <th>Parameter</th>
        <th>Type</th>
        <th>Description</th>
    </tr>
    </thead><tbody>
<tr>
    <th>tensor.format</th>
    <td>String</td>
    <td>
        <p id="format.tensors">
            Controls how tensors are rendered in the result.
        </p>
        <table class="table">
            <thead>
            <tr>
                <th>Value</th>
                <th>Description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td><code>short</code></td>
                <td>
                    <b>Default</b>. Render the tensor value in a JSON object having two keys, "type" containing the value,
                    and "cells"/"blocks"/"values" (<a href="reference/document-json-format.html#tensor">depending on the type</a>) containing the tensor content.<br/>
                    Render the tensor content in the <a href="reference/document-json-format.html#tensor">type-appropriate short form</a>.
                </td>
            </tr>
            <tr>
                <td><code>long</code></td>
                <td>
                    Render the tensor value in a JSON object having two keys, "type" containing the value,
                    and "cells" containing the tensor content.<br/>
                    Render the tensor content in the <a href="reference/document-json-format.html#tensor">general verbose form</a>.
                </td>
            </tr>
            <tr>
                <td><code>short-value</code></td>
                <td>
                    Render the tensor content directly as a JSON value.<br/>
                    Render the tensor content in the <a href="reference/document-json-format.html#tensor">type-appropriate short form</a>.
                </td>
            </tr>
            <tr>
                <td><code>long-value</code></td>
                <td>
                    Render the tensor content directly as a JSON value.<br/>
                    Render the tensor content in the <a href="reference/document-json-format.html#tensor">general verbose form</a>.
                </td>
            </tr>
            <tr>
                <td><code>string</code></td>
                <td>
                    Render the tensor content as a string on the <a href="reference/tensor.html#general-literal-form">general literal form</a>.
                </td>
            </tr>
            <tr>
                <td><code>string-short</code></td>
                <td>
                    Render the tensor content as a string on the <a href="reference/tensor.html#tensor-literal-form">appropriate literal short form</a>.
                </td>
            </tr>
        </table>
    </td>
</tr>
</table>

<h2 id="model-inference-using-java">Model inference using Java</h2>
<p>
While the REST API gives a basic interface to run model inference,
the Java interface offers far more control
allowing you to for instance implement custom input and output formats.
</p><p>
First, add the following dependency in <code>pom.xml</code>:
</p>
<pre>
&lt;dependency&gt;
    &lt;groupId&gt;com.yahoo.vespa&lt;/groupId&gt;
    &lt;artifactId&gt;container&lt;/artifactId&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
</pre>
<p>
(Or, if you want the minimal dependency,
depend on <code>model-evaluation</code> instead of <code>container</code>.)
</p><p>
With the above dependency and the <code>model-evaluation</code> element
added to <code>services.xml</code>,
you can now have your Java component that should evaluate models
take a <code>ai.vespa.models.evaluation.ModelsEvaluator</code>
(see <a href="https://github.com/vespa-engine/vespa/blob/master/model-evaluation/src/main/java/ai/vespa/models/evaluation/ModelsEvaluator.java">
ModelsEvaluator.java</a>) instance as a constructor argument
(Vespa will <a href="jdisc/injecting-components.html">automatically inject it</a>).
</p><p>
Use the <code>ModelsEvaluator</code> API (from any thread) to make inferences. Sample code:
</p>
<pre>{% highlight java %}
import ai.vespa.models.evaluation.ModelsEvaluator;
import ai.vespa.models.evaluation.FunctionEvaluator;
import com.yahoo.tensor.Tensor;

// ...

// Create evaluator
FunctionEvaluator evaluator = modelsEvaluator.evaluatorOf("myModel", "mySignature", "myOutput"); // Unambiguous args may be skipped

// Get model inputs for instance from query (here we just construct a sample tensor)
Tensor.Builder b = Tensor.Builder.of(new TensorType.Builder().indexed("d0", 3));
b.cell(0.1, 0);
b.cell(0.2, 0);
b.cell(0.3, 0);
Tensor input = b.build();

// Bind inputs to the evaluator
evaluator.bind("myInput", input);

// Evaluate model. Note: Evaluator must be discarded after a single use
Tensor result = evaluator.evaluate());

// Do something with the result
{% endhighlight %}</pre>
<p>
The <a href="https://github.com/vespa-engine/sample-apps/tree/master/model-inference">
model-inference sample app</a> also has an example of this.
</p>



<h2 id="unit-testing-model-evaluation-in-java">Unit testing model evaluation in Java</h2>
<p>
When developing your application it can be helpful to unit test your
models and/or your searchers and document processors during development. Vespa
provides a <code>ModelsEvaluatorTester</code> which can be constructed from the
contents of your "models" directory. This allows for testing that the model
works as expected in context of Vespa, and that your searcher or document
processor gets the correct results from your models.
</p>
<p>
The following dependency is needed in <code>pom.xml</code>:
</p>
<pre>
&lt;dependency&gt;
    &lt;groupId&gt;com.yahoo.vespa&lt;/groupId&gt;
    &lt;artifactId&gt;container-test&lt;/artifactId&gt;
    &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;
</pre>
<p>
With this you can construct a testable <code>ModelsEvaluator</code>:
</p>
<pre>{% highlight java %}
import com.yahoo.vespa.model.container.ml.ModelsEvaluatorTester;

public class ModelsTest {
    @Test
    public void testModels() {
        ModelsEvaluator modelsEvaluator = ModelsEvaluatorTester.create("src/main/application/models");

        // Test the modelsEvaluator directly or construct a searcher and pass it in

    }
}
{% endhighlight %}</pre>
<p>
The <code>ModelsEvaluator</code> object that is returned contains all models
found under the directory pass in. Note that this should only be used in unit
testing.
</p>
<p>
The <a href="https://github.com/vespa-engine/sample-apps/tree/master/model-inference">
model-inference sample app</a> uses this for testing handlers, searchers, and document
processors.
</p>


<h2 id="onnx-inference-options">ONNX inference options</h2>
<p>
ONNX models are evaluated using <a href="https://onnxruntime.ai/">ONNX Runtime</a>.
Vespa provides the following options to tune inference:
</p>
<pre>
&lt;model-evaluation&gt;
    &lt;onnx&gt;
        &lt;models&gt;
            &lt;model name="reranker_margin_loss_v4"&gt;
                &lt;intraop-threads&gt; number &lt;/intraop-threads&gt;
                &lt;interop-threads&gt; number &lt;/interop-threads&gt;
                &lt;execution-mode&gt;parallel | sequential&lt;/execution-mode&gt;
            &lt;/model&gt;
        &lt;/models&gt;
    &lt;/onnx&gt;
&lt;/model-evaluation&gt;
</pre>
<p>
Inter-op threads is the number of threads available for running multiple
operations in parallel. This is only applicable for <code>execution mode=parallel</code>.
The intra-op threads is the number of threads available for running operations with
multi-threaded implementations.
</p>
<p>
Since stateless model evaluation is based on auto-discovery of models under the
<code>models</code> directory in the application package, the above would only be needed for
models that should not use the default settings.
</p>
<p>
The defaults are <code>execution-mode=sequential</code>, <code>interop-threads=1</code>
and <code>intraop-threads=CPUs / 4</code>.
</p>

---
# Copyright Vespa.ai. All rights reserved.
title: Install Vespa on Kubernetes
applies_to: enterprise
---

<p>
  The Vespa Operator should be installed using the official Helm chart.<br/>
  The operator depends on the installation of the <code>VespaSet</code> <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definition</a> (CRD), which is defined at the Kubernetes cluster scope.<br/>
  Through the Helm Chart, the installation of the CRD and the required RBAC permissions can be simplified. The required permissions are listed in the <a href="#permissions">Permissions</a> section.
</p>

<p>
  Our container registry is located at <code>images.ves.pa</code>.<br/>
  For accessing the required <em></em>Vespa on Kubernetes</em> container images (and helm chart) you will need to contact us through our support portal, we will provide you with a the authentication id and token.<br>

  <strong>Important:</strong> For production use, we <em>require</em> mirroring these images into your own registry or a well-known internal repository appropriate for your infrastructure!
</p>

<p>
  Our support team will provide you with the required credentials, you will have access to the following:
</p>
<ul>
  <li>the <em>Vespa on Kubernetes</em> image, corresponding to <code>$OCI_IMAGE_REFERENCE</code> in this guide,</li>
  <li>the <em>Vespa Operator</em> image, corresponding to <code>$OCI_IMAGE_REFERENCE_OPERATOR</code> in this guide,</li>
  <li>the official <em>Vespa Helm Chart</em> OCI, referred to as <code>$HELM_OCI_CHART_REFERENCE</code> in this guide.</li>
</ul>

<h2>Requirements</h2>

<p>
  The following tools are encouraged for a smooth deployment.
</p>

<ul>
  <li><a href="https://helm.sh/docs/intro/install/">Helm CLI</a></li>
  <li>Kubernetes Command Line Tool (<a href="https://kubernetes.io/docs/reference/kubectl/">kubectl</a>)</li>
  <li><a href="https://docs.vespa.ai/en/clients/vespa-cli.html">Vespa CLI</a></li>
  <li>the Vespa on Kubernetes and Vespa Operator images</li>
  <li>the Vespa Helm Chart</li>
  <li><a href="https://minikube.sigs.k8s.io/docs/">MiniKube (Optional)</a></li>
  <li>Podman or Docker</li>
</ul>

<p>
  These instructions assume that <code>kubeconfig</code> is pointing to an active Kubernetes cluster. Refer to the <a href="https://kubernetes.io/docs/setup/">Getting Started</a> guide to create a Kubernetes cluster.
</p>

<h2>MiniKube Setup</h2>

<p>
  MiniKube allows for simple local testing of Vespa on Kubernetes. Skip to the <a href="#install-helm-chart">Deploy Vespa Operator</a> section below to directly start installation.
</p>

<p>
  Initialize a Minikube cluster with 8 nodes, 2CPUs and 4GiB memory each ausing the Podman driver. Enable Minikube's image registry
  add-on to allow the Minikube nodes to access the image.
</p>

<pre>
# Start Minikube
minikube start --nodes 8 --cpus 2 --memory 4GiB --driver=podman

# Enable Image Registry add-on
minikube addons enable registry

# Verify MiniKube cluster was created
minikube status
</pre>

<pre>
# Authenticate to the registry
echo $VESPAAI_REGISTRY_TOKEN | podman login images.ves.pa \
  -u "$VESPAAI_REGISTRY_USER" \
  --password-stdin

# Cache the images locally
podman pull images.ves.pa/kubernetes/vespa:$VESPA_VERSION
podman pull images.ves.pa/kubernetes/operator:$VESPA_VERSION
</pre>

<p>
  Once the Minikube cluster is set up, push the images to the MiniKube registry, to make accessible to the MiniKube nodes. The MiniKube registry is accessible from
  <code>$(minikube ip):5000</code>.
</p>

<pre>
# Save the minikube registry endpoint
$MINIKUBE_REGISTRY=$(minikube ip)

# Push the kubernetes/vespa image to the registry
podman tag  kubernetes/vespa:$VESPA_VERSION $MINIKUBE_REGISTRY:5000/localhost/kubernetes/vespa:$VESPA_VERSION
podman push --tls-verify=false $MINIKUBE_REGISTRY:5000/localhost/kubernetes/vespa:$VESPA_VERSION

# Push the kubernetes/operator image to the registry
podman tag  kubernetes/operator:$VESPA_VERSION $MINIKUBE_REGISTRY:5000/localhost/kubernetes/operator:$VESPA_VERSION
podman push --tls-verify=false $MINIKUBE_REGISTRY:5000/localhost/kubernetes/operator:$VESPA_VERSION
</pre>

<p>
  The images will now be available to the Minikube nodes from <code>$MINIKUBE_REGISTRY:5000/localhost/kubernetes/operator:$VESPA_VERSION</code>.
</p>

<pre>
export OCI_IMAGE_REFERENCE=$MINIKUBE_REGISTRY:5000/localhost/kubernetes/vespa
export OCI_IMAGE_REFERENCE_OPERATOR=$MINIKUBE_REGISTRY:5000/localhost/kubernetes/operator
export OCI_IMAGE_TAG=$VESPA_VERSION
</pre>

<p>
  Then, install the <a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">Local Persistent Volume</a> provisioner Helm Chart.
  This will allow Persistent Volumes to be created in a MiniKube environment. It will automatically create a StorageClass called `local-storage`, which should be
  used for the next steps.
</p>

<pre>
# Clone the local persistent volume static provisioner from the Kubernetes sigs
$ git clone git@github.com:kubernetes-sigs/sig-storage-local-static-provisioner.git

# Deploy onto the Kubernetes cluster
$ helm install -f helm/examples/baremetal-default-storage.yaml local-volume-provisioner --namespace kube-system ./helm/provisioner

# Create several volumes on each Minikube node.
$ for n in minikube minikube-m02 minikube-m03 minikube-m04 minikube-m05 minikube-m06 minikube-m07 minikube-m08; do
  echo "==> $n"
  minikube ssh -n "$n" -- '
    set -e
    for i in 1 2 3 4; do
      sudo mkdir -p /mnt/disks/vol$i
      if ! mountpoint -q /mnt/disks/vol$i; then
        sudo mount --bind /mnt/disks/vol$i /mnt/disks/vol$i
      fi
    done
    echo "Mounted:"
    mount | grep -E "/mnt/disks/vol[1-4]" || true
  '
done
</pre>

<h2 id="install-helm-chart">Deploy Vespa Operator</h2>
<p>
  The Helm Chart installs the Vespa Operator and the <code>Role</code>, <code>RoleBinding</code>, and <code>ServiceAccount</code> resources with the necessary permissions to operate Vespa. Optionally, the
  CRD specification can be installed onto the Kubernetes cluster.
</p>

<p>
  An installation can be performed as follows. This will deploy the Vespa Operator to the <code>vespa</code> namespace and apply
  the <code>VespaSet</code> CRD specification to the cluster.
</p>
<pre>
$ helm install vespa-operator $HELM_OCI_CHART_REFERENCE --namespace vespa --create-namespace --set image.repository $OCI_IMAGE_REFERENCE_OPERATOR --set image.tag $OCI_IMAGE_TAG
</pre>
<p>
  If CRDs are managed separately, the CRD installation can be disabled. However, the CRD specification must
  be manually applied to the cluster before installing the Helm Chart. Our support team can provide this specification if necessary.
</p>

<pre>
$ kubectl apply vespasets.k8s.ai.vespa-v1.yaml
$ helm install vespa-operator $HELM_OCI_CHART_REFERENCE --namespace $NAMESPACE --create-namespace --skip-crds --set image.repository $OCI_IMAGE_REFERENCE_OPERATOR --set image.tag $OCI_IMAGE_TAG
</pre>

<p>
  Ensure that the <code>Deployment</code> resource was successfully applied and the operator <code>Pod</code> was created. It can be done
  using the following check.
</p>

<pre>
$ kubectl wait --for=condition=available deployment/vespa-operator --timeout=120s -n $NAMESPACE \
&& kubectl get pods -l app=vespa-operator -o wide -n $NAMESPACE
</pre>

<p>
  The full reference guide for the Helm Chart can be found in the <a href="helm-reference.html">Helm Reference</a> section.
</p>

<h2>
  Deploy a VespaSet
</h2>

<p>
  A <code>VespaSet</code> represents a quorum of <a href="https://docs.vespa.ai/en/operations/self-managed/configuration-server.html">ConfigServers</a> that manage Vespa applications. Several examples of
  <code>VespaSet</code> specifications are provided in the Helm Chart <code>samples</code> directory.
  A sample <code>VespaSet</code> resource for <a href="https://aws.amazon.com/eks/">Amazon Elastic Kubernetes Service</a> (EKS) is shown below.
</p>

<pre>
# VespaSet configuration for AWS EKS
apiVersion: k8s.ai.vespa/v1
kind: VespaSet
metadata:
  name: vespaset-sample
  namespace: vespa
spec:
  version: $OCI_IMAGE_TAG

  configServer:
    image: "$OCI_IMAGE_REFERENCE"
    storageClass: "gp3"
    generateRbac: false

  application:
    image: "$OCI_IMAGE_REFERENCE"
    storageClass: "gp3"

  ingress:
    endpointType: "NODE_PORT"
</pre>

<p>
  An example for MiniKube would be as follows.
</p>

<pre>
# VespaSet configuration for AWS EKS
apiVersion: k8s.ai.vespa/v1
kind: VespaSet
metadata:
  name: vespaset-sample
  namespace: vespa
spec:
  version: $OCI_IMAGE_TAG

  configServer:
    image: "$OCI_IMAGE_REFERENCE"
    storageClass: "local-storage"
    generateRbac: false

  application:
    image: "$OCI_IMAGE_REFERENCE"
    storageClass: "local-storage"

  ingress:
    endpointType: "NODE_PORT"
</pre>

<p>
  Note that the <code>$OCI_IMAGE_REFERENCE</code> is shared between the ConfigServer and the Vespa Application Pods.
</p>

<p>
  Apply the <code>VespaSet</code> resource to the Kubernetes Cluster. The operator will automatically detect the <code>VespaSet</code> and create a quorum of
  ConfigServers.
</p>

<p>
  The ConfigServers will then bootstrap the Vespa infrastructure. This process takes roughly a minute. The bootstrap process is completed once
  the <code>VespaSet</code> shows the status as `RUNNING` for all ConfigServer Pods.
</p>

<pre>
$ kubectl describe vespaset vespaset-sample -n vespa
Name:         vespaset-sample
Namespace:    vespa
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  k8s.ai.vespa/v1
Kind:         VespaSet
Metadata:
  Creation Timestamp:  2026-01-29T21:32:27Z
  Finalizers:
    vespasets.k8s.ai.vespa/finalizer
  Generation:        1
  Resource Version:  121822902
  UID:               a70f56e9-6625-4011-acd7-9f7cad29dbc2
Spec:
  Application:
    Image:          $OCI_IMAGE_REFERENCE
    Storage Class:  gp3
  Config Server:
    Generate Rbac:    false
    Image:            $OCI_IMAGE_REFERENCE
    Storage Class:    gp3
  Ingress:
    Endpoint Type:  NODE_PORT
  Version:          $OCI_IMAGE_TAG
Status:
  Bootstrap Status:
    Pods:
      cfg-0:
        Last Updated:  2026-01-29T21:38:45Z
        Message:       Pod is running
        Phase:         RUNNING
      cfg-1:
        Last Updated:  2026-01-29T21:38:09Z
        Message:       Pod is running
        Phase:         RUNNING
      cfg-2:
        Last Updated:  2026-01-29T21:36:32Z
        Message:       Pod is running
        Phase:         RUNNING
  Last Transition Time:  2026-01-29T21:33:55Z
  Message:               All configservers running
  Phase:                 RUNNING
Events:                  &lt;none&gt;
</pre>

<p>
  The status of the bootstrap process can be easily queried as follows.
</p>

<pre>
$ kubectl get vespaset upgradetest-vespaset -n mramdenbourg-upgradetest -o json \
| jq -e '
  .status.bootstrapStatus.pods as $p
  | ($p["cfg-0"].phase == "RUNNING")
  and ($p["cfg-1"].phase == "RUNNING")
  and ($p["cfg-2"].phase == "RUNNING")
'
</pre>

<p>
  The full reference guide for the VespaSet CRD can be found in the <a href="vespaset-reference.html">VespaSet Reference</a> section.
</p>

<h2>Deploy a Vespa Application</h2>

<p>
  A Vespa application can be deployed once the bootstrap process has completed. Refer to the <a href="https://github.com/vespa-engine/sample-apps">Vespa Sample Applications</a>
  to get started. In the following example, we will use the <a href="https://github.com/vespa-engine/sample-apps/tree/master/album-recommendation">Album Recommendation</a> sample.
</p>

<p>
  Set up the Vespa CLI to download the Album Recommendation sample application to a working directory and use the local endpoints.
</p>

<pre>
$ vespa clone album-recommendation myapp && cd myapp
$ vespa config set target local
</pre>

<p>
  Enable a port-forward of the ConfigServer's ingress port to a port locally.
</p>

<pre>
$ kubectl -n mramdenbourg-upgradetest port-forward pod/cfg-0 19071:19071
</pre>

<p>
  The application can be deployed using the Vespa CLI.
</p>

<pre>
$ vespa deploy --wait 600
</pre>

<p>
  This will create the Container, Content, and Cluster-Controller Pods as specified in the application package. The deployment
  is considered complete once all Pods are in phase <code>RUNNING</code> in the <code>VespaSet</code> status. This can be queried
  as follows.
</p>

<pre>
$ kubectl get vespaset upgradetest-vespaset -n mramdenbourg-upgradetest -o json \
| jq -e '
  .status.bootstrapStatus.pods
  | with_entries(
      select(
        .key as $k
        | [
            "cluster-controllers-104",
            "cluster-controllers-105",
            "cluster-controllers-106",
            "default-100",
            "default-101",
            "documentation-102",
            "documentation-103"
          ]
        | index($k) | not
      )
    )
  | all(.phase == "RUNNING")
'
</pre>

<p>
  Note the names of the Pods may change depending on your specific Application Package configuration.
</p>

<p>
  Port forwarding provides a simple way to access the application locally.
  For other ingress options, see the <a href="ingress.html">Configuring the External Access Layer</a> section.
</p>

<h2>Feed Documents</h2>

<p>
  Feed documents to the Dataplane entrypoint by port-forwarding to both the Dataplane Pod and the ConfigServer Pod.
</p>

<pre>
$ kubectl -n mramdenbourg-upgradetest port-forward pod/cfg-0 19071:19071
$ kubectl -n mramdenbourg-upgradetest port-forward pod/default-100 8080:8080
</pre>

<p>
  Then, use the Vespa CLI to feed a document.
</p>

<pre>
vespa feed dataset/A-Head-Full-of-Dreams.json
</pre>

<h2 id="permissions">Permissions</h2>

<p>
  The Vespa Operator requires the following permissions.
  These permissions are listed by Kubernetes <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">API verbs</a> per resource.
</p>

<table class="table">
  <thead>
  <tr>
    <th>Kubernetes Resource</th>
    <th>Required Permissions</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>CustomResourceDefinitions</td>
    <td>create, get, list, watch</td>
  </tr>
  <tr>
    <td>VespaSet</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>VespaSet Subresources</td>
    <td>
      <code>vespasets/status</code>: update, patch<br/>
      <code>vespasets/finalizers</code>: update
    </td>
  </tr>
  <tr>
    <td>ConfigMaps</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>Services</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>Pods</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>Pod Execution</td>
    <td>get, create</td>
  </tr>
  <tr>
    <td>Events</td>
    <td>create, patch</td>
  </tr>
  <tr>
    <td>PersistentVolumeClaims</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>ServiceAccounts</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>Roles</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>RoleBindings</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  </tbody>
</table>

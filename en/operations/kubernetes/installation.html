---
# Copyright Vespa.ai. All rights reserved.
title: Install Vespa on Kubernetes
applies_to: enterprise
---

<p>
  The Vespa Operator should be installed using the official Helm chart. It depends on the installation of the <code>VespaSet</code> <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definition</a> (CRD), which is defined at the Kubernetes cluster scope.
  Through the Helm Chart, the installation of the CRD and the required RBAC permissions can be simplified. The required permissions are listed in the <a href="#permissions">Permissions</a> section.
</p>

<p>
  Our container registry is located at <code>images.ves.pa</code>. For accessing the required <em>Vespa on Kubernetes</em> container images (and helm chart) you will need to contact us through our support portal.
  We will provide you with the authentication id and token.
</p>

<p>
  <strong>Important:</strong> For production use, we <em>require</em> mirroring these images into your own registry or a well-known internal repository appropriate for your infrastructure!
</p>

<p>
  Our support team will provide you with credentials to access the Vespa on Kubernetes image, Vespa Operator image, and the Helm Chart. They will be referred to as shown below throughout the installation steps.
</p>

<pre>
# The Vespa on Kubernetes Image
export $OCI_IMAGE_REFERENCE=images.ves.pa/kubernetes/vespa

# The Vespa Operator Image
export $OCI_IMAGE_REFERENCE_OPERATOR=images.ves.pa/kubernetes/operator

# The Official Helm Chart
export $HELM_OCI_CHART_REFERENCE=oci://images.ves.pa/helm/vespa-operator
</pre>

<p>
  The Vespa Operator uses the <a href="https://docs.vespa.ai/en/learn/releases.html">Vespa Version</a> release semantics. We encourage to use the latest release during installation.
</p>

<pre>
# Vespa Version
export VESPA_VESRION=$VESPA_VERSION
</pre>

<p>
  Additionally, the Vespa Operator and all Vespa components are completely scoped to a namespace. We encourage choosing one to your liking.
</p>

<pre>
# Set the namespace
export NAMESPACE=test
</pre>

<h2>Requirements</h2>

<p>
  The following tools are encouraged for a smooth deployment.
</p>

<ul>
  <li><a href="https://helm.sh/docs/intro/install/">Helm</a></li>
  <li><a href="https://kubernetes.io/docs/reference/kubectl/">kubectl</a></li>
  <li><a href="https://docs.vespa.ai/en/clients/vespa-cli.html">Vespa CLI</a></li>
  <li><a href="https://minikube.sigs.k8s.io/docs/">MiniKube (Optional)</a></li>
  <li>Podman or Docker (Optional)</li>
</ul>

<p>
  These instructions assume that your <code>kubeconfig</code> is pointing to an active Kubernetes cluster. Refer to the <a href="https://kubernetes.io/docs/setup/">Getting Started</a> guide to create a Kubernetes cluster. For instructions on creating a
  local development environment, refer to the <a href="#minikube-setup">MiniKube Setup</a> section.
</p>

<h2 id="install-helm-chart">Deploy Vespa Operator</h2>
<p>
  The Helm Chart installs the Vespa Operator, <code>Role</code>, <code>RoleBinding</code>, and <code>ServiceAccount</code> resources with the permissions to operate Vespa. Optionally, the
  CRD specification can be installed onto the Kubernetes cluster.
</p>

<p>
  First, authenticate to the Helm Chart OCI registry. The credentials will be provided by our support team.
</p>

<pre>
helm registry login images.ves.pa -u $USER-p $TOKEN
</pre>

<p>
  Now, an installation can be performed as follows. This will deploy the Vespa Operator to the target namespace and apply
  the <code>VespaSet</code> CRD specification to the Kubernetes cluster. Set <code>image.repository</code> to the Vespa On Kubernetes Image provided by our support team.
  The <code>image.tag</code> refers to the Vespa Version to deploy.
</p>

<pre>
$ helm install vespa-operator $HELM_OCI_CHART_REFERENCE --namespace $NAMESPACE --create-namespace --set image.repository=$OCI_IMAGE_REFERENCE_OPERATOR --set image.tag=$VESPA_VERSION
</pre>

<p>
  If CRDs are managed separately, its installation can be disabled. However, the CRD specification must
  be manually applied to the Kubernetes cluster before installing the Helm Chart. Our support team can provide this specification if necessary.
</p>

<pre>
$ kubectl apply vespasets.k8s.ai.vespa-v1.yaml
$ helm install vespa-operator $HELM_OCI_CHART_REFERENCE --namespace $NAMESPACE --create-namespace --skip-crds --set image.repository $OCI_IMAGE_REFERENCE_OPERATOR --set image.tag $VESPA_VERSION
</pre>

<p>
  Ensure that the <code>Deployment</code> was successfully applied and the operator <code>Pod</code> was created. It can be done
  using the following check.
</p>

<pre>
$ kubectl wait --for=condition=available deployment/vespa-operator --timeout=120s -n $NAMESPACE \
&& kubectl get pods -l app=vespa-operator -o wide -n $NAMESPACE
</pre>

<h2>
  Deploy a VespaSet
</h2>

<p>
  A <code>VespaSet</code> represents a quorum of <a href="https://docs.vespa.ai/en/operations/self-managed/configuration-server.html">ConfigServers</a> that manage Vespa applications. Several examples of
  <code>VespaSet</code> specifications are provided in the Helm Chart <code>samples</code> directory.
  A sample <code>VespaSet</code> for <a href="https://aws.amazon.com/eks/">Amazon Elastic Kubernetes Service</a> (EKS) is shown below.
</p>

<pre>
# vespaset sample for EKS
$ cat &gt; vespaset.yaml &lt;&lt;EOF
apiVersion: k8s.ai.vespa/v1
kind: VespaSet
metadata:
  name: vespaset-sample
  namespace: ${NAMESPACE}
spec:
  version: "${VESPA_VERSION}"

  configServer:
    image: "${OCI_IMAGE_REFERENCE}"
    storageClass: "gp3"
    generateRbac: false

  application:
    image: "${OCI_IMAGE_REFERENCE}"
    storageClass: "gp3"

  ingress:
    endpointType: "LOAD_BALANCER"
EOF

$ kubectl apply -f vespaset.yaml
</pre>

<p>
  An example for a local deployment on MiniKube would be as follows.
</p>

<pre>
# vespaset sample for MiniKube
$ cat &gt; vespaset.yaml &lt;&lt;EOF
apiVersion: k8s.ai.vespa/v1
kind: VespaSet
metadata:
  name: vespaset-sample
  namespace: ${NAMESPACE}
spec:
  version: "${VESPA_VERSION}"

  configServer:
    image: "${OCI_IMAGE_REFERENCE}"
    storageClass: "local-storage"
    generateRbac: false

  application:
    image: "${OCI_IMAGE_REFERENCE}"
    storageClass: "local-storage"

  ingress:
    endpointType: "NONE"
EOF

$ kubectl apply -f vespaset.yaml
</pre>

<p>
  Note that the <code>$OCI_IMAGE_REFERENCE</code> is shared between the ConfigServer and the Vespa Application Pods.
</p>

<p>
  Once the <code>VespaSet</code> is applied, the operator will automatically detect the newly applied <code>VespaSet</code> and create a quorum of
  ConfigServers.
</p>

<p>
  The ConfigServers will then bootstrap the Vespa infrastructure. This process takes roughly a minute. The bootstrap process is completed once
  the <code>VespaSet</code> shows the status as <code>RUNNING</code> for all ConfigServer Pods.
</p>

<pre>
$ kubectl describe vespaset vespaset-sample -n $NAMESPACE
Name:         vespaset-sample
Namespace:    $NAMESPACE
API Version:  k8s.ai.vespa/v1
Kind:         VespaSet
Spec:
  Application:
    Image:          192.168.49.2:5000/localhost/vespaai/kubernetes
    Storage Class:  gp3
  Config Server:
    Generate Rbac:    false
    Image:            192.168.49.2:5000/localhost/vespaai/kubernetes
    Storage Class:    gp3
  Ingress:
    Endpoint Type:  NONE
  Version:          8.643.16
Status:
  Bootstrap Status:
    Pods:
      cfg-0:
        Last Updated:  2026-01-29T21:38:45Z
        Message:       Pod is running
        Phase:         RUNNING
        Converged Version: 8.643.16
      cfg-1:
        Last Updated:  2026-01-29T21:38:09Z
        Message:       Pod is running
        Phase:         RUNNING
        Converged Version: 8.643.16
      cfg-2:
        Last Updated:  2026-01-29T21:36:32Z
        Message:       Pod is running
        Phase:         RUNNING
        Converged Version: 8.643.16
  Last Transition Time:  2026-01-29T21:33:55Z
  Message:               All configservers running
  Phase:                 RUNNING
Events:                  &lt;none&gt;
</pre>

<h2>Deploy a Vespa Application</h2>

<p>
  A Vespa application can be deployed once the bootstrap process has completed. Refer to the <a href="https://github.com/vespa-engine/sample-apps">Vespa Sample Applications</a>
  to get started. In the following example, we will use the <a href="https://github.com/vespa-engine/sample-apps/tree/master/album-recommendation">Album Recommendation</a> sample.
</p>

<p>
  Set up the Vespa CLI to download the Album Recommendation sample application to a working directory.
</p>

<pre>
$ vespa clone album-recommendation myapp && cd myapp
</pre>

<p>
  Modify the sample application package with resource specifications and ensure the correct Pod count, as shown below:
</p>

<pre>
&lt;?xml version="1.0" encoding="utf-8" ?&gt;
&lt;services version="1.0" xmlns:deploy="vespa" xmlns:preprocess="properties"&gt;

    &lt;container id="default" version="1.0"&gt;
        &lt;document-api/&gt;
        &lt;search/&gt;

        &lt;nodes count="2"&gt;
            &lt;resources vcpu="2" memory="2Gb" disk="20Gb" /&gt;
        &lt;/nodes&gt;
    &lt;/container&gt;

    &lt;content id="music" version="1.0"&gt;
        &lt;min-redundancy&gt;2&lt;/min-redundancy&gt;
        &lt;documents&gt;
            &lt;document type="music" mode="index" /&gt;
        &lt;/documents&gt;
        &lt;nodes count="2"&gt;
            &lt;resources vcpu="2" memory="2Gb" disk="20Gb" /&gt;
        &lt;/nodes&gt;
    &lt;/content&gt;

&lt;/services&gt;
</pre>

<p>
  Enable port-forwarding from the ConfigServer's ingress port <code>19071</code> to your local port <code>19071</code>.
</p>

<pre>
$ vespa config set target local
$ kubectl -n $NAMESPACE port-forward pod/cfg-0 19071:19071
</pre>

<p>
  Deploy and activate the application using the sequence below.
</p>

<pre>
$ vespa prepare --target local
$ while not vespa --target local activate
</pre>

<p>
  The ConfigServer quorum will create the Container, Content, and Cluster-Controller Pods as specified in the application package. The deployment
  is considered complete once all Pods show the phase <code>RUNNING</code> in the <code>VespaSet</code> status.
</p>

<p>
  Port-forwarding provides a simple way to access the ingress ports locally.
  For other ingress options, see the <a href="ingress.html">Configuring the External Access Layer</a> section.
</p>

<h2>Feed Documents</h2>

<p>
  Feed documents to the Dataplane entrypoint by port-forwarding the Dataplane ingress port and the ConfigServer ingress port.
</p>

<pre>
# Ensure the port-forward to 19071 is still active
$ kubectl -n $NAMESPACE port-forward pod/cfg-0 19071:19071

# Port-forward to the dataplane ingress port
$ kubectl -n $NAMESPACE port-forward pod/default-100 8080:8080
</pre>

<p>
  Then, use the Vespa CLI to feed a document. In our same <a href="https://github.com/vespa-engine/sample-apps/tree/master/album-recommendation">Album Recommendation</a> sample:
</p>

<pre>
vespa feed dataset/A-Head-Full-of-Dreams.json
</pre>

<h2 id="permissions">Permissions</h2>

<p>
  The Vespa Operator requires the following permissions.
  These permissions are listed by Kubernetes <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">API verbs</a> per resource.
</p>

<table class="table">
  <thead>
  <tr>
    <th>Kubernetes Resource</th>
    <th>Required Permissions</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>CustomResourceDefinitions</td>
    <td>create, get, list, watch</td>
  </tr>
  <tr>
    <td>VespaSet</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>VespaSet Subresources</td>
    <td>
      <code>vespasets/status</code>: update, patch<br/>
      <code>vespasets/finalizers</code>: update
    </td>
  </tr>
  <tr>
    <td>ConfigMaps</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>Services</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>Pods</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>Pod Execution</td>
    <td>get, create</td>
  </tr>
  <tr>
    <td>Events</td>
    <td>create, patch</td>
  </tr>
  <tr>
    <td>PersistentVolumeClaims</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>ServiceAccounts</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>Roles</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  <tr>
    <td>RoleBindings</td>
    <td>get, list, watch, create, update, patch, delete</td>
  </tr>
  </tbody>
</table>

<h2 id="minikube-setup">MiniKube Setup</h2>

<p>
  MiniKube allows for simple local testing of Vespa on Kubernetes.
</p>

<p>
  Initialize a Minikube cluster with 8 nodes, each with 4GiB of memory and 2 CPUs. Enable Minikube's image registry
  add-on to allow the Minikube nodes to access the image. In this example, we use <code>podman</code> as the driver, though <code>docker</code> is also valid.
</p>

<pre>
# Start Minikube using an insecure registry. This is not recommended for production.
minikube start --nodes 8 --cpus 2 --memory 4GiB --driver=podman --insecure-registry="192.168.49.0/24"

# Enable the Image Registry add-on
minikube addons enable registry

# Verify MiniKube cluster was created
minikube status
</pre>

<p>
  Cache the images provided by our support team into the MiniKube registry.
</p>

<pre>
# Authenticate to our registry
echo $VESPAAI_REGISTRY_TOKEN | podman login images.ves.pa \
  -u "$VESPAAI_REGISTRY_USER" \
  --password-stdin

# Cache the images locally
podman pull images.ves.pa/kubernetes/vespa:$VESPA_VERSION
podman pull images.ves.pa/kubernetes/operator:$VESPA_VERSION
</pre>

<p>
  Then, push the images to the MiniKube registry. It will be accessible from
  <code>$(minikube ip):5000</code> on a standard setup.
</p>

<pre>
# Save the minikube registry endpoint
export MINIKUBE_REGISTRY=$(minikube ip)

# Push the kubernetes/vespa image to the registry
podman tag  kubernetes/vespa:$VESPA_VERSION $MINIKUBE_REGISTRY:5000/localhost/kubernetes/vespa:$VESPA_VERSION
podman push --tls-verify=false $MINIKUBE_REGISTRY:5000/localhost/kubernetes/vespa:$VESPA_VERSION

# Push the kubernetes/operator image to the registry
podman tag  kubernetes/operator:$VESPA_VERSION $MINIKUBE_REGISTRY:5000/localhost/kubernetes/operator:$VESPA_VERSION
podman push --tls-verify=false $MINIKUBE_REGISTRY:5000/localhost/kubernetes/operator:$VESPA_VERSION
</pre>

<p>
  The images will now be available to the Minikube nodes at <code>$MINIKUBE_REGISTRY:5000/localhost/kubernetes/operator:$VESPA_VERSION</code>. We encourage
  saving the new image locations below.
</p>

<pre>
export OCI_IMAGE_REFERENCE=$MINIKUBE_REGISTRY:5000/localhost/kubernetes/vespa
export OCI_IMAGE_REFERENCE_OPERATOR=$MINIKUBE_REGISTRY:5000/localhost/kubernetes/operator
</pre>

<p>
  Then, install the <a href="https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner">Local Persistent Volume</a> Helm Chart.
  This will allow Persistent Volumes which are required by Vespa to be created in a MiniKube environment. Helm will automatically create a StorageClass called <code>local-storage</code>, which should be
  used as the <code>StorageClass</code> for subsequent steps.
</p>

<pre>
# Clone the local persistent volume static provisioner from the Kubernetes sigs
$ git clone git@github.com:kubernetes-sigs/sig-storage-local-static-provisioner.git

# Install the Helm Chart onto the cluster globally
$ cd sig-storage-local-static-provisioner
$ helm install -f helm/examples/baremetal-default-storage.yaml local-volume-provisioner --namespace kube-system ./helm/provisioner
</pre>

<p>
  Create several usable volumes on each MiniKube Node. We recommend at least 4 per node for a smooth deployment.
</p>

<pre>
# Create several volumes on each Minikube node.
$ for n in minikube minikube-m02 minikube-m03 minikube-m04 minikube-m05 minikube-m06 minikube-m07 minikube-m08; do
  echo "==> $n"
  minikube ssh -n "$n" -- '
    set -e
    for i in 1 2 3 4; do
      sudo mkdir -p /mnt/disks/vol$i
      if ! mountpoint -q /mnt/disks/vol$i; then
        sudo mount --bind /mnt/disks/vol$i /mnt/disks/vol$i
      fi
    done
    echo "Mounted:"
    mount | grep -E "/mnt/disks/vol[1-4]" || true
  '
done
</pre>
<h2 id="delete">Delete a VespaSet</h2>

<p>
  Use the following helper script to delete all resources in a VespaSet. This will tear down all resources and should be used with caution.
</p>

<pre>
  #!/bin/bash
# cleanup-k8s-resources.sh
# Usage:
#   ./cleanup-k8s-resources.sh <namespace> [--delete-operator] [--delete-namespace]
#   ./cleanup-k8s-resources.sh -n <namespace> [--delete-operator] [--delete-namespace]
#   ./cleanup-k8s-resources.sh --namespace <namespace> [--delete-operator] [--delete-namespace]
#
# Prompts for confirmation before proceeding. Use --dry-run to simulate.

set -euo pipefail

usage() {
  echo "Usage: $0 <namespace> [--dry-run] [--delete-operator] [--delete-namespace]"
  echo "       $0 -n <namespace> [--dry-run] [--delete-operator] [--delete-namespace]"
  echo "       $0 --namespace <namespace> [--dry-run] [--delete-operator] [--delete-namespace]"
  echo
  echo "Options:"
  echo "  -n, --namespace     Kubernetes namespace to target"
  echo "      --dry-run       Show what would be deleted without making changes"
  echo "      --delete-operator  Also delete vespa-operator deployment and pods"
  echo "      --delete-namespace Delete the entire namespace (takes precedence)"
  echo "  -h, --help          Show this help"
}

NAMESPACE=""
DELETE_OPERATOR=0
DELETE_NAMESPACE=0

# Parse arguments
while [[ $# -gt 0 ]]; do
  case "$1" in
    -n|--namespace)
      [[ $# -ge 2 ]] || { echo "Error: $1 requires a value"; usage; exit 1; }
      NAMESPACE="$2"
      shift 2
      ;;
    --delete-operator)
      DELETE_OPERATOR=1
      shift
      ;;
    --delete-namespace)
      DELETE_NAMESPACE=1
      shift
      ;;
    -h|--help)
      usage
      exit 0
      ;;
    -*)
      echo "Unknown option: $1"
      usage
      exit 1
      ;;
    *)
      if [[ -z "${NAMESPACE}" ]]; then
        NAMESPACE="$1"
        shift
      else
        echo "Unexpected argument: $1"
        usage
        exit 1
      fi
      ;;
  esac
done

if [[ -z "${NAMESPACE}" ]]; then
  echo "Error: namespace is required."
  usage
  exit 1
fi

echo "Target namespace: ${NAMESPACE}"
echo "This will operate on:"
if [[ $DELETE_NAMESPACE -eq 1 ]]; then
  echo "- Entire namespace (all resources will be deleted)"
else
  echo "- VespaSet resources"
  if [[ $DELETE_OPERATOR -eq 1 ]]; then
    echo "- Deployment vespa-operator"
    echo "- All Pods (including vespa-operator)"
  else
    echo "- Pods (excluding those with label app=vespa-operator)"
  fi
  echo "- Services (including cfg)"
  echo "- PersistentVolumeClaims"
fi

# Confirmation
if [[ $DELETE_NAMESPACE -eq 1 ]]; then
  read -r -p "The namespace '${NAMESPACE}' and all its resources will be deleted. Are you sure? [y/N]: " CONFIRM
elif [[ $DELETE_OPERATOR -eq 1 ]]; then
  read -r -p "Your resources (including vespa-operator deployment and pods) will be deleted. Are you sure? [y/N]: " CONFIRM
else
  read -r -p "Your resources will be deleted (operator pods are excluded). Are you sure? [y/N]: " CONFIRM
fi

case "$CONFIRM" in
  [yY]|[yY][eE][sS]) ;;
  *) echo "Aborted."; exit 0 ;;
esac

# Do not delete the operator, to ensure that the VespaSet finalizer completes
echo "Deleting Pods (excluding app=vespa-operator) in namespace: $NAMESPACE"
kubectl delete pods --grace-period=0  -l 'app!=vespa-operator' -n "$NAMESPACE" --ignore-not-found

echo "Deleting VespaSet resources in namespace: $NAMESPACE"
kubectl delete vespaset --all -n "$NAMESPACE" --ignore-not-found

echo "Deleting Services in namespace: $NAMESPACE"
kubectl delete svc -l 'app!=vespa-operator' -n "$NAMESPACE" --ignore-not-found
kubectl delete svc cfg -n "$NAMESPACE" --ignore-not-found

echo "Deleting PVCs in namespace: $NAMESPACE"
kubectl delete pvc  -l 'app!=vespa-operator' -n "$NAMESPACE" --ignore-not-found

# Delete Deployment if requested
if [[ $DELETE_OPERATOR -eq 1 ]]; then
  echo "Deleting Deployment 'vespa-operator' in namespace: $NAMESPACE"
  kubectl delete deployment vespa-operator -n "$NAMESPACE" --ignore-not-found
fi

if [[ $DELETE_NAMESPACE -eq 1 ]]; then
  echo "Deleting namespace: $NAMESPACE"
  kubectl delete namespace "$NAMESPACE" --ignore-not-found
  echo "✅ Cleanup complete: namespace deleted"
  exit 0
fi

echo "✅ Cleanup complete in namespace: $NAMESPACE"
</pre>
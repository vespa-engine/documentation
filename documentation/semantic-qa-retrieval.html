---
# Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root. 
title: "Semantic Retrieval for Question Answering Applications"
---

<p>
This document describes how to represent <a href="https://en.wikipedia.org/wiki/Word_embedding">text embedding</a> tensors in Vespa and 
how to build a scalable real time semantic search engine using Vespa's <a href="approximate-nn-hnsw.html">Approximate Nearest Neighbor Search Operator</a>  to search in embedding
space generated by 
 Google's <a href="https://arxiv.org/abs/1907.04307">Multilingual Universal Sentence Encoder</a>.</p>

<p>
The introduction of Transformer NLP models like <a href="https://arxiv.org/abs/1810.04805">BERT</a> 
have led to significant advancement in the state of art for many
different tasks. Examples include question answering, classification and ad-hoc document ranking. Transformer models
give best accuracy for ranking and question answering tasks
when used as an interaction model with cross-attention between the question and document. However, running online
inference with query and document cross-attention models
over large document collections is computationally prohibitively expensive.
The problematic computational complexity of inference
using Transformer models over large collections has led
to increased interest in multi-stage retrieval and ranking
architectures. The first stage retrieves candidate documents
using a more cost efficient scoring function and the advanced
cross-attention model inference is limited to the top ranking
documents from the first stage.
</p>

<p>
In <a href="https://arxiv.org/abs/1907.04780">ReQA: An Evaluation for End-to-End Answer Retrieval Models</a> Ahmad Et al. introduce 
<em>Retrieval Question Answering (ReQA)</em>, a benchmark 
for evaluating large-scale sentence level answer retrieval models and where they establish a baseline for 
both traditional information retrieval (sparse term based) and neural (dense) encoding models on the 
<em>Stanford Question Answering Dataset (SQuAD)</em> v1.1 dataset.
 
In this document we reproduce the work done by 
Ahmad et al. on the <a href="https://rajpurkar.github.io/SQuAD-explorer/">SQuAD 1.1</a> retrieval task using Vespa serving engine. 

We replicate the results from the mentioned paper which enables
organization to deploy state of the art question answering retrieval systems with low effort using the scalable Vespa engine.</p>

<p>
Vespa has support for storing and indexing dense tensors field types along with traditional 
string fields with support for traditional sparse term based text ranking features
like <a href="reference/bm25.html">bm25</a> or Vespa's <a href="reference/nativerank.html">nativeRank</a>.  
Having both traditional text ranking features and semantic similarity features
expressed in the same engine is a powerful feature of Vespa, which enables hybrid retrieval using both sparse and dense representation.</p>

<p> 
The work described in this document can be reproduced using the 
<a href="https://github.com/vespa-engine/sample-apps/tree/master/semantic-qa-retrieval">semantic-qa-retrieval sample application</a>. 
</p>

<h2 id="universal-sentence-encoder">About Google's Universal Sentence Encoder</h2> 
<p>
The <em>Universal Sentence Encoder</em> encodes text into fixed length dense embedding space that can be used for broad range of tasks such as semantic similarity, 
 semantic retrieval and other natural language processing (NLP) tasks.
Google has released several different sentence encoder models with different goals and following the work of Ahmad et al. we use the 
<a href="https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html">Multilingual Universal Sentence Encoder for Question-Answer Retrieval</a>.

The Universal Sentence Encoder for Question-Answer Retrieval enables us to process questions and candidate answer sentences independently and map the 
high dimensional sparse text representation to a relatively low dimensional
dense tensor representation, where we can use Vespa's approximate nearest neighbor search operator to retrieve documents efficiently.  

<ul>
<li>Question text is encoded using the question encoder, which takes the question text as input and outputs a 512 dimensional dense tensor.</li> 
<li>Each sentence of text is encoded using the 
response encoder which takes the sentence and the surrounding context (e.g paragraph level) as input and outputs a 512 dimension dense tensor</li> 
</ul>
We can store and index the dense tensor embedding in Vespa using <a href="tensor-user-guide.html">tensor fields</a> 
and use Vespa's approximate nearest neighbor search operator to retrieve documents. 
</p>

<figure>
<img src="https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-similarity.png" alt="Image Courtesy https://tfhub.dev/google/universal-sentence-encoder/2" width="1272" height="280"/>
<figcaption>Image Courtesy <a href="https://tfhub.dev/google/universal-sentence-encoder/2">https://tfhub.dev/google/universal-sentence-encoder/2</a></figcaption>
</figure>

<p>Papers and resources on Google's Universal Sentence Encoder:</p>
<ul>
  <li><a href="https://arxiv.org/abs/1907.04307">Multilingual Universal Sentence Encoder for Semantic Retrieval Paper</a> </li>
  <li><a href="https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html">Google AI Blog: Multilingual Universal Sentence Encoder for Semantic Retrieval</a> </li>
  <li><a href="https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/1">Tensorflow Hub: USE-QA </a> </li>
</ul>

<p>A similar dual encoder architecture (question, document) is described in the <a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a>
by Facebook Research, where they demonstrate how a trained dense representation using a dual question encoder based on BERT outperforms traditional IR retrieval. Quote: 
<em>When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of 
top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.</em>   
</p>

<h2 id="dataset">About the SQuAD dataset</h2> 
<p> The <a href="https://arxiv.org/abs/1606.05250">SQuAD: 100,000+ Questions for Machine Comprehension of Text</a> paper introduced the SQuAD dataset 
which is available for download <a href="https://rajpurkar.github.io/SQuAD-explorer/">here</a>.

The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by 
crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. In our experiments we use the train v1.1 dataset.</p>
<p>
Sample questions and answers for a given paragraph context taken from a snapshot of 
the <a href="https://en.wikipedia.org/wiki/University_of_Notre_Dame">University_of_Notre_Dame Wikipedia page</a> is shown below:
<pre>
{
 "data": [
  {
    "title": "University_of_Notre_Dame"
    "paragraphs": [
      {
        "context": "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. 
         Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". 
         Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. 
         It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. 
         At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.",
      "qas": [
        {
          "question": "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?",
          "answers": [
            {
             "answer_start": 515,
             "text": "Saint Bernadette Soubirous"
            }
           ],
           "id": "5733be284776f41900661182"
        },
        {
          "question": "What is in front of the Notre Dame Main Building?",
          "answers": [
            {
              "answer_start": 188,
              "text": "a copper statue of Christ"
            }
          ],
          "id": "5733be284776f4190066117f"
        }
      ]
      }
    ]
   }
 ]
}
</pre>
<p>
The <em>answer_</em> start represent the offset where the answer for the question can be found.
The SQuAD v1.1 train dataset consists of 87,599 questions and 18,896 paragraphs. 
The paragraphs can further be segmented into 91,729 sentences using a sentence tokenizer.   
</p>

<h2 id="vespa-data-model">SQuAD Data modelling with Vespa</h2>
<p>We model the SQuaD data set in Vespa in two different document schema types; 
a <em>context</em> document type and a <em>sentence</em> document type</a>:</p>

<b>Context document type</b>:
<pre>
schema context {
  document context {
    field context_id type int {
      indexing: summary | attribute 
    }
    field text type string {
      indexing: summary | index
      index: enable-bm25
    }
  }
}
</pre>
<b>Sentence document type</b>:
<pre>
schema sentence {
  document sentence inherits context {
    field sentence_embedding type tensor&lt;float&gt;(x[512]) {
      indexing: attribute
       attribute {
        distance-metric: euclidean 
      }
      index {
        hnsw {
          max-links-per-node: 16 
          neighbors-to-explore-at-insert: 500
        }
      }
    }
  }
}
</pre>
<p>
See <a href="approximate-nn-hnsw.html">Approximate Nearest Neighbor Search using HNSW Index</a> for details on the <em>HNSW</em> index settings and <em>distance-metric</em>. 
In this case, we use the <em>euclidean</em> distance metric. 
</p>

<h2 id="vespa-feed-generating">Converting the SQuAD json to Vespa json feed format</h2>
<p>In order to feed the SQuAD data we need to convert it into our Vespa document schema and 
feed documents using the <a href="reference/document-json-format.html">Vespa json format</a>.</p>
<p>For each paragraph context we run a simple 
<a href="https://github.com/google/retrieval-qa-eval/blob/master/sb_sed.py">sentence tokenizer</a> published by Ahmed et al 
to extract sentences from the paragraph context. We 
simply assign a unique sentence id sentences and likewise for context. Below is a sample of one sentence extracted from the above example paragraph:</p>

<pre>
{
  "put": "id:squad:sentence::5"
    "fields": {
      "context_id": 0,
        "text": "Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\"."
        "sentence_embedding": {
          "values": [
            -0.0528511106967926,
            0.00927420798689127,
            ......
            0.011870068497955799,
            -0.06848619878292084
          ]
        }
    }
}
</pre>
<p>
We can feed the generated document set to our Vespa instance using any of the feed api's, but 
we use the <a href="vespa-http-client.html">Vespa http client</a>. After this step we have one content db
with 18,896 context documents and 91,729 sentences in another in the same Vespa content cluster. 

<h2 id="sentence-and-paragraph-retrieval">Sentence and Paragraph Retrieval</h2>
<p>
The goal of the ReQA task is to retrieve sentences which have the answer for any given question.
We can also compute context or paragraph level retrieval using the sentence level semantic similarity
by aggregating over the sentence level scores,
we can do this efficiently using the Vespa <a href="grouping.html">grouping</a> api
if we want to retrieve paragraphs instead of sentences.
</p>

<p> 
We use the Vespa <a href="query-api.html">Search API</a> to express our search request logic. 
We use the <a href="query-language.html">YQL</a> query language to express our query retrieval logic.

Using the sample question from the example the POST http search request becomes:
<pre>
{
  'yql': 'select * from sources sentence where ([{"targetNumHits":100}]nearestNeighbor(sentence_embedding,query_embedding));',
  'hits': 100,
  'ranking.features.query(query_embedding)': [-0.0466, ...,0.064],
  'ranking.profile': 'sentence-semantic-similarity' 
}
</pre>

<ul>
<li>We use the nearestNeighbor search operator to retrieve the closest 100 sentences in embedding space using euclidean distance as configured with the tensor HNSW settings</li>
<li>The top 100 top nearest sentences are ranked by the ranking profile passed in the <em>ranking.profile</em>.</li>
<li>The dense tensor representation encoded by the sentence encoder is passed by the <em>ranking.features.query(query_embedding)</em> parameter</li>
</ul>

The ranking profile is defined in the <em>sentence</em> <a href="https://github.com/vespa-engine/sample-apps/blob/master/semantic-qa-retrieval/src/main/application/schemas/sentence.sd#L26">document schema</a>:
<pre>
rank-profile sentence-semantic-similarity inherits default {
  first-phase {
    expression: closeness(sentence_embedding) 
  }
}
</pre>

<p>Where <em>closeness</em> is a Vespa <a href="reference/rank-features.html">ranking feature</a> which is defined as 1/(1 + distance). 


For paragraph level retrieval we use Vespa's <a href="grouping.html">grouping</a> feature to retrieve paragraphs instead of sentences. 
As in the paper we use the max sentence score in the paragraph to represent the paragraph level 
score. The query above is changed to add a grouping specification.
<pre>
{
  'yql': 'select * from sources sentence where ([{"targetNumHits":100}]nearestNeighbor(sentence_embedding,query_embedding))| \\
    all(group(context_id) max(100) order(-max(relevance())) each( max(2) each(output(summary())) as(sentences)) as(paragraphs));',
  'hits': 0,
  'ranking.features.query(query_embedding)': [-0.0466, ...,0.064],
  'ranking.profile': 'sentence-semantic-similarity' 
}
</pre>
<p>The grouping expression groups sentences by the context id and order the groups ( paragraphs ) by the maximum rank score and for each unique context id we get the top ranking sentences sentences ordered
by their rank score assigned by the chosen ranking profile.  </p>

<h2 id="hybrid-retrieval">Hybrid retrieval using both dense (encoding) and sparse (term) representation</h3>
<p>We can also retrieve using a hybrid combination consisting of dense retrieval and regular query term matching:</p>
<pre>
{
  'yql': 'select * from sources sentence  where ([{"targetNumHits":100}]nearestNeighbor(sentence_embedding,query_embedding)) or userQuery();',
  'query': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',
  'type': 'any',
  'hits': 100,
  'ranking.features.query(query_embedding)': [-0.0466, ...,0.064],
  'ranking.profile': 'bm25-sentence-semantic-similarity' 
}
</pre>
<p>
We use logical disjunction to combine the nearest neighbor query operator retrieving in dense embedding space with the regular term based (sparse) retrieval.
We use a simple linear combination of the bm25 score on text and the previously described closeness ranking feature:</p>
<pre>
rank-profile bm25-sentence-semantic-similarity inherits default {
  first-phase {
    expression: bm25(text) + closeness(sentence_embedding) 
  }
}
</pre>

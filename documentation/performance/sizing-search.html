---
# Copyright Verizon Media. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.
title: "Vespa Serving Scaling Guide"
---

<p>
<em>Will it scale?</em> is a question we hear a lot? The answer is usually yes as Vespa can scale in any dimension:
<ul>
  <li>Scale document volume and write (update/put) volume</li>
  <li>Scale query throughput</li>
  <li>Scale serving latency to meet service level agreement (SLA)</li>
</ul>
The question you try to answer during a sizing exercise is
"what the total cost would be to serve a use case using Vespa".
This document helps sizing an application correctly with as low cost as possible.
Vespa is used to implement many use cases, and this document is relevant for all of them:
<ul>
  <li>Serving a <a href="../text-matching-ranking.html">text ranking</a> use case
    or a <a href="..//tutorials/blog-recommendation.html">recommendation</a> use case</li>
  <li>Serving a Machine Learned Model, e.g a <a href="../tensorflow.html">Tensorflow</a>,
    <a href="../onnx.html">ONNX</a>, <a href="../xgboost.html">XGBoost</a>, or
    <a href="../lightgbm.html">LightGBM</a> model</li>
</ul>
With Vespa it is possible to do benchmarking on a few nodes
to infer the overall performance and cost of the chosen deployment architecture,
and as Vespa supports <a href="../elastic-vespa.html#resizing">live resizing</a>,
it is easy to scale from a prototype to a full production size deployment.
</p><p>
This document covers sizing and capacity planning for serving,
see <a href="sizing-feeding.html">feed performance sizing</a> for feed performance sizing
and <a href="feature-tuning.html">Vespa serving feature tuning</a>.
It also covers the following topics:
<ul>
  <li><a href="#data-distribution">Data distribution</a> in Vespa and how it impacts serving</li>
  <li><a href="#content-cluster-scalability-model">Scaling Serving Latency and Throughput</a> in Vespa</li>
  <li><a href="#scaling-document-volume">Scaling Data Volume</a> in Vespa</li>
</ul>
</p>



<h2 id="data-distribution">Data distribution in Vespa - flat versus grouped</h2>
<p>
The basic element in the Vespa search architecture is a content node, which is part of a content cluster.
A Vespa deployment can have several content clusters, which can be scaled independently.
</p><p>
A content node holds a fraction of the entire data corpus.
Data is distributed to nodes using a <a href="../content/idealstate.html">distribution algorithm</a>,
which goal is to uniformly distribute data over the set of nodes.
The goal is also to avoid distribution skew,
while at the same time supporting re-distribution of data,
with minimal data movement,
if the size of the content cluster changes.
Read <a href="../elastic-vespa.html">elastic Vespa</a> to learn
how data is distributed across nodes, and how adding or removing nodes works.
See also <a href="../content/consistency.html">Vespa's consistency model</a> documentation.
</p>
<figure>
<figcaption><strong>Flat content distribution</strong></figcaption>
<img src="../img/flat_content_cluster.svg" height="420" width="840"/>
</figure>
<p>
With a flat distribution, the content is distributed to content nodes using the
<a href="../content/idealstate.html">ideal state distribution algorithm</a>.
A query is dispatched in parallel from a container instance to
<strong>all</strong> content nodes in the content cluster.
Each content node searches the <em>active</em> part of the <em>ready</em> sub-database.
The above figure illustrates a deployment using 4 nodes with <em>redundancy</em> 2 and <em>searchable-copies</em> 2 -
see the <a href="#high-availability">availability</a> section.
</p><p>
When using flat data distribution, the only way to scale query throughput is to reduce the search latency.
Given a fixed occupancy (users, load clients),
this relationship between query throughput and latency is described by
<a href="https://en.wikipedia.org/wiki/Little%27s_law">Little's law</a> -
more on this in <a href="#content-cluster-scalability-model">content cluster scalability model</a> section.
</p>
<figure>
<figcaption><strong>Grouped content distribution</strong></figcaption>
<img src="../img/grouped_content_cluster.svg" height="420" width="840"/>
</figure>
<p>
With a grouped distribution, content is distributed to a configured set of <em>groups</em>,
such that the entire document collection is contained in each group.
A <em>group</em> contains a set of content nodes where the content is
distributed using the <a href="../content/idealstate.html">distribution algorithm</a>.
In the above illustration, there are 4 nodes in total, 2 groups with 2 nodes in each group.
<em>redundancy</em> is 2 and <em>searchable-copies</em> is also 2.
As can seen from the figure with this grouped configuration,
the content nodes only have a populated ready sub-database.
A query is dispatched in parallel to all nodes in <strong>one group</strong> at a time
using a <a href="../reference/services-content.html#dispatch-policy">dispatch-policy</a>.
The default policy is <em>adaptive</em>, which loadbalances over the set of groups, aiming at even latency.
</p>


<h3 id="high-availability">High Data Availability</h3>
<p>
Ideally, the data is available and searchable at all times, even during node failures.
High availability costs resources due to data replication.
How many replicas of the data to configure,
depends on what kind of availability guarantees the deployment should provide.
Configure availability vs cost:
</p>
<table class="table">
  <thead></thead><tbody>
    <tr id="redundancy">
      <th><a href="../reference/services-content.html#redundancy">redundancy</a></th>
      <td>
        Defines the total number of copies of each piece of data the cluster will store and maintain to avoid data loss.
        Example: with a redundancy of 2, the system tolerates 1 node failure
        before any further node failures may cause data to become unavailable.
      </td>
    </tr><tr id="searchable-copies">
      <th style="white-space: nowrap">
          <a href="../reference/services-content.html#searchable-copies">searchable-copies</a></th>
      <td>
        Configures how many of the copies (as configured with <em>redundancy</em>)
        to be indexed (<em>ready</em>) at any time.
        Configuring <em>searchable-copies</em> to be less than <em>redundancy</em> saves resources (memory, disk, cpu),
        as not all copies are indexed (<em>ready</em>).
        In case of node failure, the remaining nodes needs to a index the
        <em>not ready</em> documents which belonged to the failed node.
        In this transition period, the search has reduced search coverage.
      </td>
    </tr>
  </tbody>
</table>
<figure>
<figcaption><strong>Content node database</strong></figcaption>
<img src="../img/proton_database.svg" height="420" width="840"/>
</figure>
<p>
The above figure illustrates the three
<a href="/documentation/proton.html#sub-databases">sub-databases</a> inside a Vespa content node.
<ul>
  <li>The documents in the <strong>Ready</strong> DB are indexed,
    but only the documents in <b>Active</b> state are searchable.
    In a flat distributed system there is only one active instance of the same document,
    while with grouped distribution there is one active instance per group.</li>
  <li>The documents in the <b>Not Ready</b> DB are stored but not indexed.</li>
  <li>The documents in the <b>Removed</b> are stored but blocklisted, hidden from search.
    The documents are permanently deleted from storage by
    <a href="../proton.html#proton-maintenance-jobs">Proton maintenance jobs</a>.</li>
</ul>
If the availability guarantees tolerate temporary search coverage loss during node
failures (e.g. <em>searchable-copies</em>=1), this is by far the most optimal for for serving performance -
the query work per node is less as index structures like posting lists are smaller.
The index structures only contains documents in <em>Active</em> state,
not including <em>Not Active</em> documents.
</p><p>
With <em>searchable-copies</em>=2 and <em>redundancy</em>=2,
each replica is fully indexed on separate content nodes.
Only the documents in <em>Active</em> state is searchable,
the posting lists for a given term is (up to) doubled as compared to <em>searchable-copies</em>=1.
</p><p>
See <a href="sizing-examples.html">Content cluster Sizing example deployments</a>
for examples using grouped and flat data distribution.
</p>



<h2 id="life-of-a-query-in-vespa">Life of a query in Vespa</h2>
<p>
Vespa executes a query in two protocol phases
(or more if using <a href="../grouping.html">result grouping features</a>)
to optimize the network footprint of the parallel query execution.
The first protocol phase executes the query in parallel over content nodes in a group to find the global top hits,
the second protocol phase fetches the data of the global top hits.
</p><p>
During the first phase,
content nodes match and <a href="../ranking.html">rank</a> documents using the selected rank-profile/model.
The hits are returned to the stateless container for merging
and potentially blending when multiple content clusters are involved.
</p><p>
When the global top ranking documents are found,
the second protocol phase fetch the summary data for the global best hits
(e.g. summary snippets, the original field contents, and ranking features).
By doing the query in two protocol
phases one avoids transferring summary data for hits which will not make it into the global best hits.
</p><p>
Components Involved in Query Execution:
<ul>
<li><b>Container</b>
  <ul>
  <li>Parses the <a href="../query-api.html">API</a> request
      and the <a href="../query-language.html">query</a> and run time context features.</li>
  <li>Modify the query according to the schema specification (stemming, etc) for a text search application
      or creating run time query or user context tensors for a ML serving application.</li>
  <li>Invokes chains of custom <a href="../jdisc/container-components.html">container components/plugins</a>
      which can work on the request and query input and also the results.</li>
  <li>Dispatching of query to content nodes in the content cluster(s) for parallel execution.
      With flat distribution, queries are dispatched to all content nodes
      while with a grouped distribution the query is dispatched to all content nodes within a group
      and the queries are load-balanced between the groups using a
      <a href="../reference/services-content.html#dispatch-policy">dispatch-policy</a>.</li>
  <li>Blending of global top ranking results from cluster(s).</li>
  <li>Fetching the top ranking results with document summaries from cluster(s).</li>
  <li>Result processing and possible top-k re-ranking and finally rendering of results back to client.</li>
  </ul>
</li>
<li><strong>Search (Proton)</strong>
  <ul>
  <li>Finding all documents matching the <a href="../query-api.html">query specification</a>.
      For a ML serving use case, the selection might be a subset of the content pool
      (e.g limit the model to only be evaluated for content-type video documents),
      while for a text ranking application it might be a
      <a href="../using-wand-with-vespa.html">WAND</a> text matching query.</li>
  <li>Calculating the score (which might be a text ranking relevancy score or
      the inferred score of a Machine Learned model) of each hit,
      using the chosen rank-profile. See <a href="../ranking.html">ranking with Vespa</a>.</li>
  <li>Aggregating information over all the generated hits using <a href="../grouping.html">result grouping</a>.</li>
  <li>Sorting hits on relevancy score (text ranking) or inference score (e.g ML model serving),
      or on attribute(s).
      See <em>max-hits-per-partition</em> and <em>top-k-probability</em> in
      <a href="../reference/services-content.html#dispatch-tuning">dispatch tuning</a>
      for how to tune how many hits to return.</li>
  <li>Processing and returning the document summaries of the selected top hits
      (during summary fetch phase after merging and blending has happened on levels above).</li>
  </ul>
</li>
</ul>
The detailed execution inside Proton during the matching and ranking first protocol phase is:
<ol>
  <li>Build up the query tree from the serialized network representation.</li>
  <li>Lookup the query terms in the index and B-tree dictionaries
      and estimate the number of hits each term and parts of the query tree will produce.
      Terms which searches attribute fields without <a href="../attributes.html#fast-search">fast-search</a>
      will be given a hit count estimate to the total number of documents.</li>
  <li>Optimize and re-arrange the query tree for most efficient performance trying to move terms or
      operators with the lowest hit ratio estimate first in the query tree.</li>
  <li>Prepare for query execution, by fetching posting lists from the index and B-tree structures.</li>
  <li>Multi-threaded execution per search starts using the above information.
      Each thread will do its own thread local setup.</li>
  <li>Each search thread will evaluate the query over its document space.</li>
  <li>The search threads complete first phase and agree which hits will continue to second phase ranking
      (if enabled per the used rank-profile).
      The threads operate over a shared heap with the global top ranking hits.</li>
  <li>Each thread will the complete second phase and grouping/aggregation/sorting.</li>
  <li>Merge all threads results and return back up to the container.</li>
</ol>
<a href="../jdisc/">Container</a> clusters are stateless and hence easy to scale horizontally,
and don't require any data distribution during re-sizing.
The set of stateful content clusters can be scaled independently
and <a href="../elastic-vespa.html#resizing">re-sized</a> which requires re-distribution of data.
Re-distribution of data in Vespa, is supported and designed to be done without significant serving impact.
Altering the number of nodes or groups in a Vespa content cluster does not require re-feeding of the corpus,
so it's easy to start out with a sample prototype and scale it to production scale workloads.
</p>



<h2 id="content-cluster-scalability-model">Content cluster scalability model</h2>
<p>
Vespa is a parallel computing platform
where the work of matching and ranking is parallelized across a set of nodes and processors.
The speedup we can get by altering the number of nodes in a Vespa content group follows
<a href="https://en.wikipedia.org/wiki/Amdahl%27s_law">Amdahl's law</a>,
which is a formula used to find the maximum improvement possible by improving a particular part of a system.
In parallel computing, <em>Amdahl's law</em> is mainly used to predict the theoretical
maximum speedup for program processing using multiple processors.
In Vespa, as in any parallel computing system,
there is work which can be parallelized and work which cannot.
The relationship between these two work types determine how to best scale the system,
using a flat or grouped distribution.
We introduce the following concepts:
<table class="table">
  <tr id="static-query-work">
    <td><strong>static query work</strong></td>
    <td>
      Portion of the query work on a content node that does not depend on the
      number of documents indexed on the node.
      This is an administrative overhead caused by system design and abstractions,
      e.g. number of memory allocations per query term.
      Typically a large query tree means higher static work,
      and this work cannot be parallelized over multiple processors, threads or nodes.
      The static query work portion is described in step 1 to 4 and step 9 in the detailed life of a query explanation above.
    </td>
  </tr><tr id="dynamic-query-work">
    <td><strong>dynamic query work</strong></td>
    <td>
      Portion of the query work on a content node that depends on the number
      of documents indexed and active on the node.
      This portion of the work scales mostly linearly with the number of matched documents.
      The dynamic query work can be parallelized over multiple processors and nodes.
      Referenced later as <em>DQW</em>.
      The <em>DQW</em> also depends on the phase two protocol summary fill
      where the actual contents of the global best documents is fetched from the content nodes
      which produced the hit in the first protocol phase.
    </td>
  </tr><tr id="total-query-work">
    <td><strong>Total query work</strong></td>
    <td>
      The total query work is given as the dynamic query work (<em>DQW</em>) + static query work (<em>SQW</em>).
    </td>
  </tr>
</table>
Adding content nodes to content cluster (keeping the total document volume fixed) configured with flat distribution,
reduces the dynamic query work per node (<em>DQW</em>)
but does not reduce the static query work (<em>SQW</em>).
The overall system cost also increases as you need to rent another node.</p>
Since <em>DQW</em> depends and scales almost linearly with the number of documents  on the content nodes,
we can try to distribute the work over more nodes.
<em>Amdahl's law</em> specifies that the maximum speedup we achieve by parallelizing the
dynamic work (<em>DQW</em>) is given by the formula:
</p><p><!-- depends on mathjax -->
  $$\text{max_speedup}_{\text{group}} = \frac{1}{1 - \frac{DQW}{SQW+DQW}}$$
</p><p>
For example, if we through <a href="#metrics-for-capacity-plan">metrics</a> see that the <em>DQW</em> = 0.50,
the maximum speedup we can get by increasing parallelism by using more nodes and decreasing <em>DQW</em> is 2.
With fixed occupancy (number of users, clients or load),
<a href="https://en.wikipedia.org/wiki/Little%27s_law">Little's Law'</a>
tells us that we cans achieve 2 times throughput if we are able to speed up the latency by 2 times:
</p><p><!-- depends on mathjax -->
  $$\frac{1}{1 - \frac{0.5}{0.5+0.5}} = 2$$
</p><p>
When <em>SQW</em> is no longer significantly less than <em>DQW</em>,
adding more nodes in a flat distributed cluster just increases the overall system cost.
This without any serving performance gain,
except increasing overall supported feed throughput,
which increases almost linearly with number of nodes.
</p><p>
Two different <em>DQW/(DQW+SQW)</em> factors are are illustrated in the figures below.
The overall query work <em>TQW</em> is the same for both cases (10 ms),
but the <em>DQW</em> portion of the <em>TQW</em> is different.
The throughput (QPS) is a function of the latency (<a href="https://en.wikipedia.org/wiki/Little%27s_law">Little's Law</a>)
and the number of cpu cores * nodes.
Using 1 node with 24 v-cpu cores and 10 ms service time (<em>TQW</em>),
we expect to reach close to 2400 QPS at 100% utilization
(unless there are other bottlenecks like network or stateless container processing).
</p>
<figure>
  <img src="../img/ScalingLatencyFactor0.5.svg" />
  <img src="../img/ScalingLatencyFactor0.005.svg" />
</figure>
<p>
In the first figure the overall latency is 10 ms,
but the dynamic query work (latency) is only 50%
and given <em>Amdahl's law</em> it follows that the maximum speedup we can get is 2.
This is true regardless of how many processors or nodes we distribute the dynamic query work over.
No matter how many nodes we add, we don't get above 4800 queries/s.
The only thing we achieve by adding more nodes
is increasing the cost without any performance benefits.
</p><p>
In the second figure we illustrate a system where the dynamic work portion is much higher (0.9),
and the theoretical maximum speedup becomes bound by 10x as given by <em>Amdahl's law</em>.
Note that both figures are with a single flat distributed content cluster with a fixed document volume.
</p><p>
Given the theoretical explanation above we can provide two rules of thumb for scaling throughput and latency:</p>
<table class="table">
  <thead></thead><tbody>
    <tr>
      <th>Add nodes in a flat distribution</th>
      <td>When DQW/TQW is large (close to 1.0),
        throughput QPS can be scaled by just adding more content nodes in a system using flat distribution.
        This will reduce the number of documents per node,
        and thus reduce the <em>DQW</em> per node.</td>
    </tr><tr>
      <th style="white-space:nowrap;">Add groups using grouped distribution</th>
      <td>When DQW/TQW is low, one can no longer just add more content nodes to scale throughput
        and must instead use a grouped distribution to scale throughput.</td>
    </tr>
  </tbody>
</table>



<h2 id="scaling-latency">Scaling latency in a content group</h2>
<p>
Whether we have a single group (flat distribution) or multiple groups,
the serving latency depends on the factors already described; <em>DQW</em> and <em>SQW</em>.
For use cases where <em>DQW</em> dominates the total query work <em>TQW</em>,
we can effectively scale latency down by parallelizing the <em>DQW</em> over more nodes per group.
</p><p>
It is important to decide on a latency service level agreement (SLA)
before sizing the Vespa deployment for the application and query features.
A latency SLA is often specified as a latency percentile at a certain throughput level - example:
<ul>
  <li>SLA Example 1: 95.0 percentile &lt; 100 ms @ 2000 QPS</li>
  <li>SLA Example 2: 95.0 percentile &lt; 40 ms @ 8000 QPS</li>
</ul>
As we have seen in the previous section,
different use cases might have different performance characteristics,
depending on how the dynamic work query portion is compared to the static query work portion.
This graph illustrates the relationship between overall latency versus number of documents indexed per node:
</p>
<figure>
  <img src="../img/Latency_documents.svg" />
</figure>
<ul>
  <li>For the yellow use case,
    the measured latency is as we can see almost independent of the total document volume.
    The observed latency at 10M documents per node is almost the same as with 1M documents per node.
    This illustrates a use case with low dynamic query work portion.
    Such a use case would be most cost effective by storing as many documents as possible
    (within the memory/disk/feeding constrains set by the concurrency settings and node flavor)
    and scale throughput by using a grouped distribution.</li>
  <li>For the blue use case,
      the measured latency shows a clear correlation with the document volume.
      This tells us that the dynamic query work portion is high,
      and that adding nodes to the group will help reduce the DQW per node.
      The sweet spot is found where we meet the targeted latency SLA at a given document volume.
      This sweet spot depends on which model or ranking features are in use,
      e.g how expensive the model is per hit.
      E.g. a <a href="../xgboost.html">xgboost model</a> with 3000 trees
      might breach the targeted SLA already at 200K documents,
      while a 300 tree model might be below our SLA at 2M documents.
      See also <a href="feature-tuning.html">feature tuning</a>.
  </li>
</ul>
<p>
It is possible to reduce latency of queries
where the <a href="#dynamic-query-work">dynamic query work</a> portion is high,
query throughput is relatively low, and using a multi-cpu core node.
Using multiple threads per search for a use case where the static query work is high,
will be as wasteful as adding nodes to a flat distribution,
as demonstrated in the previous sections.
</p>
<figure>
  <img src="../img/Threads-per-search.svg" />
</figure>
<p>
Using more threads per search will reduce <em>DQW</em> and latency as long as there are cpu cores available.
Typically there is a small synchronization overhead when concurrency becomes higher,
as the searcher threads needs to communicate through a shared heap containing the best hits found.
A search request with 4 threads will occupy all 4 threads until the last thread has completed,
and the intra-node per thread document space partitioning must be balanced to give optimal results.
By default this number is 1,
as that gives the best resource usage measured as cpu/query.
The optimal threads per search depends on the query use case,
and should be evaluated by benchmarking.
</p><p>
The threads per search settings globally is tuned by
<a href="../reference/services-content.html#requestthreads-persearch">persearch</a>.
This can be overridden to a lower value in
<a href="../reference/schema-reference.html#num-threads-per-search">rank profiles</a>
so that different query use cases can use different number of threads per search.
Using multiple threads per search allows better utilization of
multi-core cpu architectures for low query volume applications.
</p>



<h2 id="scaling-document-volume">Scaling document volume per node</h2>
<p>
One want to fit as many documents as possible into a node given the node constrains
(e.g available cpu, memory, disk) while maintaining:
<ul>
  <li>The targeted search latency SLA</li>
  <li>The targeted feed and update rate, and feed latency</li>
</ul>
With the latency SLA in mind, benchmark with increasing number of documents per node
and watch system level metrics and Vespa metrics.
If latency is within the stated latency SLA and  the system meets the targeted sustained feed rate,
overall cost is reduced by fitting more documents into each node
(e.g. by increasing memory, cpu and disk constraints set by the node flavor).
</p><p>
Vespa will block feed operations
if <a href="../reference/services-content.html#resource-limits">resource limits</a> have been reached.
</p>


<h3 id="disk-usage-sizing">Disk usage sizing</h3>
<p>
Disk usage of a content node increases as the document volume increases:
The disk usage per document depends on various factors like the number of schemas,
the number of indexed fields and their settings,
and most important the size of the fields that are indexed and stored.
The simplest way to determine the disk usage is to simply index documents
and watch the disk usage along with the relevant metrics.
The <em>redundancy</em> (number of copies) impact the disk usage footprint, obviously.
</p><p>
Note that <a href="../proton.html#proton-maintenance-jobs">content node maintenance jobs</a>
temporarily increases disk usage.
E.g. <em>index fusion</em> is an example, where new index files are written,
causing an increase in used disk space while running.
Space used depends on configuration and data - headroom must include the temporary usage.
See <a href="#metrics-for-capacity-plan">metrics for capacity planning</a>.
</p>


<h3 id="memory-usage-sizing">Memory usage sizing</h3>
<p>
The memory usage on a content node increases as the document volume increases.
The memory usage increases almost linearly with the number of documents.
The Vespa vespa-proton-bin process (content node) uses the full 64 bit virtual address space,
so the virtual memory usage reported might be very high,
as both index and summary files are mapped into memory using mmap
and pages are paged into memory as needed.
</p><p>
The memory usage per document depends on the number of fields,
the raw size of the documents and how many of the fields are defined as
<a href="https://docs.vespa.ai/documentation/attributes.html">attributes</a>.
Also see <a href="#metrics-for-capacity-plan">metrics for capacity planning</a>.
</p>


<h2 id="scaling-throughput">Scaling Throughput</h2>
<p>
As seen in the previous sections,
when the static query work (<em>SQW</em>) becomes large,
scale throughput using grouped distribution.
Regardless, if throughput is scaled by grouped distribution for use cases
with high static query work portion
or a flat distribution for high dynamic query work portion,
we should identify how much throughput the total system supports.
</p><p>
Finding where the latency starts climbing exponentially versus throughput
is important in order to make sure that the deployed system is scaled well below this throughput threshold.
Also, that it has capacity to absorb load increases over time,
as well as having sufficient capacity to sustain node outages during peak traffic.
</p><p>
At some throughput level, some resource(s) in the system will be fully saturated,
and requests will be queued up causing latency to spike up exponentially,
as requests are spending more time in the queue.
The more queries/s we try to push by increasing the load,
the longer a request needs to be in the queue waiting to be served.
This behaviour is illustrated in the figure below:
<figure>
  <img src="../img/QPS-scaling.svg" />
</figure>
A small increase in serving latency is observed as throughput increases,
until saturated at approximately 2200 QPS.
Pushing more QPS than this only increases queueing time,
and overall latency increases.
</p>



<h2 id="scaling-for-failures-and-headroom">Scaling for failures and headroom</h2>
<p>
It is important to also measure behaviour under non-ideal circumstances,
to avoid getting too good results.
E.g. simulating node failures or node replacements,
verifying feeding concurrency versus search and serving.
See <a href="sizing-examples.html#serving-availability-tuning">Serving availability tuning</a>.
</p><p>
Generally, the higher utilization a system has in production,
the more fragile it becomes when changing query patterns or ranking models.
</p><p>
The target system utilization should be kept sufficiently low for the response times to be reasonable and within latency SLA,
even with some extra traffic occurring at peak hours.
See also <a href="../graceful-degradation.html">graceful degradation</a>.
</p>



<h2 id="metrics-for-capacity-plan">Metrics for Vespa Sizing</h2>
<p>
The relevant <a href="../reference/metrics.html">Vespa Metrics</a> for measuring the cost factors,
in addition to system level metrics like cpu util, are:
</p>
<em>Metric capturing static query work (SQW) at content nodes </em>
<pre>
content.proton.documentdb.matching.rank_profile.query_setup_time
</pre>
<em>Metric capturing dynamic query work (DQW) at content nodes</em>
<pre>
content.proton.documentdb.matching.rank_profile.query_latency 
</pre>
<p>
By sampling these metrics,
we can calculate the theoretical speedup we can achieve by increasing number of nodes using flat distribution,
by using <em>Amdahl's law</em>:
</p><p><!-- depends on mathjax -->
  $$\text{max_speedup}_{\text{}} = \frac{1}{1 - \frac{query\_setup\_time}{query\_setup\_time+match\_time}}$$
</p><p>
In addition, the following metrics are used to find number of matches per query per node:
<pre>
content.proton.documentdb.matching.rank_profile.docs_matched
content.proton.documentdb.matching.rank_profile.queries
</pre>
<strong>Disk usage</strong>:
<ul>
  <li>documentdb: <em>vespa.content.proton.documentdb.disk_usage.last</em></li>
  <li>transaction log: <em>vespa.content.proton.transactionlog.disk_usage.last</em></li>
</ul>
<strong>Memory usage</strong>:
<ul>
  <li>documentdb: <em>vespa.content.proton.documentdb.memory_usage.allocated_bytes.last</em></li>
</ul>
</p>
